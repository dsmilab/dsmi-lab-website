{"meta":{"title":"DSMI Lab's website","subtitle":"吳姐：承穎是不是很悲觀啊？","description":"吳姊：承穎是不是很悲觀啊？","author":"DSMI members","url":"https://github.com/dsmilab/dsmi-lab-website","root":"/dsmi-lab-website/"},"pages":[{"title":"about","date":"2020-03-31T14:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"about/index.html","permalink":"https://github.com/dsmilab/dsmi-lab-website/about/index.html","excerpt":"","text":"Data Science and Machine Intelligence Lab (DSMI Lab) locates in SA314 at National Chiao Tung University. Our researches focus on data science, machine learning, and deep learning. The following are our main research topics in recent years: Natural Language Processing:We focus on disinformation and Chatbot system for customer survice. The disinformation project aims to help reader determine whether the information is true or not. Recently, we focus on the research of clickbait analysis and title-body text similarity. Smart Green HouseWe mainly cooperate with Taiwan Agricultural Research Institute Council of Agriculture. For the first-year project, we utilize the uniform experimental design to allocate the synthetic sensors. We estimate those synthetic sensor readings on the basis of linear model locally. We then apply $\\epsilon$-SSVR to fit the globally three-dimensional heat map by combining real sensor and synthetic sensor readings.The following is the link of the paper:https://www.mdpi.com/2504-3900/31/1/63.For the second-year project, our ultimate goal is to induce the micro-weather of the greenhouse from outside in by the meteorological data forecast by Central Weather Bureau; before directly applying the meteorological data, our first step is to examine the feasibility of deriving each of inner readings (i.e., temperature, humidity or luminosity) purely from those of outer weather stations. Anomaly detectionWe propose an online oversampling principal component analysis (osPCA) algorithm to detecting outliers from a large amount of data via an online updating technique. Unlike prior principal component analysis (PCA)-based approaches, we do not store the entire data matrix or covariance matrix, and thus our approach is especially of interest in online or large-scale problems.The link of the published paper is as follows:https://ieeexplore.ieee.org/abstract/document/6200273 Distributed Learning:As …, we applied distributed…. on SVM and PCA. SVMIn DSMI lab, we are intrested in SVM, SSVM (Smooth Support Vector Machine), and RSVM (Reduced Support Vector Machine). Security:In the era of big data, it is a very important issue to assist information security with more effective legal detection and protection through the technology of data science. The current research is to classify malicious programs through deep learning technology. Through the deep learning technology can automatic feature extraction, it can quickly learn and identify different malicious program families, which can reduce the cost of past security experts in analyzing malicious programs, and we aim to assist security experts focus on the suspicious part of the malware. Time Series Alumni 孫茂勛 黃彥傑 楊于萱 陳麒宇 翁玄懷Members 謝幸娟 吳芷綺 邱弋瑋 柯柏宏 李承穎 林永祥 邱信程 鄭翊騏 莊沅璟 張巧蓁 Advisor Data Science and Machine Intelligence Laboratory Yuh Jye Lee: yuhjye@math.nctu.edu.tw SA 314, No.1001, Daxue Rd., East Dist., Hsinchu City 300, Taiwan (R.O.C.)","author":"Chris, Jeff, John"},{"title":"categories","date":"2020-03-24T17:01:31.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"categories/index.html","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-03-24T17:01:24.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"tags/index.html","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Long Range Arena - A Benchmark For Efficient Transformers","slug":"Long Range Arena_ A Benchmark For Efficient Transformers","date":"2021-12-17T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2021/12/17/Long Range Arena_ A Benchmark For Efficient Transformers/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2021/12/17/Long%20Range%20Arena_%20A%20Benchmark%20For%20Efficient%20Transformers/","excerpt":"Long Range Arena: A Benchmark For Efficient TransformersConference: ICLRYear: 2021link: https://openreview.net/pdf?id=qVyeW-grC2k This is the new benchmark evaluating the efficiency of variant Transformer models 測試的資料範圍：長度 1K to 16K tokens (K=thousand)範圍 text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning","text":"Long Range Arena: A Benchmark For Efficient TransformersConference: ICLRYear: 2021link: https://openreview.net/pdf?id=qVyeW-grC2k This is the new benchmark evaluating the efficiency of variant Transformer models 測試的資料範圍：長度 1K to 16K tokens (K=thousand)範圍 text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning Benchmark評測項目的選擇標準 Generality: All efficient Transformers models should be applicable to our tasks. For instance, given that not all xformer models are able to perform autoregressive decoding (Wang et al., 2020), we include tasks that only require encoding. Simplicity: The tasks should have a simple setup. All factors that make comparisons difficult should be removed. This encourages simple models instead of cumbersome pipelined approaches. For instance, we avoid including any particular data augmentation and consider pretraining to be out of scope of this benchmark. Challenging: The tasks should be difficult enough for current models to ensure there is room for improvement to encourage future research in this direction. Long inputs: The input sequence lengths should be reasonably long since assessing how different models capture long-range dependencies is a core focus of LRA. Probing diverse aspects: The set of tasks should assess different capabilities of models like their ability to model relations and hierarchical/spatial structures, generalization capability, etc. Non-resource intensive and accessible: The benchmarks should be deliberately designed to be lightweight so as to be accessible to researchers without industry-grade computing resources TaskLong ListOpsTest: read hierarchically structured data in a long-context Test the ability to reason hierarchically while handling long contexts Byte-Level Text ClassificationTest: read real-world data and long documents Text classification in particular is associated with many real-world applications Byte-Level Document RetrievalTest: encode and store compressed representations Image Classification On Sequences Of Pixelsimage is flattened to a sequence (similar to how the previous tasks require capturing the hierarchical structure in the data) not allow extra modules such as a CNN Pathfinder (Long-Range Spatial Dependency)Test: learning long-range spatial dependencies. (also as sequences of pixels) make a binary decision whether two points represented as circles are connected by a path consisting of dashes. Pathfinder-Xchange the pixel of the image from 32 × 32 to 128 × 128 see if the same algorithmic challenges bear a different extent of difficulty when sequence lengths are much longer Result","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"Attention","slug":"Attention","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Attention/"}],"author":"Corrine"},{"title":"Data Noising as Smoothing in Neural Network Language Models","slug":"Data Noising as Smoothing in Neural Network Language Models","date":"2021-05-07T14:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2021/05/07/Data Noising as Smoothing in Neural Network Language Models/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2021/05/07/Data%20Noising%20as%20Smoothing%20in%20Neural%20Network%20Language%20Models/","excerpt":"ICLR 2017 Smoothing對NLP來說，很容易發生test dataset的字並沒有出現在training dataset，所計算 $c(word)$ 會等於零。這不代表那些字就不重要，所以要用Smoothing的方法調整機率。 If data sparsity isn’t a problem for you, your model is too simple!","text":"ICLR 2017 Smoothing對NLP來說，很容易發生test dataset的字並沒有出現在training dataset，所計算 $c(word)$ 會等於零。這不代表那些字就不重要，所以要用Smoothing的方法調整機率。 If data sparsity isn’t a problem for you, your model is too simple! Add-one smoothing應該是最早的方式 $V= {w:c(w)&gt;0 } \\cup {unknown }$ 可是這個方法效果並不好，如下圖 Additive smoothing前一個方法，把1替換成了 $\\delta$ Good-Turing estimation$r^{}=\\left ( r+1 \\right )\\frac{ n_{r+1} }{ n_{r} }$$n_{r}$是在n-grams出現r次的個數所以經過調整之後，機率會變成 $p_{GT}\\left ( x:c\\left ( x \\right )=r \\right )=\\frac{ r^{} }{N}$使得$c(w)=0$的時候機率不會為零，並且總和不變$N=\\sum_{r=0}^{\\infty }r^{*}n_{r}=\\sum_{r=1}^{\\infty }rn_{r}$ 問題:如果$n_{r+1}=0$?調整: $r^{*}=\\left ( r+1 \\right )\\frac{E\\left [ n_{r+1} \\right ]}{E\\left [ n_{r} \\right ]}$改用期望值代替 Interpolation就是插值法!!比起前面的方式更進一步考慮實際情況，例如說，同樣是烤肉跟燉肉都沒有出現在training dataset當中，但是明顯烤會比燉常見，所以烤肉的機率應該比燉高。$p_{interp}(w_{i}|w_{i−1}) = \\lambda p_{ML}\\left(w_{i}|w_{i−1}\\right) + (1 − \\lambda)p_{ML}(w_{i})$ Absolute discounting$p_{abs}(w_{i}|w_{i-1}) = \\frac{c(w_{i-1},w_{i})-d}{c(w_{i-1})}+\\lambda (w_{i-1})p(w_{i})$ Kneser-Ney舉例:”San Francisco” and “Francisco”，如果依照前一個的方法，任何字接Francisco機率都會變大，但是就比較常跟San一起出現。因此他轉換計算unigram的方式$p_{count}=\\frac{N(\\bullet w_{i})}{\\sum { {w}’ }N(\\bullet {w}’)}$$p{KN}(w_{i}|w_{i-1}) = \\frac{c(w_{i-1},w_{i})-d}{c(w_{i-1})}+\\beta (w_{i-1})p_{count}(w_{i})$ Apply on RNN model可是，RNN模型沒辦法計算count，所以他想到兩個方法可以有smoothing的效果 unigram noising隨機$\\gamma$的機率取代字，取代的字會從unigram frequency distribution抽樣 blank noising隨機$\\gamma$的機率取代字，用底線做取代”_” unigram noising as interpolation插值法公式noised blank noising as interpolationnoised 其他改進 彈性的$\\gamma$考慮兩種bigrams，”and the”和”Humpty Dumpty”第一種就是很常見的詞組，不希望他被noising影響而降低了機率，第二種就是A出現B通常也會一起出現的類型，考慮bigram的訊息量比unigram豐富，因此不希望被back off。$\\gamma_{AD}(x_{1})=\\gamma_{0}\\frac{N_{1+}(x_{1},\\bullet )}{\\sum { x{2} } c(x_{1},x_{2})}$ 改用其他的機率分布$q(x)\\propto N_{1+}(\\bullet ,x)$","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Corrine"},{"title":"Attention Is All You Need?","slug":"Attention Is All You Need","date":"2021-01-25T12:30:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2021/01/25/Attention Is All You Need/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2021/01/25/Attention%20Is%20All%20You%20Need/","excerpt":"","text":"paper : Attention Is All You Need AbstractThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely .這篇論文中提出了一個新的簡單網路結構 Transformer，它是一個基於 attention mechanisms的網路結構。 1. IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t−1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. RNN主要是透過將sequence的位置與time steps對齊來考慮不同sequence之間的關係 有一個問題 : 無法平行運算 2. Background3. Model Architecture 3.1 Encoder and Decoder Stacks Encoder : 每層有兩個sub-layers multi-head self-attention fully connected residual connection + layer normalization the output of each sub-layer is $LayerNorm(x + Sublayer(x))$ Decoder : 在原本的兩個子層中間插入一個新的 sub-layer，其連接 Encoder 的輸出進行 Multi-Head Attention。與 Encoder 部分相同，每一個 sub-layer均有 Residual Connection，並進行 Layer Normalization。 masked multi-head attention 透過一個 mask 確保 attention 在 i 時刻不會關注到後面的的資料(by masking out (setting to −∞)) multi-head attention fully connected residual connection + layer normalization 3.2 AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 透過 query 和 key-value 的 mapping$q$ : query(to match others)$\\quad q^i = W^q a^i$$k$ : key(to be matched)$\\quad k^i = W^k a^i$$v$ : value(information to be extracted)$\\quad v^i = W^v a^i$ ![](https://i.imgur.com/MAtkxWw.png =80%x) 3.2.1 Scaled Dot-Product Attention$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ $d_k$ 是Q, K的dimension attention常見的兩種操作 additive attention dot attention 3.2.2 Multi-Head Attention$MultiHead(Q,K,V) = Concat(head_1,…,head_h)W^O$where $head_i$ = $Attention(Q{W_i}^Q,K{W_i}^K,V{W_i}^V)$ ![](https://i.imgur.com/VR9ZUH7.png =80%x) 3.2.3 Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:![](https://i.imgur.com/FFe0szL.png =50%x) encoder-decoder attention layers : Q來自上一層decoder的輸出; K跟V來自encoder最後一層的輸出 encoder layers : Q,K,V來自相同的input decoder layers : Q,K,V來自相同的input 3.3 Position-wise Feed-Forward Networks有兩層FC, 第一層的acivation 是 ReLU, 第二層是 linear activation, 可以表示為$$FFN(Z)= max(0,ZW_1+b_1)W_2+b_2$$ 3.4 Embeddings and Softmax3.5 Positional Encoding由於 self attention 會看 sequence 的每一個資料(也就是說 , “天涯若比鄰” “比天若涯鄰” 的結果會是相同的 )![](https://i.imgur.com/woA80xJ.png =30%x)![](https://i.imgur.com/etHqUjb.png =80%x) $$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$ $$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$$ ![](https://i.imgur.com/d9pVblB.png =40%x) 以下引用自此文章 , 有興趣可以去看。 畫起來就是下面這種神奇的圖 有點像用binary format來表示的概念,只是把他轉成用$cos, sin$以下引用自此文章 But using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - Sinusoidal functions. 為什麼要用這個position轉換呢？ We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. （至於詳細證明可以參考這篇文章） 4. Attention Visualization Reference https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf https://meetonfriday.com/posts/5839a8bf/#Position-wise-Feed-Forward-Networks","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"yuan"},{"title":"TSVM (Transductive Inference for Text Classification using Support Vector Machines)","slug":"TSVM (Transductive Inference for Text Classification using Support Vector Machines)","date":"2020-12-01T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/12/01/TSVM (Transductive Inference for Text Classification using Support Vector Machines)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/12/01/TSVM%20(Transductive%20Inference%20for%20Text%20Classification%20using%20Support%20Vector%20Machines)/","excerpt":"","text":"TSVM (Transductive Inference for Text Classification using Support Vector Machines)tags: SVM, semi-supervised learning, SSLpaper linkcited：3517參考書籍：Learning to Classify Text Using Support Vector Machines Particularly, S3VMs have been widely applied to many tasks [10], and their representative algorithm, TSVM [23], has won the Ten-Year Best Paper Award for machine learning in 2009. (from Towards Making Unlabeled Data Never Hurt) 簡介這篇論文的方法主要是針對“text classification”這個task，從這個task研發出semi-supervised learning演算法 – TSVM。 TSVM是一個SVM based的semi-supervised 方法。 附註：What is text classification? text classification的目標是要讓model能夠自動將文檔分類。各文檔的類別有可能是multiple, exactly one, or no category. 因此，可以分別將每個類別視作二元分類的問題（屬於此類別or不屬於此類別）。 To facilitate effective and efficient learning, each category is treated as a separate binary classifcation problem. Each such problem answers the question of whether or not a document should be assigned to a particular category. TSVM (Transductive Support Vector Machine)論文提出的方法如下： 為什麼TSVM適用text classification? 附註Text classification的特點 High dimensional input space:文本裡的字都是features. Document vectors are sparse Few irrelevant features Arguments from [Joachims, 1998] show that SVMs are especially well-suited for this setting, outperforming conventional methods substantially while also being more robust. Dumais et al. [Dumais et al., 1998] come to similar conclusio 為什麼TSVM適用text classification? TSVM大致上保留SVM的特性 text的資料會有許多co-occurrence的資訊。因為TSVM是transductive，因此也同時考慮了test data裡的co-occurrence資訊（inductive learning沒有辦法去考慮test data的資訊）。 Algorithm for TSVM上面的方法若遇到大量unlabeled data，則需要非常大的運算量。而通常semi-supervised learning的問題，unlabeled data通常滿大的（text classification通常有大量unlabeled data）。因此，作者提出以下加速的最佳化問題，來近似OP2。 實際演算法如下：主要的想法： 先用labeled data去train一個inductive SVM，對unlabeled data標註。 再透過替換掉一些slack variable比較大的label，去降低objective function。 The key idea of the algorithm is that it begins with a labeling of the test data based on the classification of an inductive SVM. Then it improves the solution by switching the labels of test examples so that the objective function 附註： Loop 1:透過不斷的提高 $C_-^$ 和 $C_+^$，慢慢提升unlabeled data對模型的影響力。 Loop 2:過替換掉slack variable值較大的unlabeled data的label，去降低objective function的值。 論文中也提出定理來說Algorithm TSVM會在有限步收斂 Theorem 2 Algorithm 1 converges in a fnite number of steps. Experiment resultevaluation: Precision/Recall-Breakeven Point (text classification常用的measure) The P/R-breakeven point is defined as that valuefor which precision and recall are equal. 註： Precision: probability that a document predicted to be in class“+” truly belongs to this class.Recall: probability that a document belonging to class “+” is classified into this class Referencehttps://github.com/muratayoshio/tsvm","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SVM/"},{"name":"SSL","slug":"SSL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SSL/"},{"name":"Semi supervised learning","slug":"Semi-supervised-learning","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Semi-supervised-learning/"}],"author":"Puchi"},{"title":"快速掌握 git github","slug":"cheatsheet_git_github","date":"2020-11-28T22:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/11/28/cheatsheet_git_github/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/11/28/cheatsheet_git_github/","excerpt":"","text":"前情提要: 為甚麼需要git?如果不用版本控制工具 就會有以下的悲劇…. GIT要建專案時若要用git來追蹤，則強烈建議先用github 建專案後在clone到本地端(你的電腦)否則之後若要把在本地端建立的專案 傳到github 時會變得比較麻煩 初始設置(從github clone 下來的不用做) 裝git裝完後 設置user.name 及 user.email:12git config --global user.name \"USERNAME\" # 設置 user.namegit config --global user.email \"??????@???.COM\" # 設置 user.email 1git clone https://github.com/91884227/fake_news_feature.git git 的檔案週期 初始 Repository在專案目錄下執行: 1git init git 更蹤新文件 (Untracked -&gt; Unmodified)在專案目錄下執行:git add 文件名稱 example1git add test.py git 提交更新成下一個版本 (Modified -&gt; Staged)在專案目錄下執行:git commit -m &quot;更新訊息&quot; example1git commit -m \"更新test.py\" 如何回到上一個版本? 回復到最新提交版本 在專案目錄下執行: 1git reset --hard HEAD 回復到上一個提交版本 在專案目錄下執行: 1git reset --hard HEAD GITHUBGITHUB是甚麼 Q: 要如何把放在 github的專案(remote端) 載到 你的電腦呢 (local端) 呢? 1git clone \"那串在HTTPS下面的網址\" Q: local 更新了 如何把更新上傳到remote?在專案目錄下執行: 1git push 可能要先設定 user / user name user name 1git user.name \"學姊萬歲\" email 1git user.email \"xuejiewansui@gmail.com\" Q: remote 更新了 如何把更新拉到local?在專案目錄下執行: 1git pull 參考資料教學影片柯大大的筆記","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/GitHub/"}],"author":"Chuck"},{"title":"Feature Denoising for Improving Adversarial Robustness","slug":"Feature Denoising for Improving Adversarial Robustness","date":"2020-11-20T20:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/11/20/Feature Denoising for Improving Adversarial Robustness/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/11/20/Feature%20Denoising%20for%20Improving%20Adversarial%20Robustness/","excerpt":"","text":"Feature Denoising for Improving Adversarial Robustnesstags: adversarialCihang Xie(1,2) Yuxin Wu(2) Laurens van der Maaten(2) Alan Yuille(1) Kaiming He(2)1 Johns Hopkins University 2 Facebook AI Research2019 CVPR - 228次引用由於圖像攻擊對卷積網路來說產生了挑戰，並且卷積網路可能會提取那些噪音的特徵。基於這種觀察，我們開發了新的網絡體系結構，該結構通過執行特徵去噪來提高對抗性。 使用了一個塊狀結構去去噪，內部使用了non-local mean or 其他的filter.Trained End-to-End. 與adversarial training 結合時可以提高robustness.原本十個迴圈才只有27.9%的準確率，現在可以提高到55.7%，即使最高的2000次迴圈，也可以擁有42.6%準確率。 Introduction此圖為ResNet50所提取的特徵圖可以看到再PGD $\\epsilon=16$的擾動下，會讓ResNet注意到其餘不好的雜訊 再結果上來看 最好表現的去噪方法是再network裡面使用non-local means for feature denoising 相關於 self-attention Network and non-local network.後面的Ablattion study show that using mean filters, median filters, and bilateral filters 都能提高adversarial robustness. Related workAdversarial training 防禦那些 adversarial perturbation by training model on adversarial images[6] I. J. Goodfellow, J. Shlens, and C. Szegedy.Explaining and harnessing adversarial examples. In ICLR, 2015.[10] H. Kannan, A. Kurakin, and I. Goodfellow.Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.對x and x’之間的預測做logit相似.[16] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.Towards deep learning models resistant to adversarial attacks. In ICLR, 2018. pixel denoising[15] F. Liao, M. Liang, Y. Dong, and T. Pang.Defense against adversarial attacks using high-level representation guided denoiser. In CVPR, 2018.In constrast, our denoising in feature. Transform method with non-differentiable image preprocessing[8] C. Guo, M. Rana, M. Cisse, and L. van der Maaten.Countering adversarial images using input transformations. In ICLR,2018.這篇他使用了許多transformations 的方法像是image quilting, total variance minimization and quantization. 但這些方法大多只適用於灰盒(自訂)和黑盒上，白盒上例如Obfuscated Gradients Give a False Sense of Security:Circumventing Defenses to Adversarial Examples這篇的攻擊BackPropagation Differentiable Approximation(BPDA)就可以輕鬆擊潰。[4] A. A. Efros and W. T. Freeman.Image quilting for texture synthesis and transfer. In SIGGRAPH, 2001.Efros present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. They call this process image quilting.Efros提出一個簡單的圖像基底的方法, 把一些現有的圖像, 相同的縫製在一起, 何成一個新的圖像, 產生與眾不同的視覺外貌。 他們稱這個方法叫做圖像縫製。[17] L. I. Rudin, S. Osher, and E. Fatemi.Nonlinear total variation based noise removal algorithms. Physica D: nonlinearphenomena, 1992但在這篇我們是differentiable, and still improve adversarial robustness in white box attack. Feature Noise當圖片透過網路傳播的時候，adversarial perturbation 會逐漸增加，使得特徵圖中原本不存在的激活產生幻覺而激活，而導致做出錯誤的判斷。 從下圖中可以發現在沒有特徵的地方會有激發的現象 下圖為我們嘗試使用特徵去噪的結果，左邊是adversarial image 中間是 feature map 右邊是denoising feature map.這可以顯示他將噪聲的地方很好的抑制住了。 再開始描述方法前，還是要提醒一下，雖然我們可以很容易的觀察到一定的feature noise, 但是他卻很難被測量出來。並且比較不同模型和架構下或是不同的訓練方式(standard or adversarial)之間的特徵噪聲階層並非易事。We found it is nontrivial to compare feature noise levels between different models, in particular, when the network architecture and/or training methods (standard or adversarial) change.儘管如此我們還是相信觀察到的feature noise 可以反映出與對抗圖像的真實模樣。 Feature denoising paper In Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser(Fang-zhou Liao) purpose a High-Level Representation Guided Denoiser(HGD) to polish the feature affected by the adversarial perturbation. HGD trains a denoising U-Net using a feature-level loss function to minimize the feature-level difference between benign and adversarial examples In Towards Robust Neural Networks via Random Self-ensemble(Xuanqing Liu) propose the noise layer and the Ensemble learning. It need to train a model to fit the noise layer model(Randomness) and finally use the ensemble answer to print out the answer(Ensemble). In Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks (ICCV’19)(Aamir Mustafa) for counter adversarial attacks, it propose Prototype Conformity Loss to class-wise disentangle intermediate features of deep neural networks. From the figure, the existence of such adversarial examples is the close proximity of learnt features in the latent feature space. Method : Denoising Feature maps在卷積層的後面加上去噪的方塊去增加模型的robustness，使用對抗訓練的方式，train in end to end model with all layer of the model. 利用end to end adversarial training 的方式有助於消除依賴於數據上的特徵圖噪聲。通過對更早曾的更動也能更好的處理後面幾層的噪聲。 經驗上來說，最好的消除方法是用於機器翻譯的self-attention transformers和用於影像分類的non-local networks在這裡我們專注於去噪方塊的研究，並且也在裡面嘗試的加入了bilateral filtering, mean filtering, and median filtering inside our convolutional networks. Denoising Block基本的去噪方塊如下圖所示，Denoising operation 是 non-local means 或是其他變體的去噪方法所做，然後再讓其通過$1\\times 1$的convolutional layer，然後利用殘差連接做相加。以上主要去噪的只有該denoising operation 其餘的都只是在做特徵的組合。但是為何不直接執行去噪就好（？），因為去噪也會影響原本的特徵訊息，所以利用殘差連接去保留信號，並且利用$1\\times 1$來去做去噪以及保留信號的折衷。Ablation 會告訴你這兩個是有效的。 Denoising Operations Non-Local means : compute a denoising feature map $y$ of an input feature map $x$ by taking a weighted mean of features in all spatial locations $L$.$$y_i=\\frac{1}{C(x)}\\sum_{\\forall j\\in L}f(x_i,x_j)\\cdot x_j$$where $f(x_i,x_j)$ is a feature-dependent weighting function(特徵加權函數) and $C(x)$ is a normalization function(正則化函數), $L$ 是空間座標點.以下還考慮兩種feature-dependent weighting function form Gaussian(softmax) set $f(x_j,x_i)=e^{\\frac{1}{\\sqrt(d)}\\theta(x_i)^T\\phi(x_j)}$, where $\\theta(x)$ and $\\phi(x)$ 都是convolutional function with $1\\times 1$的kernel, $d$是channel數量. $f/C$is the softmax function. Dot Product set $f(x_j,x_i)=x_i^Tx_j$ and $C(x)=N$, $N$ is the number of pixels in x. 不像上面會是weight sum = 1, 但是實驗證明可以抑制噪聲並且提高robustness.這個版本可以不用使用額外的參數去做訓練。 Bilateral filter : 其實就是上面non-local means的局部版本$$y_i=\\frac{1}{C(x)}\\sum_{\\forall j\\in \\Omega(i)}f(x_i,x_j)\\cdot x_j$$$\\Omega(i)$ is a local region(eg. a $3\\times 3$ patch) around the pixel i. Mean filter : 這個是最簡單的方式均值濾波器，他可以降噪但是會使結構平滑。 Median filter : Defined as $y_i = median{\\forall j\\in \\Omega(i):x_j}$The median is over a local region $\\Omega(i)$ 並且是針對每個channel都去做。這個方法是很好的去防禦salt and pepper noise 和 相同的異常值。 Adversarial training這裡使用PGD adversarial training, 這是用來訓練出我們的網路並且也要訓練他本身的去噪能力。 PGD 是個迭代的攻擊方法，將所產生的adversarial image 投影到可行的解決方案空間中，在原始的圖像的每個最大像素擾動範圍內（受到$L_\\infty$約束） Perturbation $\\epsilon=16$, stepsize =$\\alpha=1$, iteration $n=30$圖形的初始點可以是乾淨的(20%)，也可以是randomly within allowed $\\epsilon$ (80%). 我們的訓練再mini-natch上，我們使用PGD來生成圖像，並且使用SGD去更新weight (在這裡的訓練資料全部都是PGD生成的adversarial image)他使用128個Nvidia V100 GPUs, 每個mini-batch 32張圖片再35,70,95的時候會把learning rate 下降十倍（總共110 epochs）ResNet101 - 36hrResNet152 - 52hrDataset : 128萬訓練 images in 1000 classes. 5萬測試集A label smoothing C. Szegedy, V. Vanhoucke, S. Ioffe, J.Shlens, and Z. Wojna.Rethinking the inception architecture for computer vision. In CVPR, 2016. of 0.1 is used. 並且攻擊時的white-box PGD $\\alpha=1$ (除了10次迭代的用$\\alpha=1.6$)),$\\epsilon =16$iteration = 10~2000 Experiment with result將ALP與其他三個模型去比較，由於ALP做了10次的PGD attack有27.9%的成功率。(在Evaluating and Understanding the Robustness of Adversarial Logit Pairing)這篇文章中，提及ALP其實並不robustness有可能會下降到0.6%在PGD的攻擊之下的)baseline(adversarial training without denoising block)ResNet152 baseline 52.5% -&gt; ResNet152 Denoising 55.7% 在長時間的PGD攻擊下，此模型還能有42.6的成功率(+3.4)A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018這個論文告訴我們pixel denoising 的方法可以在白盒的設置下去被破解的 相比較於pixel denoising， feature denosing可以持續的增進，這表明特徵去噪塊使欺騙網路更加的困難。 Ablation study:不同種類的去噪方塊，我們嘗試了幾種其他的去噪方塊和原本的比較，$3\\times 3$ mean filtering, $3\\times 3$ median filtering, $3\\times 3$ bilateral filtering (Eqn. (2)), and non-local filtering(原本的)。並且還與加上其他的東西來比較例如說bottleneck或是沒有denoising operation 只有$1\\times 1$的block下圖可以看到其他種類的去噪方塊都大於此三種(1)baseline (2)加上4個的bottleneck (3)加上四個沒有denoising operation 只有 $1\\times 1$的block最好的結果就是使用non-local mean filter +gaussian，但有趣的是很多參數的Gaussian version只比dot product 好了一點點。 The Ablation with denoising block觀察在這個裡面如果不要其他東西的效果雖然這兩個東西並沒有去噪的功能，但是卻是不可或缺的一部分可以看到若沒有$1\\times 1$的最後一部分，那麼他的準確度會下降很多若沒有殘差性的話，他的訓練效果會不穩定原因 ： 抑制噪音也會抑制我們的特徵，因此有效地把我們的降噪後的特徵和輸入的特徵做正確的組合顯得更為重要。 Black-Box attack :這裡使用5個2017CAAD攻擊者來使用，並且依照CAAD2018的標準，要全部都抵禦才算成功(全有或全無)根據他們黑盒的設定最大的$\\epsilon=32$，但這裡模型是用$\\epsilon=16$來訓練的 下圖表示了在ImageNet上的黑盒攻擊的結果，CAAD2017的獲獎者只有0.04%, 刪除兩個後還是只有13.4%, 這裡我們得到的準確率是43.1%的baseline 以及 46.4%的標準4個去噪塊其他均值，中值和雙邊濾波器也嘗試了一下，只有43.6%~44.4%的準確度，表示針對黑盒的攻擊，非本地的去噪比本地的去噪更為重要。並且最後也對每個residual block後面都加上一層denoising block, 可以達到49.5% CAAD 2018 attack“every defense entry needs to defend against 48 unknown attackers submitted to the same challenge” 在此比賽(黑盒)得到50.6%的成功率，在白盒10次PGD攻擊和100次PGD攻擊下，其準確率分別達到56.0％和40.4％ Denoising Blocks in Non-Adversarial Settings這裡要去觀察如果加上去噪塊，可是我們卻是在正常的資料下去做training 並且用正常的資料testing 那麼還會有差不多的準確率嗎？ 答案是差不多的 但問題是在最後，我們報告說，在經過清晰訓練的圖像上進行測試後，經過對抗訓練的ResNet-152基準具有62.32％的準確性，而經過“清晰”訓練的對等基準則具有78.91％。對於降噪版本（非本地，高斯），在乾淨圖像上，經過對抗訓練的網絡的準確性為65.30％，而經過乾淨訓練的對等網絡的準確性為79.08％。先前已經觀察到了對抗訓練和乾淨訓練之間的權衡（例如，在D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv:1805.12152, 2018.中）；我們希望這種權衡成為未來研究的主題 Conclusion我們發現特徵去噪的潛力，可以提高卷積網絡的對抗魯棒性研究表明，某些架構設計（即降噪塊）對於對抗魯棒性特別有用，即使與“乾淨”訓練和測試場景中的基線模型相比，它們(一般模型)也無法提高準確性。當與對抗訓練相結合時，這些特定的架構設計可能更適合於建模對抗圖像的基礎分佈。我們希望我們的工作將鼓勵研究人員開始設計具有“固有”對抗性魯棒性的捲積網絡架構。 The robustness potential for the feature denoising. The Feature noise is big to observed in feature level. The pixel denoising is insufficient, we need feature denoising. When combined with adversarial training, these particular architecture designs may be more appropriate for modeling the underlying distribution of adversarial images.","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"}],"author":"YiWei"},{"title":"A Defense Against Adversarial Attacks Using Deep Denoising Sparse Autoencoder(DDSA)","slug":"A Defense Against Adversarial Attacks Using Deep Denoising Sparse Autoencoder(DDSA)","date":"2020-11-03T13:10:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2020/11/03/A Defense Against Adversarial Attacks Using Deep Denoising Sparse Autoencoder(DDSA)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/11/03/A%20Defense%20Against%20Adversarial%20Attacks%20Using%20Deep%20Denoising%20Sparse%20Autoencoder(DDSA)/","excerpt":"","text":"DDSA: A Defense Against Adversarial Attacks Using Deep Denoising Sparse Autoencodertags: adversarial精心製作的細緻擾動會大大降低其性能並導致毀滅性後果，尤其是對於自動駕駛汽車，醫療保健和面部識別等對安全要求嚴格的應用而言使用了基於DDSA的方式在前處理的步驟執行，其在白,灰,黑盒子上都有不錯的效果。 Introduction在很多對安全敏感的應用上出現了很多DNN模型，因此robustness就會是一個對於抵抗adversarial attack的關鍵。在圖像分類上所製造出來的adversarial attack被人類正確分類但機器錯誤分類。這是可能的，因為所有的DNN model 都是基於(Independent and Identically Distributed)IID 的假設，這就代表其實正常的數據集都是落在相似的數據分佈或相同的流形上。因此攻擊方就是去最大化他的loss，但要讓他的擾動不被察覺並且還要導致分類錯誤。最主要的想法也就是在尋找最短的方向，push input image out to the decision boundary to the target class(target attack) or any other class(untarget attack).(See Figure 1) 這裡有三種不同攻擊方式 white box attack : 對方知道你所有的防禦方法，模型架構以及你的參數。 black box attack : 對方不知道你的模型架構和參數，攻擊方只會知道你的模型輸出(標籤或是信心值), 攻擊方可能會訓練另外的模型或是替代模型，利用adversarial example的傳遞性去實作。 gray box attack : 通常來說介於白盒和黑盒之間，在這篇論文下，攻擊方可以完全訪問模型的體系結構和參數，但對防禦技術一無所知。在本文中使用DDSA的圖像預處理，再輸入階段降低噪聲，再將轉換過後的圖片做輸入丟入分類器在一般的模型情況下，隨著圖像在網絡中傳播，對抗性擾動會逐漸增加，這會導致噪聲較大的特徵圖和不適當的激活，從而有分類錯誤的風險，故在這裡使用了稀疏約束的架構，他要確保神經元僅針對有意義的Neuron去觸發，從而限制那些被觸發的adversarial pattern. 利用稀疏約束的效果去抵禦有挑戰性的攻擊，得到還不錯的robustness. Related work先介紹一下什麼是adversarial attack(problem formulation)，接著介紹一些攻擊的方法，再介紹一些防禦的方法 Problem Formulation給你一張圖片的Space $X = [0,1]^{H\\times W\\times C}$, target classification model $f(\\cdot)$. 我們的正常輸入圖片為$x \\in X$而 adversarial example $x’ \\in X$ 但目標是 $f(x) \\neq f(x’)$ and $d(x,x’) \\leq \\epsilon$, where $d(\\cdot)$ is the distance function with three types $L_0$ distance, Euclidean distance($L_2$) and Chebyshev distance($L_\\infty$) and the $\\epsilon \\geq 0$ is the threshold. Adversarial attack Fast Gradient Sign Method(FGSM) : $x’ = x+\\epsilon sign(\\nabla_x J_\\theta(x,y))$, $\\theta$ 是模型的參數, $\\nabla J(\\cdot)$計算gradient w.r.t x, $sign(\\cdot)$為方向函數, $\\epsilon$為小的係數。利用損失函數的一階近似來得到adversarial examples. Boosting Adversarial Attacks with Momentum(Momentum Iterative method MIM) : 增進FGSM攻擊的有效性利用Momentum的習性，$g_{t+1} = \\omega g_t + \\frac{\\nabla_x J_{\\theta}(x_t^’,y)}{|\\nabla_x J_{\\theta}(x_t^’,y)|_1}$ ,且 $g_t$ 是第t個迴圈的梯度, $\\omega$是係數, $|\\cdot|1$ is the $L_1$ distance.最後他的遞迴方程式為 $x{t+1}^’ = x_t^’ + \\epsilon sign(g_{t+1})$ PGD CW RAND+FGSM Defense adversarial training : defense distillation : Magnet : Proposed Model DDSAThreat Model : Assume that the attacker has full access to the classification model. (gray box attack), and the adversarial example $x’$ is satisfied $f(x) \\neq f(x’)$ and $d(x,x’) \\leq \\epsilon$. We don’t consider the specific classification model, we defend all types of the classification model against any attack Deep Denoising sparse auto-encoder(DDSA) :This is the whole model, DDSA+classifier(Figure 3)This is a auto-encoder block in front of the classification model(Figure 4)(Autoencoder 介紹)自動編碼器利用數據分佈集中在低維流形周圍的事實，並旨在學習該流形的結構。故目標也是將數據推回原本流形架構上。在denoising autoencoder下的idea 是我想要學習在autoencoder下的壓縮表示對於adversarial perturbation 是robust的cost function $J = \\frac{1}{n}\\sum^n_{i=1}(x_i-f_{\\theta}(x_i^’))^2$, $n$ is the number of samples and $x^’$ is the perturbed version of $x$. (Use PGD to train this, 因為PGD比較能夠給予更好的wide range) The training$$\\theta^* = \\arg\\min_\\theta[E_{(x,f_\\theta(x^’))\\in\\hat p_{data}}(\\max_{\\delta \\in S}J(x,f_\\theta(x^’)))+ E_{(x,f_\\theta(x))\\in\\hat p_{data}}(J(x,f_\\theta(x)))]$$ 並且利用增加sparsity constraint to the Fully Connected layers of the proposed DDSA.希望能夠去限制那些神經元不常被觸發方便去提取有意義的點，並且稀疏約束允許強制隱藏單元的激活等於某個目標激活$\\mu$。 當我們使用ReLU激活函數時，$\\mu$的值設置為0.1（即接近0的值)$E_{x\\sim D}[a_i^{(j)}]=\\mu$, where $x$ is the input image sampled from a distribution $D$ and $a_i^{(j)}$ is the activation of the $i^{th}$ hidden unit at the $j^{th}$ hidden layer, which is the 2nd one in our case.A running-estimate $\\hat \\mu_i$初始值=0$$\\hat \\mu_i = 0.999\\hat \\mu_i+0.001a_i^{(j)}-(1), b_i^{j-1} = b_i^{j-1} -\\alpha \\beta (\\hat \\mu_i-\\mu) - (2)$$ the bias term $b_i^{j-1}$ is used as a regulator for our constraint and $\\alpha$ is the auto-encoder learning rate.Thus, when the condition $\\hat \\mu_i &gt; \\mu$ is valid, then we decrease the activation by decreasing $b_i^{j-1}$ and vice versa, with $\\beta$ is the learning rate trying to satisfy the sparsity constraint.For gradient stepWe first update the weight with $\\theta^*$, and then update the $\\hat \\mu_i, b_i^{j-1}$ using (1) and (2). ExperimentDataset : MNIST and CIFAR-10 Results on Black boxAttacks with FGSM, Rand+FGSM, MIM, PGD, C&amp;W as black-box attackers.由於這些都是白盒攻擊的例子，故我們使用替代模型去模擬黑盒攻擊, 使用隨機的150筆資料(test set)並利用分類器直接做標記當作我們替代模型的訓練。MNISTDDSA vs DDA Results on Gray boxMNIST with DDSA穩定性佳 觀察DDSA在不同的擾動情況下的比較 迴圈數與正確率的圖， more robust than DDA defense. CIFAR-10 還原效果 Results on White box","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"}],"author":"Yi-Wei"},{"title":"Topic Modeling in Embedding Spaces","slug":"Topic Modeling in Embedding Spaces","date":"2020-10-24T12:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/10/24/Topic Modeling in Embedding Spaces/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/10/24/Topic%20Modeling%20in%20Embedding%20Spaces/","excerpt":"","text":"Topic Modeling in Embedding Spacestags: NLP,Topic Modeling論文網址參考網址LDA LDA(Latent Dirichlet allocation)假設有一個字典詞庫，在不同的topic下，同一個詞有不同的被選擇的機率，當然，同一個topic下，不同的詞有不同的被選擇的機率。考慮有D個文檔的羽料庫，包含V個不同的術語$w_{dn} \\in {1…V}$表示在d個文檔的第n個字在LDA中，topic distribution：$\\theta$ 和 vocabulary distribution：$\\psi$都是根據類別而來的。所以LDA要預先確定主題的數量：K:$\\psi_{1:K}$(每一個主題都是整個vocabulary的分佈)，不同的主題有對應的distribution，總計有k種不同的distribution，且LDA假設每一個文檔都是來自主題的混合，主題在語料庫是共享的，每一個文檔的比例都是唯一的(對不同document會有不同的主題分佈)。$\\psi_i,{i\\in (1,K)}$對一個document有K個topic，而對一個corpus有D個document，對不同的document會有不同的topic，其對應的$\\theta_d$也會不同。以下為LDA的distribution流程： 確定文檔數目$D$，主題數目$K$，單詞字典集$V$，預設參數$\\alpha = (\\alpha_1,…,\\alpha_d,…,\\alpha_D)$和$\\beta = (B_1,…,B_k,…,B_K)$ 生成$K$個主題的單詞分佈$\\psi = {\\psi_1,…,\\psi_k,…,\\psi_K}$ 對特定的文檔$w_d$： 生成文檔對應的主題分佈$\\theta_d \\sim Dirichlet(\\alpha_\\theta)$ 對文檔的$w_d$中的第n個字$w_{dn}$ 生成其主題 $z_{dn}\\sim cat(\\theta_d)$(cat表示分類的分佈) 生成對詞的distribution : $w_{dn}\\sim cat(\\psi_{z_{dn}})$ 其中$\\psi_k\\sim Dirichlet(\\alpha_\\psi)$ for k = 1,…,K 從上面我們可以發現，LDA的效果受預設的主題數$K$ ，以及預設參數 $\\alpha$和 $\\beta$ 的影響。 雖然LDA有著廣泛的應用，但是其也存在不少問題，LDA面對擁有large vocabulary（long tail effect）的語料時效果不好，時常需要去除最常發生和最不常發生的word，單是這樣做會去除掉一些重要的word且限制model的範圍。其次LDA是基於 bow(bag of words)來實作的，所以詞彙間的關係常常被忽略。後續的做法分成兩類 引入 knowledge external knowledge：使用WordNet 引入相近關係的word lexical and semantic knowledge：將句子變成parsing tree，考慮字詞之間的關係 Makov modeling(HMM)：建構詞與詞之間的關係 引入 embedding 改變原本bow的word distribution，有主題性的 word distribution改成 normal distribution 用 NN學出 distribution分析文檔來學習有意義的單詞表達，現有的model對於大型以及heavy-tailed(尾端分佈不小的vocabulary table)的表現不好，作者提出ETM(embedded topic model)，用傳統的topic modeling 與 word embedding做結合，可以用有分類(topic model)的分佈對每一個單詞做modeling 分類分布的參數是word embedding 和 topic embedding的內積。對於 rare word 和 stop word 的 large vocabulary dictionary，ETM也可以發現可解釋的主題，在topic prediction方面比傳統的LDA較好。 topic model是發現文檔中的Latent semantic structure的統計方法。 關注CBOW的word embedding作法，每一個$w_{dn}$的likelihood為$w_{dn}\\sim softmax(\\rho^T\\alpha_{dn})$此論文的具體做法：詞變成word embedding $\\rho \\in \\mathbb{R}{(L,V)}$ V($\\rho_v\\in \\mathbb{R}^L$)是詞的數量 L是word embedding size。而context embedding是$w{dn}$周圍單辭的embedding總和($\\alpha_v$ for each word v)同樣把K個topic訂為K個長度為L的向量$\\alpha_k$，主題和word處於同一個semantic space，與傳統的topic modeling不同。 在傳統 topic modeling 每一個主題都是整個vocabulary的分佈，但是在ETM中，第k個主題是在embedding space中的一點$\\alpha_k\\in \\mathbb{R}^L$稱$\\alpha_k$是一個主題embedding，它是單詞語義空間中第k個主題的分佈式表示。 在生成過程，ETM使用topic embedding在vocabulary上形成按主題的分佈，具題來說ETM使用loglinear model(用word embedding 和 topic embedding的內積)。ETM通過測試單詞embedding和topic embedding的一致性為單詞v的主題k分佈判斷 ETM在ETM下文章的生成過程和LDA一樣，依然需要使用文檔的topic distribution $\\theta$以及topic vocabulary distribution(主題的單詞分佈)$\\psi$在LDA中 $\\theta \\sim Dir(\\alpha), \\psi\\sim Dir(\\beta)$。而ETM中$\\theta \\sim LN(0,I)\\Longleftrightarrow\\delta \\sim N(0,I),\\theta = softmax(\\delta),\\psi_k\\sim softmax(\\rho^T\\alpha_k)$。讓文檔的topic distribution跟一個 logistic-normal distribution一樣，而主題的單詞分佈則是topic distribution和每一個詞的word vector 點乘後再softmax的結果。生成過變為： 確定文檔數目$D$，主題數目$K$，單詞字典集$V$ 對文檔 $w_d$： 1生成對應於文檔的主題分佈 $\\theta_d = softmax(\\delta_d),\\delta_d \\sim N(0,I)$ 2對文檔 $w_d$中的每一個詞 $w_{dn}$： a生成對應主題 $z_{dn} \\sim cat(\\theta_d)$ b生成詞 $w_{dn}\\sim softmax(\\rho^T\\alpha_{z_{dn}})$ LN表示log normalize distribution，其中抽取的$\\theta_d$表示為$\\theta_d = softmax(\\delta_d),\\delta_d \\sim N(0,I)$(（我們將Dirichlet替換為logistic normal distribution，以便更輕鬆地在推理算法(amortize algorithm)中使用reparameterization；請參閱第5節。）) 1 and 2a主題建模的標準步驟:文檔表示為topic distribution，並且為觀測到的每一個單詞作出他的topic distribution，與2b不同，使用$\\rho$和topic embedding$\\alpha_{z_{dn}}$($z_{dn}$的topic embedding) 2b的topic embedding反映了CBOW的likelihood(CBOW使用上下文單字反映向量$\\alpha_{dn}$)。與之相反的，ETM使用$\\alpha_{z_{dn}}$作為上下文向量，其中$z_{dn}$為從每一個文檔的變量$\\theta_d$抽取的主題分布，ETM從文檔的上下文抽取而不是從單詞周圍的字 ETM likelihood 使用了$\\rho$表示低維空間的vocabulary，實際上他也可以使用預先訓練好的或是逐步訓練(當ETM在訓練過程訓練embedding時他會找到topic和embedding space) 當ETM用先訓練好的embedding時，他會在特定的空間學習主題分布，當embedding中有沒有出現在羽料庫的單詞時，這招特別有用，ETM可以使得這些未出現過的如何分配到主題，因為它可以計算$\\rho_v^T\\alpha_k$，即便v沒有出現在corpus內 Inference and Estimation參數說明:corpus of documents{$w_1…w_D$}, $w_d$ 是$N_d$的字集合而成word embedding : $\\rho_{1:V}$topic embedding : $\\alpha_{1:K}$每一個$\\alpha_k$是在embedding空間中的一個點 在訓練過程中，word embedding可以使用預訓練好的也可以動態訓練。 topic embedding則需要動態訓練。這時候，ETM的參數就變為word embedding $\\rho$ 和topic embedding $\\alpha$ 。所以，一個文檔的marginal likelihood表示為：$L(\\alpha,\\rho)=\\sum^D_{d=1} = logp(w_d|\\alpha,\\rho)$ 因為每個文檔的marginal likelihood 難以計算(它涉及了主題模型$\\delta_d$的積分) 每個單字的條件分布會邊緣化主題分布$z_{dn}$因此$\\theta_{dk}$表示(轉換後的)topic proportions $\\beta_{kv}$代表傳統的topic(由word embedding和topic embedding來的$\\beta_{kv} = softmax(\\rho^T\\alpha_k)|_v$) 使用邊分方法(variational inference)迴避難以推理的積分問題，用以優化每一個文檔的log marginal likelihood範圍(Eq.4)，所有有兩套參數需要優化: 如上所述的model parameter和 variational parameter(用來拉近marginal likelihood的範圍) Variational parameter首先假定未經轉換的topic distribution(proportions) $q(\\delta_d;w_d,v)$，使用amortized inference，其中$\\delta_d$的分佈取自於文檔$w_d$和v而來，且$q(\\delta_d;w_d,v)$也是一個gaussian distribution，其mean 和variance來自v參數的NN，算出$\\delta_d$的mean和variance，為了容納不同長度的文檔，透過用單詞數量$N_d$normalize bag of word 來當成inference network的input。使用這組variational distribution來限制marginal likelihood(ELBO(包含model parameter和variational parameter))第一項注重在優先觀察單字的topic distribution$\\delta_d$第二項鼓勵他們接近先驗機率(prior)$p(\\delta_d)$且開object function最大化了原先預期的log-likelihood$\\sum_d logp(\\delta_d,w_d|\\alpha,\\rho)$ 因為 $p(w_d|\\alpha,\\rho)$涉及到一個latent variable $\\delta_d$的積分，如果將$\\delta_d$的所有可能都計算，積分會變得intractable。所以文章中使用了近似方法（假設），由於$\\delta_d$取決於$w_d$和$v$，且由於$\\delta_d\\sim N(0,I)$，所以$q(\\delta_d;w_d,v)$是一個normal distribbution，文章使用了VAE的想法，將原本encoder出來的output分成$\\mu$以及$\\sigma$再藉由reparameter trick使得bp可以成立（也同時讓$\\delta_d$可以被運算），這時log-likelihood function變成$$L(\\alpha,\\rho,v) = \\sum^D_{d=1} \\sum^{n_d}{n=1}E_q[logp(w{nd}|\\delta_d,\\rho,\\alpha ）]-\\sum^D_{d=1}KL(q(\\delta_d;w_d,v)|p(\\delta_d)) $$ 通過reparameter trick將全梯度的Monte Carlo引進noisy gradient，還使用subsampling來處理large collection of documents，Lr: Adam 整體算法為：NN(x;v)表示輸入x和參數v的NN在training時的loss：使用kld_theta + recon loss(predition*bag of words) Empericaldataset : 20Newsgroups , New York Times為了研究ETM的性能:一個好的document model 應該要提供語言連貫性和單字的準確度，因此要根據預測的準確性和主題的可解釋性來衡量效果。以loglikelihood來衡量文檔完成特定task的準確度，以主題一致性和文字多樣性的結合來衡量可解釋性，實驗結果顯示:在可解釋的模型中，ETM是提供更好的預測和主題的模型。 在6.1節研究了停用詞存在時各種方法的robustness。標準的topic model會在這種情況下出問題，因為停用詞在很多文檔中，所以所學習到的主題都包含一些停用詞，導致其解釋性很差。相反ETM能再用停用詞下可解釋。 20Newsgroup是新聞集合，透過過濾掉在70%以上文檔都出現的字來預處理。將閾值從100（較小的詞彙表，其中V = 3,102）更改為2（較大的詞彙表，其中V = 52,258）。 預處理後，我們進一步從驗證和測試集中刪除單字文檔。 我們將語料庫分為11260個文檔的訓練集，7552個文檔的測試集和100個文檔的驗證集。在相同處理方法下New York TimesV = 5,921到V = 212,237的詞彙表形成該語料庫的版本。 預處理後，我們將85％的文檔用於培訓，10％的文檔用於測試，並將5％的文檔用於驗證。 對照model我們將ETM的性能與其他兩個文檔模型進行了比較： latent Dirichlet allocation（LDA）和the neural variational document model（NVDM）。LDA 主題$\\beta_k$主題分布$\\theta_d\\sim Dirichlet \\ prior$這是一個條件共軛模型，可以通過坐標上升進行變分推斷。 NVDM假設likelihood$w_{dn}\\sim softmax(\\beta^T\\theta_d)$且$\\theta_d\\sim N(0,I_k)$(K-dimensional vector 對每一個文檔都有各自的變量)，$\\beta \\in \\mathbb_{(KxV)}$，NVDM使用了對每個文檔的latent vector $\\theta_d$ 對 $\\beta$ embedding做平均。跟ETM依樣NVDM也是永amortized variational inference來找出近似的posterior for $\\theta_d和\\beta$，由於NVDM不能解釋為主題模型，他的latent variable沒有限制，所以提出了其變體(∆-NVDM)約束$\\theta_d$用logistic normal取代原本的Gaussian priorThis can be thought of as a semi-nonnegative matrix factorization.) 用pre train好的embedding: labeled ETM(使用skip-gram embedding) 算法設定:給定corpus每個model都有一個近似的posterior inference problem，實驗中使用了variational inference且使用stochastic variational inference (SVI)來加速優化，minibatch :1000對LDA delay is 10 and the forgetting factor is 0.85.在SVI中，LDA享受坐標上升變化更新，並通過5個內部步驟來優化局部變量。 對於其他模型，我們對局部變量θd使用攤銷推斷。 我們使用三層推理網絡，並將本地學習率設置為0.002。 我們對變分參數使用2正則化（權重衰減參數為1.2×10−6） 表1說明了不同模型的嵌入。 所有方法都提供可解釋的嵌入-具有相關含義的詞彼此接近。 ETM和NVDM從嵌入圖中學習與嵌入類似的嵌入。 ∆-NVDM的嵌入方式不同； 局部變量上的單純形約束改變了嵌入的性質。 接下來，我們將學習所學的主題。 表2(K=300個主題)顯示了所有方法使用的7個最常用的主題前五個字，由主題分布$\\theta_d$的平均值給出。 LDA和ETM都提供了可解釋的主題。 NVDM和∆-NVDM均未提供可解釋的主題。 它們的模型參數β不能解釋為混合形成文檔的詞彙分佈 Quantitative results而在evaluation時使用了3種function來作指標： topic coherence(主題一致性) ：選取同一個topic 內的兩個自的 mutual information的平均值，認為一個連貫的主題通常標示那些經常在相同文檔中出現的詞（代表mutual infrmation會較高）。$P(w_i,w_j)$是單詞$w_i,w_j$同時出現在文檔中的機率，而$P(w_i)$是單詞$w_i$的marginal probability(用empirical counts近似)想法: 一個具有一致性的主題應該會有一些在一個文檔中一直出現的字，其mutual information較高(那些字彼此相關性較高) topic diversity(主題多樣性) ： 我們將主題多樣性定義為所有主題的前25個單詞中唯一單詞的百分比。所有主題的前n個詞裡面不同詞的比例。分數愈接近０代表相同字出現愈多，接近１代表主題的多樣性愈高。我們將模型主題質量的總體指標定義為模型主題多樣性和主題一致性的產物 好的主題模型應該提供好的word distribution ：計算ETM在文檔補全任務上的log-likelihood，做法是將每一個文檔分成２個字典集，前半用來獲取topic distribution，並且依照這個distribution來生成其文檔剩下的詞，將生成分布與另外一半的詞做比較，計算log-likelihood ，較好的model會有較高的值。&#39;A good document model should provide higher log-likelihood on the second half.&#39;Note: 1,2,3只會有一個來計算，3的計算方法和在計算training loss時接近 通過主題一致性和主題多樣性的標準化乘積（越高越好）衡量的主題質量與通過文檔完成時的標準化對數似然法衡量的預測性能（越好）相比。圖4顯示了主題質量與預測能力的關係。 （為簡化可視化，我們通過減去均值並除以標準偏差來對兩個指標進行歸一化。）最佳模型位於右上角。NVDM的主題質量遠遠低於其他方法。 （它不提供“主題”，因此我們評估其β矩陣的可解釋性。）在預測中，兩種ETM版本至少與單純形約束的∆-NVDM一樣好。 Stop word現在，我們研究包含所有停用詞的《紐約時報》語料庫。 我們刪除不常用的單詞以形成10,283大小的詞彙表。 我們的目標是證明標記的ETM即使在停用詞的存在下也能提供可解釋的主題，這是主題模型通常失敗的另一種情況。 特別是，鑑於停用詞出現在許多文檔中，傳統的主題模型將學習包含停用詞的主題，而與主題的實際語義無關。 這導致較差的主題可解釋性。我們將LDA，∆-NVDM和帶標籤的ETM與K = 300的主題進行擬合。 （我們不報告NVDM，因為它不提供可解釋的主題。）表3顯示了主題質量（主題一致性和主題多樣性的乘積）。 總體而言，貼有標籤的ETM在主題質量方面可提供最佳性能 ETM有一些專門針對停用詞的“停用主題”（請參見圖5），而∆NVDM和LDA幾乎在每個主題中都有停用詞。 原因是停用詞與其他每個詞同時出現在同一文檔中； 因此，傳統主題模型很難區分內容詞和停用詞。 帶標籤的ETM識別停用詞在嵌入空間中的位置； 它使他們脫離了自己的主題。 Conclusion開發了ETM，這是一種將LDA與單詞嵌入結合起來的文檔生成模型。 ETM假定主題和單詞生活在相同的嵌入空間中，並且單詞是從分類分佈生成的，其參數是單詞嵌入和所分配主題的嵌入的內積。ETM甚至在帶有大量詞彙的語料庫中也學習可解釋的詞嵌入和主題。 我們針對幾種文檔模型研究了ETM的性能。 ETM學習語言的連貫模式和單詞的準確分佈。 大型vocabulary對於大多數主題模型，主題比例的後驗很難計算。 我們推導了一種有效的算法，可以用變分推斷來近似後驗，並另外使用攤銷推斷來有效地近似主題比例。 生成的算法使ETM適合具有大量詞彙的大型語料庫。 ETM的算法既可以使用先前擬合的單詞嵌入，也可以與其他參數一起擬合。 （特別是，圖1至圖3是使用ETM版本獲得的，該版本已獲得pretrain的skip-gram單詞嵌入。） Related work單詞相似度納入主題模型，並且已有先前的研究分享了這一目標。 這些方法要么修改主題先驗（Petterson等，2010; Zhao等，2017b; Shi等，2017; Zhao等，2017a），要么修改主題分配先驗（Xie等，2015）。 還有其他幾種組合LDA和嵌入的方法。 Nguyen等。 （2015年）將LDA定義的似然性與使用預擬合詞嵌入的對數線性模型混合； Bunk和Krestel（2018）用從高斯中提取的嵌入內容隨機替換了從某個主題中提取的單詞; 和徐等。 （2018）採用了幾何學的觀點，利用Wasserstein距離共同學習主題和單詞嵌入。 具體來說，這些方法通過攤銷推斷和變分自動編碼器來減少文本數據的大小（Kingma和Welling，2014年； Rezende等人，2014年）。 為了在ETM中執行推理，我們還利用了攤銷的推理方法 ETM建立在LDA基礎上 Code Explain 在ETM model內 當input bows:會經過nn.Linear將vocab_size的bows encode到hidden_size當成是q_theta 再透過q_theta 做出vae中的latent space 的mu和variance(800-&gt;50維)，透過sample從mu和variance sample出一個gaussian distribution，在經過reparameterization trick 形成一個gaussian $z\\sim N(0,I)$(batch_size,num_topics) alpha：word embedding(vocab,embed_size(300)) * nn.Linear(embed_size(300),num_topics) beta：alpha.transpose(1,0) 再將 z(batch_size,num_topics)與 beta(num_topics,vocab)做內積 做出來的prediction(vocab,batch_size)與原先的bows(vocab,batch_size)相乘當成loss","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP Topic_Modeling","slug":"NLP-Topic-Modeling","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Topic-Modeling/"}],"author":"PoH_Ko"},{"title":"Get To The Point- Summarization with Pointer-Generator Networks","slug":"Get To The Point_ Summarization with Pointer-Generator Networks","date":"2020-10-22T12:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/10/22/Get To The Point_ Summarization with Pointer-Generator Networks/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/10/22/Get%20To%20The%20Point_%20Summarization%20with%20Pointer-Generator%20Networks/","excerpt":"","text":"Get To The Point: Summarization with Pointer-Generator Networkstags: NLP,S2S,Pointer Network,Summary,Loss主要目的因為在s2s時遇到人名或是地名，可能遇到OOV問題而只輸出[UNK]這種輸出。而Pointer Generator Network則是從原本輸入的某一些片段直接擷取過來當成輸出，彌補字典集限制的問題。Extractive(pointing),Abstractive(generating)傳統S2S問題，可能複製同樣的錯誤，重複同樣錯誤的字詞 提出： hybrid pointer-generator network Coverage用來追蹤有哪一些已經被summary model提出過了，確保不要再被重複提出Modeling Coverage for Neural Machine Translation 在CNN/Daily Mail Summarization 有SOTA的2 ROUGE 分數 一般來說Extractive 會有較好的效果且較簡單保證了基礎以及語法上的正確性。但是對於高質量的抓取摘要至關重要的能力：釋義，概括，知識整合(knowledge)，只在abstractive network中可能。 RNN使得abstractive summary 可行，但是會有不斷重複某同樣意義的片段以及 OOV的問題 現今的abstractive專注在headline generation tasks（使得句子被總結成headline），而作者認為長度較長且不重複片段的summary較有挑戰性且更有用。 ModelsS2S attentional(Baseline)在encoder(bilstm)，decoder(undirctional LSTM)每一個token$w_i$對應一個hidden state$h_i$在每個步驟t上，decoder（單層單向LSTM）都會接收到前一個單詞的token embedding（訓練時，這是參考摘要的前一個單詞(Teacher forcing)；在測試時，它是解碼器發出的前一個單詞，並具有解碼器狀態$s_t$。對應的attention distribution可被當成是從source words中找出的word probability（decoder產出下一個字），接下來用對應的attention weight與對應的hidden state加權總和當成上下文vectpr$h^_t=\\sum_ia^t_ih_i$(第t個step的sentence vector)把 $h^t$和decoder的state$s_t$在經過兩層linear layer來產出 $P{vocab}$P(w)是詞彙表中所有單詞的概率分佈，提供了預測單詞w的分佈loss：negtive log likelihood of target word $w^*_t$ Pointer-genretator 前面的Baseline加上pointer的hybrid model，可以從source text copy 或是generate words from a fixed vocabulary。attention distribution and context vector $h^_t$都和baseline的計算一樣。Note：pgen$\\in[0,1]$(由context vector $h^t$和decoder state $s_t$以及decoder input $x_t$計算而來)而$p{gen}$用來軒則下一個toekn是要generating from vocabulary by sampling from ‘$P_{vocab}$’或是 copying from input sequence by sampling from attention distribution ‘$a^t$’對於每個文檔，’extended vocabulary’表表示詞彙表的聯集，以及所有出現在source text中的單詞。當w 是OOV的單詞，那麼對應的$P_{vocab}(w)=0$，若w 沒有在對應的source document那麼$\\sum_{i:w_i=w}a^t_i=0$產生OOV字是pointer network 的主要優點之一，與之相對的baseline只能夠根據預設的vocabulary dictionary而loss function 跟 baseline 一樣 但相對的P(w)是用這邊有的pgen 來選distributrion Pointer + coverage“重複片段”是S2S的一個常見問題，再生成多序列句子的文本時最為常見。在這裡，多新增一個”coverage vector”$c_t$(sum of attention distributions over previous decoder timesteps)$c_t$是對於source text上的分佈，代表了目前這些單詞從注意力機制中接收到的”覆蓋(重複)“程度。Note $c_0$是”零向量“，因為在t=0的時候沒有覆蓋到任何source text。”coverage vector“是作用於注意力機制的額外輸入$e^t_i=v^Ttanh(W_hh_i + W_ss_t+w_cc^t_i+b_{attn})$$a^t = softmax(e^t)$ This should make it easier for the attention mechanism to avoid repeatedly attending to the same locations, and thus avoid generating repetitive text. 作者說這樣可以避免重複關注相同位置，來避免生成重複的文本(why?)之後再多加上一個loss避免在同一個關注位置：對在同一個地方重複關注加上penality這個loss function &lt;=1，與Machine Translation中的 coverage loss不同，在MT中預設是1-1的翻譯;因此，弱勢coverage vector是&gt;=1或是&lt;=1就會有penality，而在這裡的summary coverage 則不用到1-1(uniform) ，所以作者這邊只對attention distribution中重疊的部分做penality來防止重複關注。最後對這個coverage loss添加權重。 Related Work此方法近似Coverage Embedding Models for Neural Machine Translation 但是與CopyNet有幾處不同 計算了$p_{gen}$，與CopyNet通過共享的softmax函數引發競爭不同。 重複使用了以前出先過的attention distribution，但是CopyNet把他們當成是獨立的。 當在source text有一個單詞重複出現多次，把它相對應的attention distribution相加，而CopyNet沒有。 貢獻（對比CopyNet）： 計算$p_{gen}$能有效的降低.控制生成或是複製單詞的概率，而不是只有提高。 這個方法較簡單。 提出pointer network 經常複製某一個在source text中多次出現得單詞 Copy mechanism 對於準確複製稀有單詞(在字典集內)至關重要 Mixture approach(copy distribution 和 vocabulary distribution) mixture LM+ copynet = abstracvite copying 用attention distribution update coverage 比其他用GRU update 來得有效且簡單(summing the attention distribution to obtain the coverage vector Coverage vs. Temporal attention(作用於NMT)，attention distribution都會處以先前的總和，有效抑制重複注意力，但是破壞力太大，使得信號歪曲並且降低性能，假設coverge比Temporal attention更好，最好是通知注意機制以幫助其做出更好的決策，而不是完全忽略其決策。用相同task比較 coverage比temporal 有較高的ROUGE分數 DatasetCNN/Daily Mail dataset online news (781 tokens avg) multi-sentence summaries(3.75 sentences or 56 tokens avg) Compare with A recurrent neural network based sequence model for extractive summarization of documents. 287,226 training pairs, 13,368 validation pairs and 11,490 testing pairs 直接對原始文本（或數據的非匿名版本）進行操作，認為這是一個值得解決的有利問題，因為它不需要預處理。(不對name entity 做 anonymized version of data) 實驗 256 hidden state 128-dim word embedding Batch size = 16 50k vocabulary for source and target，因為pointer network 可以處理OOV words，原始的vocaulary size 可以較小 因為pointer 和coverage 只需要一點點的額外parameter 沒有pre train word embedding(learn during training) Training using Adagrad(optimizer)：Learning rate = 0.15 ，從0.1開始上升(SGD Adadelta Momentum Adam RMSProp) Maximum gradient norm of 2 不用Regularization 在validation 上用early stop 在training 和test時只使用最大長度400的token size 且限制summary的最大長度=100，實驗發現這樣的刪減可以提升model的性能 在訓練時從被高度刪減的句子開始，然後再提升最大長度。在testing時summary 是透過”beam search”(size=4)產生 Baseline model要訓練較久4d，相對的pointer(作者的)訓練時間較短3d Coverge loss 的$\\lambda$=1，在3000次iter後從coverage loss從0.5下降到了0.2。在$\\lambda$=2時減少了coverage loss 但是使得主要的loss上升了。 Ablation：在沒有loss function下訓練coverage model(但沒有效果)。在第一次就開始coverage而不是將其作為單獨的訓練階段，在訓練的早期階段，coverage干擾了主要目標，從而降低了整體績效。y軸：重複片段出現% Resultseval by standard ROUGE metric F1 scores for ROUGE-1,ROUGE-2 and ROUGE-L(word-overlap,bigram-overlap and longest common sequence between the reference summary and the summary to be evaluated) 使用pyrouge package. METEOR metric eact match mode (rewarding only exact matches between words) full mode (which additionlly rewards matching stems, synonyms and paraphrases). Comparsion完整數據集 Lead-3 (使用文章的前三個詞作為摘要) Abstractive:Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond Extractive:A recurrent neural network based sequence model for extractive summarization of documents. 下面的兩個lead-3會不同是因為一個是anony dataset兩個基準模型在ROUGE和METEOR方面均表現不佳，實際上較大的詞彙量（150k）似乎無濟於事。 即使是性能更好的基準（詞彙量為50k）也會產生帶有幾個常見問題的摘要。 事實細節經常被錯誤地複制，經常用一個更常見的替代詞來代替一個不常見的詞（但在詞彙中）。更具有災難性的是，摘要有時會變成重複的廢話，例如圖1中的基線模型產生的第三句話。此外，基線模型無法複製詞彙外的單詞（例如，圖1中的muhammadu buhari ）。 所有這些問題的更多示例在補充材料中提供。為何Baseline-3會較好也會在後續討論 Why Lead-3 betterextractive 會比abstractive有較高的ROUGE分數 新聞文章往往在一開始就以最重要的信息為結構。 這部分解釋了Lead-3的強度。 實際上，僅使用文章的前400個tokens（約20個句子）會比使用前800個tokens產生更高的ROUGE分數。 參考摘要的內容選擇非常主觀，有時要用句子做成一段獨立的摘要，其他時候只是抓取一些細節。給一定數目的句子有不少有用的方法選擇３到４種重點。而Abstrative引入了更多可能選擇（choice of phrasing），進一步降低了與參考摘要匹配的可能 僅具有一個參考摘要會加劇降低ROUGE的這種靈活性，與多個參考摘要相比，它已降低了ROUGE的可靠性(example：”smugglers profit from desperate migrants”也是一種可代表的abstractive summary(對第一個article但是他的ROUGE=0) 由於任務的主觀性以及有效摘要的多樣性，ROUGE似乎對諸如選擇先出現的內容或保留原始措詞之類的安全策略給予了獎勵。 Model 有多 Abstractive 我們最終模型的摘要包含的新n-gram（即那些未出現在文章中的n-gram）的比率比參考摘要要低得多，這表明抽象程度較低。 65%包含了一系列的abstractive techniques：文章的句子被截斷以形成語法上正確的較短版本，而新的句子則通過將片段拼接在一起而構成。 有時在復制的段落中會省略不必要的感嘆詞，子句和括號內的短語。 圖中表示兩個有相似結構的abstractive example。由於數據集包含很多體育類新聞，其摘要格式通常是“X beat Y ⟨score⟩ on ⟨day⟩”，而這是這一個model最有信心的abtractive summary。 但是，總的來說，我們的模型不會像圖7常規地生成摘要，並且也不像圖5那樣接近生成摘要。 4.可以用$p_{gen}$衡量抽象性：在訓練過程中，$p_{gen}$從大約0.30的值開始，然後增加，到訓練結束時收斂到大約0.53。這表明該模型首先學習大部分複制，然後學習生成大約一半的時間。但是，在測試時，$p_{gen}$嚴重偏向複製，平均值為0.17。差異可能是由於這樣的事實，即在訓練過程中，該模型以參考摘要的形式接受逐字監督(Teacher forcing)，但在測試時卻沒有。但還是有用的：例如句子的開頭，縫合在一起的片段之間的連接以及產生截斷被複製句子的時間段時，$p_{gen}$最高。 Conclusion Hybrid on pointer generator architecture with coverage reduce repetition long-text dataset with high solution （1）在模型训练到一定程度后，再使用Coverage Mechanism。否则模型容易收敛到局部最优点，影响整体效果。 （2）传统Attention机制的基线模型包含21,499,600个参数，训练了33个epochs。本文模型添加了1153个额外的参数，训练了12.8个epochs。所以合适的模型不但效果好，而且快。 （3）在模型的训练环节，刚开始的时候，大约有70%的输出序列是由Pointer Network产生的，随着模型逐渐收敛，这个概率下降到47%。然而，在测试环节中，有83%的输出序列是由Pointer Network产生的。作者猜测这个差异的原因在于：训练环节的decoder使用了真实的目标序列。 （4）虽然Generator Network生效的概率不高，但是其依旧不可或缺，例如在下面的几个场合，模型有较大的概率会使用Generator Network：在句子的开头，在关键词之间的承接文本。 （5）在摘要任务中，适当地截断句子反而能产生更好的预测效果，原因在于这篇论文用的语料是新闻语料，而新闻语料经常把最重要的内容放在开头。 （6）作者曾尝试使用一个15万长度的大词表，但是并不能显著改善模型效果。 Appendix補充材料本附錄提供了測試集中的示例，並與參考摘要和我們的模型產生的摘要進行了並行比較。 在每個示例中：•斜體表示詞彙外的單詞•紅色表示摘要中的事實錯誤•綠色陰影強度表示生成概率pgen的值•黃色陰影強度表示最終模型匯總過程結束時coverage向量的最終值 參考網址https://medium.com/nlp-tsupei/pointer-generator-network-5a5a3a2bce3https://github.com/atulkum/pointer_summarizer/tree/master/training_ptr_genhttps://zhuanlan.zhihu.com/p/22993927","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP Pointer Summary","slug":"NLP-Pointer-Summary","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Pointer-Summary/"}],"author":"PoH_Ko"},{"title":"Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques","slug":"Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques","date":"2020-10-22T10:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/10/22/Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/10/22/Supervised%20Topic%20Modeling%20Using%20Word%20Embedding%20with%20Machine%20Learning%20Techniques/","excerpt":"","text":"Supervised Topic Modeling Using Word Embedding with Machine Learning Techniquestags: NLP,Topic Modelingieee論文網址 Abstract基於 HMM model做出的 supervised embedding達成分類問題貢獻：單詞含義和順序對於topic model的重要性。使用LSTM和CNN達到SOTA Introduction非結構化的文本資料到處都有。儘管文本中包含了豐富的訊息，為了利用這些資料，必須先根據內容組織，理解和總結。需要通過新的統計模型分析這些數據，且速度必須要快。 Topic modeling使用 supervised 或是 unsupervised 的統計機器學習技術來處理大型語料庫。 與unsupervised的方法相比較，supervised可以節省研究者大量時間。 而unsupervised的方法在於可以處理大多dataset，但代價是其準確性和更長的訓練時間 LSA和LDA分析文件中的單詞，已發現貫穿他們的主題以及這些主體如何與另一個主題聯繫，但是由於缺少對於特定task的優化，缺少task-specific feature。而後來有word embedding可以用來fine tuning on task。此外，這個model可以使用單詞結構，含義和順序，因為由HMM和RNN組成，可以分辨出數據序列。當雨word embedding一起使用時，可以帶別單詞的含義。 本文提出了一種新的主題建模方法，該方法考慮了語義（單詞的含義）並具有句子中單詞順序的感覺。 Model在訓練時，使用”分佈式表示(就是embedding i.e.Word2vec and Glove)”來訓練模型，這些表示允許具有相似含義或語法特性的單詞具有相似向量。 Related WorkGenerative Models(機率) 用來抓取全局的語意一致性的model，例如LDA使用Dirichlet distribution，假設每一個文件都是主題的機率分佈，每一個主題都是文件中單詞的機率分佈，使用conjugate prior 可以從小型到大型數據之間衍生。 主題關聯模型(CTM)，解決LDA無法對建構的主題考量其關聯性的問題。使用normalize log normal prior，可以抓取成對的主題相關性，使用stochastic variational inference。其增強版本：Efficient CTM，可以透過比較topic vector的相近成度，判斷相關性。 Gaussian-LDA假設文件的輸入是一系列的word embedding而不是系列的單詞類型。 最近較新的方法(DocNADE,an extension of NADE)，從未標記的文件中學習有意義的文件表示，並以bows表示文件。 Using Word Embedding TWE通過使用lantent topic model，給文件中的每一個單詞分配主題，TWE通過使用NN來同時學習topic 和word。 BoE(Bag-of-Embeddings)，通過給定文件的指定主題最大化其單詞的embedding probability來預測其主題。 ToWE(Task-oriented Word Embedding)，學習與給定的task相關的word distribution representation。著重於結合單詞的語意和特定任務的特徵。透過regularize突出詞的分佈，使其具有明確的分類邊界獲取task specific feature，丙且調整空間中的其他詞的分佈。 GMNTM：使用文件中單詞的語意和順序，且作為另一個方向：使用Neural probabilistic methods，例如RNNLM。 Machine Learning NMF，關注主題一致性（有意義的主題），數入室Bow並產生兩個較小的矩陣，文件到主題矩陣和單詞到主題矩陣，相乘後生成Bow矩陣 NNDSVD，為NMF得增強版，使用SVD以其向量初始化NMF，對於sparse data特別有效(i.e. text) Ensemble learning strategy：基於NFM，結合一堆ML方法：SVM,KNN,CNN LRP，用以識別文件中的相關單詞。 ctx-DocNADE：DocNADE和LSTM，學習補充語意的功能。 LTMF：unsupervised透過使用LSTM和LDA WORD EMBEDDING AND MACHINE LEARNING MODEL ARCHITECTUREWord Embedding Models Word2Vec：CBOW or Skip-Gram Glove：Count-based，計算單詞在上下文中出現的頻率來建造co-matrix，大型矩陣要通過分解來降維。 Machine Learning HMM：一種圖形模型，預測順序的hidden state。不會觀察到狀態，但是可以透過使用前－後遞algorithm來推斷state CNN RNN：GRU用在小型dataset，LSTM用在大的 Proposed MethodWord Embedding and HMM使用word embedding替代word。然後為每一個topic建構一個HMM。在dataset中有20個topic所以有20個HMM model，看哪一種state和Gaussian Mixture會給出較好的結果，在每一個state$state_1,…,state_{10}$，每一個GM都會進行測試$GM_1,…,GM_{10}$ Word Embedding and CNN圖太大去看原論文 Word Embedding and RNN-CNN圖太大去看原論文 Experimentaldataset：20NewsGroup dataset(分類)18846 document分為20個topic60% for training 40% for testing。使用word2vec(google)並且在google news dataset pretrain，每一個都是300維。 Discussion值得注意的是，與基於LSTM-CNN的模型經過訓練以找出最有區別的特徵並同時捕獲上下文相比，使用非歧視性算法訓練的基於HMM的模型提供的結果較差。 Conclusion and Furure work提出supervised topic modeling，抓取單詞相關的global semantic meaning。獲取單詞序列和局部結構，就Topic modeling而言，純粹的CNN會比RNN好，將兩個合在一起會更好。Future work：跟訓練在特定領域的word embedding和通用的word embedding比較。比較右到左dataset的各類model的性能，嘗試與unsupervised 獲得可比較的結果。 為啥你能上IEEE","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP Topic_Modeling","slug":"NLP-Topic-Modeling","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Topic-Modeling/"}],"author":"PoH_Ko"},{"title":"Supervised Understanding of Word Embeddings","slug":"Supervised Understanding of Word Embeddings","date":"2020-10-22T10:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/10/22/Supervised Understanding of Word Embeddings/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/10/22/Supervised%20Understanding%20of%20Word%20Embeddings/","excerpt":"","text":"Supervised Understanding of Word Embeddings[TOC] tags: NLP,Embedding論文網址 Introduction預訓練的word embedding被廣泛用於自然語言處理中的轉移學習(transfer learning) 由於w2v是在unsupervised下學出來的，其維度無法很好與task做mapping，所以通常會再多加上一層linear(dense)來map到對應的問題，雖然這最後一層是optimized by task 但是這一層不那麼有解釋性：在這一層dense下，某一些維度可以被視為近似而有一部分的dimension則在有稀有單詞出現時才被激活，在使用LSTM或是CNN時，這一個現象更難被解釋，且在做完這一步之後很難transfer learning到不同的task上，因為已經被optimized過了。在此文章中，研究有意義的投影，透過使用關鍵詞(supervision)的label來找出”supervised dimensions”（有意義的projection） example：投影到兩個 supervised dimension(sports,animal)：SD1 trained for keywords related to sports word &quot;bat&quot; is closer to &quot;ball&quot; in this dimension compared to the projection of word &quot;fly&quot;. On the other hand, SD2 is trained for keywords related to the animal category. Hence for this dimension, projections of &quot;bat&quot; and &quot;fly&quot; are closer to each other compared to &quot;bat&quot; and &quot;ball&quot;. Keeping dimensions aligned in a semantically coherent way Obtaining groups of semantically similar words(semantic dictionary) Each supervised dimension can be treated independently and does not influence the order of words in any other projection(因為沒有normalized) 根據字的機率做排序有機會可以進一步完善 semantic dictionary on target topic 在這種情況下的dictionary 可以被單個向量所表示 由於基於主題所建構的相似性模型可進一步用於在語義表示空間中獲得document representation 通過與另一種語言的embedding做對齊，可進一步或的該語言的semantic dictionary 而不必再重新訓練 supervised dimensions(因為是連續性的結構，可以很輕鬆的轉換到不同的task)例如，使用這些線性投影的學習權重來初始化高階網絡的可訓練層 在各種網絡配置中，supervised dimensions可以提高F1 score Related WorkEmbedding Word vector Contextual vectorW2V + Dense傳統的model都是word embedding + dense(轉到高維度):會使得這些embedding在特定部分被激活，這些激活的詞可被視為主題或是字典集，但是這一功能沒有明確的可控制性（解釋性）。要有控制性的話，可以針對任務主題去增加或刪除單詞（選擇字典集）但是這樣做很耗時。 基於word embedding的半自動管理方式：將新的關鍵字自動推薦到術語擴展字典集。SetExpander: End-to-end term set expan- sion based on multi-context term embeddings 在此文的設置中，使用一個簡單的線性邏輯分類器來訓練 feature。 在這項研究中，研究了這些投影的不同性質以及如何使用在高階網絡中投影中 Topic models參考網址相關：topic embedding,LDA2vec Topic model(train on specific corpus) not optimized for final task。For example：medical words grouped into one topic，but cardiovascular diseases(心血管疾病)不會有一個獨立的topic。換句話說，要控制model針對特定的性質有一個主題是非常困難的。 Interpretable dimension 對unsupervised 降低維度透過AE or PCA，在此文中側重利用supervision 來提取出 interpretable dimensions 像圖中所示，通過訓練二元分類器來使用詞嵌入的線性投影，而這個二元分類器透過使用主動式監督學習(on normalized vector:確保分類器有相似結果在cosine similarity的分數很高時，因此在相似主題(positive word)有較高的分數) 由於正規化的詞向量位於超球面上，因此每個分類器在幾何上對應於一個hypercap。Step Logistic regression as classifier model(其他線性分類model也行) Initial stage:positive keywords(same topic) are provided Randomly sampled negative words are added to train a binary logistic regression. 其餘的字透過使永logistic regression來找出結果 排序的字中最高分數的加入初始的postive”關鍵字集” 重複3到5for few iteration來得到最終版本的分類器 通過添加 positive或negative關鍵字來管理(enrich)topic base projection and generalize level projection Applications of Supervised Dimensions 使用scikit-learn線性logistic回歸模型（正類權重為2）來增強positive word的效果。 在L2 normalization之後，使用了Fasttext Common Crawl詞嵌入的前250k個詞 Transfer Learning 用易於獲得的大型數據集學習部分或是整個NN的weight，ie.Language model，其後對應於特定的task來fine-tuned這一個task-specific loss functions Model:Concatenated transformed embedding from unsupervised and supervised dense layer + multi-layer BiLSTM 訓練好的logistic regression(keyword classifiers) 在network中被當成是nodes用來取代在dense layer中的權重。 Activation function可以更改，因為線性的初始化(Supervised)為後續的訓練提供了重要的基礎，允許這些預先訓練好的權重(Embedding and Supervised)可以被更改可以增加效能，除了原先訓練好的supervised dimensions，再加上隨機初始化的unsupervised dense可以增加準確度，dropout用來regularize這整個structure Task:Identification on smoking status(label：”previous smoker”, ”non-smoker”, ”current- smoker” and ”unknown”)通過某些關鍵字選擇相關的句子，從報告的文字中判斷吸菸狀態 NLTK tokenizer F1 score to assess performance Standard BiLSTM 在embedding後面接上unsupervised dense 3846個 example 5-fold cross-validation with 20 repetitions and reported the average for each configuration Adam optimizer，lr=$10^{-3}$像是圖中所示在embedding後面加上dense 可以增加performance，此外，初始化過的linear keyword classifier提供了額外的(supervised)performance The dense layer activations in this experiment can be precomputed since every word corresponds to a unique value. Thus our method can be considered as augmentation of word embedding dimensions via informative interpretable projections. Dictionary Curation(詞典管理)透過之前描述的supervised dimension，可以直接找出topic-specific dictionary(藉由上一節描述的embedding database的預測結果來做到)，可以設定threshold來取得完整的word list，可以用來找尋關鍵字(KS)或是實體識別(NER)任務 ✩這裡有一個主要的問題：要如何準確的判斷未被標記的部分被線性分類器正確的分對$\\quad$對這些分類器進行評估，保留了10%帶有註釋的關鍵字以用來進行驗證對３０個不同的分類器(based on current data)計算(micro,macro)平均的 F1 score，就結果表明此方法可以用於創建具有”高度連貫性結構”??的topic-specific dictionary Dictionary Curation on Multiple Languages(subtask)在沒有重新訓練分類器來判斷不同語言的主題性(其他語言的字典)可以使用一種語言訓練出來的分類器可以在多種語言上執行作法： 在基礎的word embedding上對齊 證明方法： 預先訓練好一個分類器(用”smoking”,”smoker”,”tobacco”當成positive) 並用 German(德語) 和Dutch(荷蘭語)對齊 word embedding 排序(base on probability) 用supervised dimension 可以減少將模型縮放到多種語言 Polysemy Representations(多意義表示) Different projections of the same keyword contain different meanings in the word vector space. In other words, cosine similarities among words are different in sub-spaces. This can be verified by training specific supervised dimensions with a few keywords.同一個關鍵詞在空間向量中的不同投影中有不同的含義換句話說：一個單字在子空間中的cosine similarity是不一樣的。可以使用一些關鍵字訓練特定的supervised dimension來驗證 實驗：用３個positive keywords 並且在supervised dimension上隨機取3個negative sample。圖中顯示了對詞彙表中其餘單詞投影的前8個在左半邊，選定keyword，第一個play 代表了 baseball-related的concept(用play+run+bat)，而第二個代表了art。兩個都可以與play相關，且有”play”這個字在內。當在數入文本中出現了play這一個字，這兩個相關的supervised dimension會被activate，代表了它具有多義性的功能。同樣的bat在”animal”vs.”play”有不同的最高得分的單詞。 Interpretation of Supervised Dimensions說明supervised dimensions 的可解釋性 Activated words in Dense layer outputs為了檢測supervised dimension的相關性，研究了dense layer的outputs且蒐集了可以觸發各種supervised dimension的字典集。使用在吸菸狀態的識別實驗中訓練好的分類器以及相同網路架構。從dense layer(supervised)選擇了三個相關的dimensions(initializaed weight learned from classifier)且對這些dimensions找出分數最高的幾個字。同樣的，從random initialized(unsupervised)的dense layer 從三個output nodes選出分數最高的幾個words 有趣的是，即使在經過fine-tuning之後，這些維度也大多保留了原本對主題訓練的特性，與之相對的unpuservised dimension在這些單詞之間則無連貫性的結構。Toy example from mtsamples for word “chest”，使用了另一個dataset中的unique words且激活值小於5的沒有被分配到任何分類器，且大多是由停用詞(stopword)組成。","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP Embedding","slug":"NLP-Embedding","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Embedding/"}],"author":"PoH_Ko"},{"title":"Towards Making Unlabeled Data Never Hurt","slug":"Towards Making Unlabeled Data Never Hurt","date":"2020-10-02T10:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/10/02/Towards Making Unlabeled Data Never Hurt/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/10/02/Towards%20Making%20Unlabeled%20Data%20Never%20Hurt/","excerpt":"","text":"Towards Making Unlabeled Data Never Hurttags: ‘SSL’, ‘SVM’, ‘semi-supervised learning’, ‘dual voting’原論文在此 Brief introduction What problem was studied?In this paper, the authors focused on improving the safeness of S3VMs. (Remark: S3VM is a semi-supervised learning approach based on SVM.) Safe, here means that the generalization performance is never statistically significantly worse than methods using only labeled data. Why was the study undertaken?Nowadays, semi-supervised learning has become an important issue. However, sometimes semi-supervised leaning performs worse than the supervised learning. So, it’s desirable to research on the safeness of semi-supervised learning. Preliminary inductive learning訓練時testing data和unlabeled data是分開的。 transductive learning訓練時testing data就是unlabeled data。（當有新的unknown data進來的時候，要全部重train） S3VM: $B$ is a set of label assignments obtained from domain knowledge.${\\bf y}={y_{l+1},…,y_{l+u}}$ MethodsIn this paper, the authors first proposed two simple approaches: S3VM-c and S3VM-p. However, they found some drawbacks of S3VM-c and S3VM-p. Thus, S3VM-us and S4VM were designed. The main contribution of this paper is S4VM. How was the problem studied? S3VM-us:For S3VM-us, the authors provide the safeness S3VM approach, S3VM-us, by adding the “confidential unlabeled instances” during the learning process. S4VM: (the main contribution in this paper)For S4VM, the authors re-examine the foundamental assumption of S3VMs. Notation: a set of $l$ labeled instances ${x_i,y_i}_{i=1}^{l},$ where $y_i\\in {+1,-1}$ a set of $u$ unlabeled instances ${x_j}_{j=l+1}^{l+u}$ $f： X \\longrightarrow {+1,-1}$ $D = ({x_i,y_i}{i=1}^{l},{x_j}{j=l+1}^{l+u})$S3VM-c 演算法：* 先決定k，之後用K-means clustering將$D$裡的data分群。 分完群後，對每一群： 對群內的所有的點，分別計算「用SVM, S3VM的classifiers $f$」 得到的值。 計算bias (目的：分別去看SVM, S3VM對這群label的看法) 計算confidence （目的：看SVM, S3VM對這群的label標註的信心有多大） 如果S3VM和SVM對某群的label的想法一致（bias）、且S3VM的信心極大，才使用S3VM的看法，不然就使用SVM的看法。 Remark: k-means clustering 個人看法：因為目的是要解決safeness的問題，因此只有當「semi-supervised learning的方法」和「supervised方法」看法一致，且很有信心時才加進去。用這樣的方法來確保s「emi supervised learning的方法做出來比supervised learning好」。 weakness of this method? Q: Why do we need $c$ here? S3VM-pRemark: S3VM-p is motivated by theconfidence estimation in label propagation methods. S3VM-c, S3VM-p問題： However, they both suffer from some deficiencies. S3VM-c works in a local manner and the relations between clusters are never considered. In S3VM-p, as stated in [41], the confidence estimated with label propagation methods might be incorrect if the label initialization is highly imbalanced. Moreover, both S3VM-c and S3VM-p heavily rely on S3VM predictions. This might be risky when S3VM suffers from a serious reduced performance. S3VM-us考慮到需要加入clusters彼此之間的關係資訊、降低對label initialization起始值的敏感度，作者提出S3VM-us。 補充： (只是示意圖) single linkage method (Hierarchical clustering)圖片來源 If $x_j$ is closer to $n_{j-1}$, then $n_{j-1}&lt;p_{j-1}$.i.e. $t_{j-1}&lt;0$. If $x_j$ is closer to $p_{j-1}$, then $n_{j-1}&gt;p_{j-1}$.i.e. $t_{j-1}&gt;0$. Ｑ：為何這樣可以降低sensitivity to the label initialization?Ｑ：為什麼需要step 5? S4VMS3VM-us跑出來的實驗結果，都和S3VM的差距不大。（Why?) As previously mentioned, the underlying assumption of S3VMs is low-density separation. That is, the ground-truth is realized by a large-margin low-density separator. However, as illustrated in Fig. 1, given limited labeled data and many more unlabeled data, there usually exist multiple large-margin low-density separators. 因此，作者詳細考慮了S3VM的想法，並提出了S4VM。 However, as illustrated in Fig. 1, given limited labeled data and many more unlabeled data, there usually exist multiple large-margin low-density separators. S4VM演算法主要有兩步驟： 找出T組large-margin low-density separators. 再找出${\\bf y}$使得對上面找出的T組separators，ac最大的。 第1.步驟： 透過計算以下最佳化去找出T組separators以及其labeling。 而在論文中，作者提出兩種計算上面的最佳化（14）的方法-Global Simulated Annealing Search(實驗數據中的S4VMa)以及Representative Sampling(實驗數據中的S4VMs)。 第2.步驟： 期待找出使得和真正結果進步最多的那組y去當實際的label。 但因為沒有實際的label，因此透過第1.步驟找出的separators去當作label。 Results What were the findings?強調比supervised SVM好的穩定度。 （Ｑ：雖然比S3VM穩定，但有些看起來S3VM的結果較好？） S4VMa: S4VM using simulated annealing S4VMs: S4VM using sampling Discussions What do these findings mean?結果顯示，S4VM較S3VM穩定。 Related work T. Joachims, “Transductive Inference for Text Classification Using Support Vector Machines,” Proc. 16th Int’l Conf. Machine Learning, pp. 200-209, 1999. Questions in this paperMethods:Q: Why do we need $c$ in S3VM-c?","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SVM/"},{"name":"SSL","slug":"SSL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SSL/"},{"name":"Semi supervised learning","slug":"Semi-supervised-learning","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Semi-supervised-learning/"}],"author":"Puchi"},{"title":"LSTMEmbed- Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories","slug":"LSTMEmbed_ Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories","date":"2020-08-28T14:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/08/28/LSTMEmbed_ Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/08/28/LSTMEmbed_%20Learning%20Word%20and%20Sense%20Representations%20from%20a%20Large%20Semantically%20Annotated%20Corpus%20with%20Long%20Short-Term%20Memories/","excerpt":"","text":"論文網址 問題：一字多義在傳統的word2vec並不能很好的表示利用LSTM從語意註釋的corpus中學習word representationAPI:WordNet,BabelNet,Freebase Abstract對於人來說：一字多義（模糊）不是問題，因為常常使用上下文以及語言常識來判斷意義與上下文的落差。因此這個模型想要做的是能在一定程度上處理語言模糊問題。 原本的Word2vec：在同一個空間中把所以有一字多義的意義放在同一個vector中(每一種詞的各種含義混在一起，最常出現的representation優先)。 context embedding(GPT2,Elmo)是根據上下文，而可能把同一個意義的字map到不同的空間。 多語言詞彙語義：這一篇論文主要目的是把相同的詞義的不同詞map到相同的word2vec在同一空間中，同字不同義map到不同的vec(所以training是by word annotation) 在解決解決詞彙多義性問題的一連串工作提出了意義嵌入的創建，即將單詞中每個單詞的不同意義分開embedding。缺點之一：沒有考慮詞語順序。而在基於RNN考慮字詞結構的embedding在速度和品質方面較差。 在圖中 word vector 和sense vector是分開的ex:首先，模棱兩可的單詞“ bank”位於與其同時出現的單詞附近（圖中的正方形），其次，最接近的銀行含義（點代表金融機構的含義，並與 它的地理含義）聚集在兩個分開的區域中，而與與此相關的詞（可能含糊不清）沒有明顯的關聯。 Main idea主要想法： ★用biLSTM考慮到word ordering 把pre-train的embedding當成training objective LSTM不只學習上下文資訊，亦可以表示各個單詞的representation 以及sense 與已經存在的knowledge resource結合，可以利用先前就已經得到的semantic information Embedding for words and senses(related work)Word Embedding Glove train on word-word co-occurences, good for single word；however fail to represent non-dominant ‘sense’ of word.(Glove只能學到很常出現的字意) 另一個問題： bar和pub，bar和stick 要相同，但是pub和stick應要無關 (word sense disambiguous) Solve this issue To make more similar for same word type,vice versa.：Word type extract from PPDB or WordNet Context2vec learning sentence and word embedding(large raw text corpora) Base on word2vec ：embedding extracted from the output topmost weight matrix(使用output matrix as embedding而不是與input最接近的) Sense Embedding與上述每種方法旨在學習詞彙表述(word2vec)的方法相比，sense embedding將各個詞義表示為單獨的向量。 使用額外API：embedding的主要訓練方法是根據knowledge base，依賴預先定義好的model，i.e.:WordNet,BabelNet,Freebase. SensEmbed：’SOTA of WSD and NE’： 一個用於詞義消除歧義和實體鏈接的先進工具，用於構建詞義標註的語料庫，而該語料庫又可用於使用word2vec訓練詞義的向量空間模型。’SenseEmbed using BabelNet &amp;distributional information from text corpora’。 Spliting the vector which came from pretrained word embedding into their respective senses ★AutoExtend:延伸word2vec，在學習過程加上了wordnet的資訊:(words are sums of their lexemes and synsets are sums of their lexemes.)(AE架構)參考網站作者認為單詞集合W和synset集合S之間存在這一種線性映射關係 S =E⊗W $\\hat W$=D⊗S於是這個就可以很自然看成一個encode和decode的過程了，損失函數自然是讓decode後的結果和encode前的一致。不使用額外API：Contexualize embedding(不使用額外的API)在不使用額外API時，在訓練之後所表示的這些向量僅被識別為彼此不同，而沒有清楚地識別其內在含義，所以要額外的上下文資訊。 每一個單字都train多個代表的vector(by上下文representation cluster pool) 在訓練w2v過程，如果一個單詞所訓練出來的向量和之前的差很多，就為該單詞延伸一個新的sense vector ELmo：作者表示雖然是train by bilstm，但是每個token由三個向量表示，其中兩個是上下文向量。 這些模型通常由於缺乏與詞義語義資源(babelnet…)的聯繫而難以評估。 LSTMEmbed旨在學習單詞sense representation，並與諸如BabelNet的多語言詞彙語義資源鏈接，同時處理’word ordering’並使用預訓練好的embedding作為目標 。 ModelSense tagged input $s_i$這個可能是一個單字或是word sense(from inventory ex:BabelNet) 主要架構圖★ 在這裡的訓練過程當成是w2v的CBOW 左半邊 因為是雙向lstm 對特定單字$s_i$，其embedding包含了$s_{i-W},…s_{i-1}$以及$s_{i+1}…s_{i+W}$ 每一個$s_i$都有其對應的embedding(by lookup table) $v(s_i)\\in R^n$ 進入LSTM後 Concate 2個LSTM output丟入一層的Linear layer 得到左半邊的LSTMembed 右半邊 用來與左半邊得出來的$out_{LSTMEmbed}$比較，$emb(s_i)$是使用pretrain好的embedding vector 根據使用的預訓練數據集和註釋，這個$s_i$可以是單字或是word sense 而最後面的objective loss則是cosine similarity Once the training is over, we obtain latent semantic representations of words and senses jointly in the same vector space from the look-up table, i.e., the embedding matrix between the input and the LSTM, with the embedding vector of an item &apos;s&apos; given by v(s)在訓練完後，使用lookup tabel的embedding，在相同的向量空間中共同獲得單詞和sense的 semantic representations。 和一般bi-lstm不同處 使用帶有註釋的語料庫（包括單詞和word sense）來train。 從單個lookup table中的單詞和感官的embedding形式，在左右兩個不同方向的LSTM之間共享 一種新的學習方法，它以一組預訓練的embedding為目標，這使我們能夠學習大詞彙量的embedding Implement detailsense-level tasksword level tasks Training data Sense inventory: BabelNet(大型多語言百科全書和semantic網絡，包括大約1600萬個通過語義關係鏈接的概念和命名實體條目) Training corpus:BabelWiki(包含英語維基百科的多語言語料庫，使用Babelfy自動標註了命名的實體和概念) 30億個tokens和約300萬個unique tokens Embedding setting look-up table : 200-dim 丟棄前1000個最常出現的token batch size : 2048 1 epoch Optimizer : Adam or AME(Adaptive Moment Estimation) $emb(s_i)$ : 400-dim，使用w2v的SkipGram window_size=10，negtive sampling=10，sub-sampling of frequent words set to $10^3$ ExperimenSense evaluation第一組實驗旨在展示模型在需要語義（而不僅僅是詞彙）相關性的任務中的影響。 我們分析了兩個任務，Cross-Level Semantic Similarity(跨級別語義相似性)和Most Frequent Sense Induction Cross-Level Semantic Similarity(word-to-sense) 為了最好地評估embedding以區分單詞的各種含義的能力，選擇了SemEval-2014的跨級別語義相似性任務其中包括單詞間的相似度作為其子任務之一: CLSS單詞感知相似性數據集包含500個單詞，每個單字與WordNet候選單詞的簡短列表配對，並對其進行人類評分其semantic similarity。 為了計算單詞間的相似度，使用了lookup table的embedding space，並使用cosine similarity來計算相似度。 MeerkatMafia:which uses Latent Semantic Analysis and WordNet glosses to get word-sense similarity measurementsSemantiKLU :an approach based on a distributional semantic model trained on a large Web corpus from different sourcesSimCompass : which combines word2vec with information from WordNet. Most Frequent Sense InductionWSD system comparsion:counting the word sense pairs in an annotated corpus such as SemCor94比較爛… Word-based Evaluation基於單詞的評估，目的是證明模型能夠解決傳統上使用基於單詞的模型處理的任務 Synonym Recognition給定目標詞和一組替代詞，此任務的目的是從集合中選擇與目標詞含義最相似的成員(考試囉) Dataset: synonym questions of the TOEFL &amp; ESL 單詞相似性任務WordSim353根據不同預訓練embedding與LSTMEmbed的餘弦相似度之間的Spearman相關性來計算LSTMEmbed的性能。 Conclusions 這是一種基於雙向LSTM的新模型，用於聯合學習單詞和word sense的 embedding 更好地學習semantic representation semantic representation能夠正確反映單詞和意義表示之間的相似性，在word-to-sense以及most frequent sense induction有不錯表現。 還能夠在基於標准單詞的語義評估中有不錯表現","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"PoH_Ko"},{"title":"Transfer Learning without Knowing - Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources","slug":"Transfer-Learning-without-Knowing-Reprogramming-Black-box-Machine-Learning-Models-with-Scarce-Data-and-Limited-Resources","date":"2020-08-21T17:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/08/21/Transfer-Learning-without-Knowing-Reprogramming-Black-box-Machine-Learning-Models-with-Scarce-Data-and-Limited-Resources/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/08/21/Transfer-Learning-without-Knowing-Reprogramming-Black-box-Machine-Learning-Models-with-Scarce-Data-and-Limited-Resources/","excerpt":"Transfer Learning without Knowing - Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources論文網址ICML 2020black-box adversarial reprogramming (BAR)Using zeroth order optimization and multi-label mapping techniques, BAR can reprogram a blackbox ML model solely based on its input-output responses without knowing the model architecture or changing any parameter BAR also outperforms baseline transfer learning approaches by a significant margin, demonstrating cost-effective means and new insights for transfer learning.","text":"Transfer Learning without Knowing - Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources論文網址ICML 2020black-box adversarial reprogramming (BAR)Using zeroth order optimization and multi-label mapping techniques, BAR can reprogram a blackbox ML model solely based on its input-output responses without knowing the model architecture or changing any parameter BAR also outperforms baseline transfer learning approaches by a significant margin, demonstrating cost-effective means and new insights for transfer learning. introductionIn this paper, we revisit transfer learning to address two fundamental questions:(i) Is finetuning a pretrained model necessary for learning a new task?(ii) Can transfer learning be expanded to black-box ML models where nothing but only the input-output model responses (data samples and their predictions) are observable? Indeed, the adversarial reprogramming (AR) method proposed in (Elsayed et al., 2019) partially gives a negative answer to Question (i) by showing simply learning a universal target-domain data perturbation is sufficient to repurpose a pretrained sourcedomain model.But in AR it requires backpropagation of a deep learning model with doesn’t address Question(ii) For bridge this gap, we propose a novel approach named black-box adversarial reprogramming(BAR), to reprogram a deployed ML model for black-box transfer learning.The following are the substantial differences and unique challenges: Black-box setting : In AR method assumes complete knowledge of the pretrained model. Data scarcity and resource constraint : For the medical data, it may be expensive and clinical trials, expert annotation or privacy-sensitive data are involve. And for the limitation with the large commercial ML model. In the experiment, BAR can leverage the powerful feature extraction capability of black-box ImageNet classifiers to achieve high performance in three medical image classification tasks with limited data. Main purpose of BAR(black-box Adversarial Reprogramming)To adapt the black-box setting, we use zeroth-order optimization in black-box transfer learning. Also use multi-label mapping of source-domain and target-domain labels to improve the performance in BAR.The following are the main contribution: BAR is the first work that expand transfer learning to the black-box setting. Evaluate with three medical dataset in BAR.(1) autism spectrum disorder (ASD) classification;(2) diabetic retinopathy (DR) detectionl;(3) melanoma detection. The result show that our method consistently outperforms the state-of-the-art methods and improves the accuracy of the finetuning approach by a significant margin. Use the real-life image-classification APIs from Clarifai.com for demonstrating the practicality and effectiveness of BAR. Related workAdversarial reprogramming (AR) is a recently introduced technique that aims to reprogram a target ML model for performing a different task.(Elsayed et al., 2019)Different from typical transfer learning that modify the model architecture or parameters for solving a new task with target-domain data, AR remains the model architecture and parameters unchanged.Zeroth Order Optimization for Black-box Setting : In vanilla AR method, it lacks the ability to the access-limited ML model in prediction API and the backpropagation in the model. So we use Zeroth Order Optimization for Black-box AR for transfer learning. Black box Adversarial Reprogramming(BAR) : Method and Algorithm(1)setting :$F(x), \\forall x \\in X$ and the $\\nabla F(x)$ is in admissible.Let $X=[-1,1]^d$, $d$ is the (vectorized) input dimension.Target domain by ${D_i}^n_{i=1} \\in [-1,1]^{d’}$, $d’&lt;d$ and for each data sample $i \\in [n]$, where [n] denotes the integer set ${1,2,…,n}$.And let the $X_i$ be the zero-padded data sample containing $D_i$, embedding a brain-regional correlation graph of size $200 \\times 200$ to the center of an $299 \\times 299$The transformed data sample for BAR is defined as : $\\tilde X_i = {D_i}_{padding} + P$, where $P = tanh(W \\odot M)$, Trainable parameters : $W \\in \\mathbb{R}^d$, Binary Mask : $M\\in\\mathbb{R}^d$, $M_j=0$ means the area is used for embedding $D_i$ (2) Multi-label mapping(MLM):For AR, we need to map the source task’s output labels(different objects) to the target task’s output labels(ASD/non-ASD). Muiltiple-source-labels improve the accuracy is more than one-to-one label mapping.We use the notation $h_j(*)$ to denote m to 1 mapping functionFor example : $h_{ASD}(F(X)) = [F_{a}(X)+F_{b}(X)+F_{c}(X)]/3$More generally : $S \\subset [K]$ map to the target $j \\subset [K’]$, then $h_j(F(X)) = \\frac{1}{|S|}\\sum_{s\\in S}F_s(X)$, where $|S|$ is a cardinality set.And a frequency-based label mapping is better than randomly chose. (3) The Loss function of AR:Since for the softmax function, we get the model output with $\\sum^K_{j=1}F_j(X) = 1$ and $F_j(X) \\geq 0, \\forall j \\in [K]$.For training the adversarial program P parametrized by W, Used the focal loss to imporve the performance in AR/BAR.Since the focal loss is to focus on the hard example. small talk in focal loss:easy example : prediction 高的hard example : prediction 低的 在訓練的時候常常都會太關注在easy example上，所以有可能雖然一個hard example 的 loss很大，但是卻跟1000個easy example 一樣大。最主要就是希望在訓練的時候能夠關注hard example,然後忽略一下easy example故最簡單的方法就是在Cross-entropy 前面加一個係數但其實只有提升一點點，因為只是一個比例放大放小的問題，故在focal loss裡加入modulating factor, $r \\geq0$:再將它們融合一下就變成For $\\alpha=1,\\gamma = 0$, it is equal to CE loss. 機器學習最終奧義：把別人的變成是我的special case, 那我就變強了。For the ground truth label ${y_i}{i=1}^n$, and the transformed prediction probability ${h(F(X_i + P))}{i=1}^n$.And the focal loss for above is where $w_j=\\frac{1}{n_j} &gt; 0$ is a class balancing coefficient, $n_j$ is the number of the class $j$, $\\gamma =2 \\geq 0$ is a focusing parameter which down-weights high-confidence(large $h_j$) samples. (4) The gradient based on Zeroth Order Optimization for BARWe use Auto-Zoom for gradient. Gradient estimation of $\\nabla f(W)$For any random gradient $g_j = b \\cdot \\frac{f(W+\\beta U_j) - f(W)}{\\beta} \\cdot U_j$, and get the mean for the $g_j$, (5) The BAR algorithmFor the $\\bar g$, we use SGD with $\\bar g$ to optimize the parameters $W$ in BAR. minibatchsize n = 20, $\\alpha_t$ is the step size with using exponential decay with initial learning rate $\\eta$Training process with : iteration $\\times$ minibatchsize $\\times$ (q+1) queries. Experiment Reprogramming three pretrained black-box ImageNet classifiers form ResNet50, Inception V3, DenseNet121 for three medical imaging classification tasks. Reprogramming two online classsification APIs for ASD. Testing the q and the MLM size m for BAR. Testing the difference of loss function(CE-loss vs. F-loss) Testing the mapping method(random vs. frequency) Three baseline : (1) AR method (2) Transfer learning (3) SOTA Three dataset: Autism Spectrum Disorder Classification : (ASD,ABIDE database,2 classes)split in ten folds and 503 ASD + 531 non-ASD samples, test sample=104, data sample is a 200 $\\times$ 200 brain-regional correlation graph of fMRI measurements, which is embedded in each color channel of ImageNet-sized inputs. Set $\\eta=0.05$ and $q=25$Table1 is the test performance:We can see the performance is similar to AR with Resnet50 and InceptV3 Diabetic Retinopathy Detection: (DR,5 classes)5400 training data, 2400 testing data. Set $\\eta=0.05$ and $q=55$. Use 10 labels per target class for MLM.The current best performance accuracy is 81.36%.(data augumentation on ResNet50) Melanoma Detection : (ISIC database, 7 classes)It is a kind of skin cancer. Containing 7 types of skin cancer. Imagesize with 450 $\\times$ 600 , resized to 64 $\\times$ 64 and the data is imbalanced. Perform the re-sampling on the training data to ensure the same sample size for each class.train/test data ~ 7800/780. Use 10 labels per target class for MLM.The Best reported accuracy is 78.65%.(data augumentation on DenseNet) Reprogramming The Real life Prediction APIswe want to reprogram them for Autism spectrum disorder classification task. Clarifai Not Safe For Work API : (NSFW API)Recognize Image or videos with in appropriate contents.(labels : NSFW or SFW) Clarifai Moderation API :Recognize whether images or videos have contents such a “gore”, “drugs”, “explicit nudity”, “suggestive nudity” or “safe”.(labels : 5 classes) Microsoft Custom Vision API :use this API to obtain a black-box traffic sign image recognition model (with 43 classes) trained with GTSRB dataset. Then we seperated the dataset into train/test ~ 930/104(ASD) and 1500/2400(DR). Use random label mapping instead of frequency mapping to avoid extra query cost. Ablation Study and Sensitivity Analysis Number of random vectors (q) and mapping size (m) :Resnet 50 ImageNet model to perform ASD classification, DR detection, and Melanoma detection with different q and m values. Random and frequency multi-label mapping :Random mapping : Randomly assign m seperate labels from thje source domain.Frequency mapping : (1)Obtain the source-label prediction distribution of the target-domain data before reprogramming in each task. (2) Assign the most frequent m source-labels.For both AR and BAR, frequency mapping roughly 3% to 5% gain in test accuracy. Cross entropy loss (CE-loss) and focal loss (F-loss) :The Performance increase when using F-Loss in BAR.F-loss greatly improves the accuracy of BAR by 3%-5%. Visualization :ResNet 50 feature maps of the pre-logit layer Conclusion Proposed a novel black-box adversarial reprogramming framework for limited data classification tasks. Used the multi label mapping and gradient free approach to handle the black-box model without knowing the pre-trained model. Reprogramming the black-box model for three medical imaging tasks and outperformed the general transfer learning model. Appendix","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"}],"author":"yiwei"},{"title":"An RSVM based two-teachers-one-student semi-supervised learning algorithm","slug":"An RSVM based two-teachers-one-student semi-supervised learning algorithm","date":"2020-08-14T14:00:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2020/08/14/An RSVM based two-teachers-one-student semi-supervised learning algorithm/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/08/14/An%20RSVM%20based%20two-teachers-one-student%20semi-supervised%20learning%20algorithm/","excerpt":"An RSVM based two-teachers-one-student semi-supervised learning algorithmtags: ‘Dual voting’, ‘SVM’原論文在此","text":"An RSVM based two-teachers-one-student semi-supervised learning algorithmtags: ‘Dual voting’, ‘SVM’原論文在此 Introduction:12Why was the study undertaken? What problem was studied? The author proposed a semi-supervised learning method which is based on RSVM. 如標題，這篇文章主要提出了一個基於RSVM的特性，延伸出的semi-supervised learning的演算法：two-teachers-one-student。 概念：三個RSVM learning models, 每次從三個中挑兩個models去對unlabeled data投票(每次迭代會把所有的unlabeled data跑過)，有共識，再enlarged labeled data，讓第三個model去做training。輪流進行，讓label點越來越多。 因為這篇用到RSVM model，因此稍微回顧一下RSVM的幾個核心概念。RSVM的幾個核心重點： classifier可以表示成basis function的線性組合： （basis function=${1 \\cup k(\\cdot, \\tilde{A_1})\\cup k(\\cdot, \\tilde{A_2}),\\dots,\\cup k(\\cdot, \\tilde{A_\\tilde{m}})}$）。如下圖： 也可看作是把每個點$x$以和「basis function裡的每個元素」的相似度重新表達$x$。 (李育杰老師投影片) 2T1S Algorithm12How was the problem studied?The approach proposed in this article is 2-teachers-1-student. 2T1S演算法如下： 2T1S = co-training + consunsus training 三個models是？前面提到了有三個RSVM models，實際上就是取出三組不同的reduced sets得到三個不同的models。而這三個models，直覺上若視角越不一致，應該能得到越多的資訊。而在RSVM model裡，「視角」在RSVM裡就關係到「reduced set的選取」。而在此篇文章，使用Incremental RSVM (IRSVM) 來選取reduced set。 Remark:如何選取reduced set？有很多的方法，如：CRSVM, SSRSVM [2]。 IRSVM:目標：找出較有表達力的Reduced set.（找出的reduced set，希望裡面的每個向量看到的觀點越不同越好。） 演算法如下： 透過下圖中的(4)式，評估$A_i$是否值得加入reduced set。如何評估呢？ (4)式所想表達的，主要是希望以$A_i$ （和原training data的相似度）來表達$A$的向量,和以目前的functions $K(A,\\tilde{A})$ 所能組出的向量不要太靠近。因此，若連最小的值都大過門檻值（自訂）$\\delta$，就不列入考量。 附註：事實上(4)式的解就是 $\\beta^=(\\tilde{K’}\\tilde{K})^{-1}\\tilde{K’}\\tilde{K}(A,A_i)$。也就是，只要計算$r=|\\tilde{K}\\beta^-K(A,A_i)|_2$即為（4）。 Remark:If the columns of the rectangular kernel matrix generated by the initial reduced set are linearly independent, the IRSVM algorithm will retain the independence property throughout the whole process, so that the least squares problem (4) has a unique solution $\\beta^*=(\\tilde{K’}\\tilde{K})^{-1}\\tilde{K’}\\tilde{K}(A,A_i)$ 所以要怎麼選出三個「視角很不同」的models? 譬如說，期望每個RSVM model的reduced set size為$\\tilde{m}$,則透過IRSM找出$3\\tilde{m}$個向量，再以“Round-Robin partition method”分成三等份。 Results:1What were the findings? 比較有supervised learning和semi-supervised learning結果： 比較SL和2T1S-i, 2T1S-ii(使用不同的datasets) 以p value來看是否有顯著差異（有semi-supervised learning和單純supervised learning）。 比較co-tr, tri-tr, 2T1S-i, 2T1S-ii","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SVM/"}],"author":"Puchi"},{"title":"Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity","slug":"Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity","date":"2020-07-24T02:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/07/24/Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/07/24/Sentence%20Meta-Embeddings%20for%20Unsupervised%20Semantic%20Textual%20Similarity/","excerpt":"tags: ‘NLP’, ‘paper’, ‘sentence similarity’","text":"tags: ‘NLP’, ‘paper’, ‘sentence similarity’ paper：Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity proper noun list in this paper meta embedding Instead of picking one word embedding why not use them all!Meta Embedding Learning 有很多種embedding方法，如：Bert, Ernie, Elmo，meta embedding就是找個方法將他們全部ensemble起來。 Introduction 本篇論文在做什麼？此篇論文主要是被word meta-embeddings的想法所啟發。並在文中提出幾個sentence meta-embeddings的方法。 主要探討的議題：Unsupervised Semantic Textual Similarity方法：想法源自於word meta embedding的概念。將此概念應用到句子，在此篇文章中，嘗試用三種不同的meta embedding的方法-Naive meta embedding, SVD, GCCA，將多種sentence embedding結合，並得到了unsupervised的方法中達到State of The Art (簡稱為SoTA). 貢獻：提出sentence meta embedding的方法，並達到unsupervised方法的SoTA。 Method 符號：$F_1, F_2, …, F_J$ 代表不同方法得到的sentence encoders。每個不同的$F_j$將原句映射到維度為$d_j$的空間。(彼此維度可能不同)$S$表示句子 整個流程如下：使用不同方法得到不同的sentence embedding，再運用此篇提到的方法，得到meta embedding。最後使用cosine similarity去看句子的相似度，在使用Pearson &amp; Spearman 去做evaluation。 meta embedding方法包含：Naive meta-embeddings, SVD, GCCA, Autoencoders(AEs) Naive meta-embeddings方法如下: SVD方法如下: 可參考此[連結] (https://www.cnblogs.com/pinard/p/6251584.html) GCCA （Generalized Canonical Correlation Analysis） GCCA是CCA的一般化。 Remark: CCA是什麼？ 給定任意兩$x_1$, $x_2$,CCA主要是在找出能使$\\theta_1x_1$和$\\theta_2x_2$最相關的linear projections。 作法如下：（可再去詳細看一下GCCA，下次等永祥講了) 之後新的句子s’的embedding則為： Autoencoders (AEs) 不同方法得到的的sentence encoders $F_j$ （在下圖為$x_j$） 都各自有一個trainable encoder $\\varepsilon_j:R^{d_j}\\rightarrow R^d$及trainable decoder $D_j:R^d\\rightarrow R^{d_j}$ (d是hyperparameter)。 接著計算loss：（希望和encode再decode後的和原embedding相似） 最後使用以下做為句子s’的meta embedding： 結果： Experiment使用的data:使用Billion Word Corpus (BWC）做training。使用STS12-STS16及unsupervised STS Benchmark test set做evaluation。 資料形式： $（s_1, s_2,y）$ 句子$s_1$, 句子$s_2$, y為相似度分數。 sentence embedding方法：包含Univesal Sentence Encoder (USE), Sentence-BERT(SBERT), ParaNMTa。 We select our ensemble according to the following criteria: Every encoder should have near-SoTA performance on the unsupervised STS benchmark, and the encoders should not be too similar with regards to their training regime. Experiment result使用Pearson 及 Spearman相關係數。值得一提的是，高維度的sentence representations會比低維度的具優勢。為了排除結果僅是因為高維度的關係，作者做了以下實驗：對single USE及single ParaNMT做了up-projected的動作。結果顯示單純升高維度並不會提高效果，藉此去說分數較高是因為meta embedding的關係。 從上圖可發現，不同sentence embedding的方法都是有所貢獻的。 結論由實驗結果顯示sentence meta-embeddings的結果比原先的方法好，並達到的SoTA。 Question 為什麼在此用SVD, GCCA的方法會比較好？（合理嗎？） 延伸閱讀： GCCA Univesal Sentence Encoder (USE), Sentence-BERT(SBERT), ParaNMTa","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"sentence similarity","slug":"sentence-similarity","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/sentence-similarity/"}],"author":"Puchi"},{"title":"BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","slug":"BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","date":"2020-07-16T23:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/07/16/BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/07/16/BART%20Denoising%20Sequence-to-Sequence%20Pre-training%20for%20Natural%20Language%20Generation,%20Translation,%20and%20Comprehension/","excerpt":"","text":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension論文網址 AbstractBart爲預訓練Sequence to Sequence model而設計的去噪自編碼器基於之前的預訓練結構之後，利用兩種方法去得到BART(1) 使用任意的噪聲去破壞文本(2) 學習重建文本(原本為有破壞的文本)會在生成文本之類的任務相當有效，例如：問答，對話，總結等等…。其他類型也都跟RoBerta差不多 123We present BART, a denoising autoencoderfor pretraining sequence-to-sequence models.BART is trained by (1) corrupting text with anarbitrary noising function, and (2) learning amodel to reconstruct the original text. IntroductionBART 是一個預訓練的模型組合了 Bidirectional and Auto-Regressive TransformersBART 基礎模型是使用standard Transformer-based neural machine translation結構，但可以看作是對BERT（雙向編碼器），GPT（從左至右解碼器）以及許多其他較新的預訓練方案進行了概括。 1234BART uses a standard Tranformer-based neural machine translation architecturewhich, despite its simplicity, can be seen as generalizing BERT(due to thebidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. BERT採用隨機選擇token進行mask，但Bidirectional Encode會去獨立預測單個mask的字，故不太適合文本生成任務。GPT採用單向Encoder和利用Autoregressive Decoder去預測下一個字的token是什麼，但缺陷在於無法學習到雙向的部位BART採用Sequence-to-Sequence的架構把BERT和GPT做一個整合，使用雙向模型去對損壞文本做編碼，在用自回歸模型做解碼去預測該字的下個字，最後的Finetune也會把完整的文本拿進來做微調的動作。（這邊的mask不只改掉一個，有可能會連續的抹掉） Modelbase model : 6 layers in encoder and decoderlarge model : 12 layers in encoder and decoder基本模型跟BERT相似（transformer Sequence-to-Sequence）並且還刪除了最後不需要的Linear的地方，整個模型縮小了10% 123456(1) each layer of the decoder additionally performs cross-attention overthe final hidden layer of the encoder (as in the transformer sequence-to-sequence model)(2) BERT uses an additional feed-forward network before wordprediction,which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model. Pretraining in Noising input他們實驗了很多種類的加噪的方法（還說這個地方是可以被研究，應該是一個很重要的地方） Token Masking : 如同Bert一樣，單純對某些東西做隱藏 Token Deletion : 隨機的刪除輸入中的tokens，只需要決定tokens的位置即可(要給定的) Sentence Permutation : 將句子做亂數排列，same as XLNET. Document Rotation : 在句子內隨機選取一個token，然後將該token設為開始旋轉，目的是要認得句子的開頭。 Text Infilling : 自poison分佈($\\lambda = 3$)，對句字的某段部分做採樣(mask超過1個)，similar to spanBERT(different distribution). Fine-tuning BART在模型的訓練完之後會有一個fine-tuning的微調動作 Sequence Classification Tasks : 給予encoder和decoder相同的輸入，並且將最後一層的decoder 的 hidden state 再丟入一個linear classifier去做預測label的動作。 Token Classification Tasks : 給予encoder和decoder相同的輸入，但這次是給予decoder 的 hidden state，用這個state去分類這些token Sequence Generation Tasks : 由於有自回歸的decoder，所以可以直接執行生成文本的任務訓練。 Machine Translation : 神奇的解釋（你只要多加了一個encoder，就會變好喔，出處：Edunov et al. (2019))，train end-to-end, 用randomly initialized Encoder取代原本的embedding, 並訓練其將外來詞映射到BART（BART能將其降噪成英語的輸入）。訓練步驟如下：步驟1 : 先將pretrained encoder, decoder 去固定住，用bitext(平行機器翻譯文本)去對randomly initialized Encoder 去做訓練步驟2 : 再將全部不要固定住，再做一次fine-tuningPrevious work Edunov et al (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.Dataset SQuAD : (Rajpurkar et al., 2016) an extractive question answering task onWikipedia paragraphs. Answers are text spans extracted from a given document context.(問答任務) MNLI : (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. （觀看句子包含的任務） ELI5 : (Fan et al., 2019), a long-form abstractive question answering dataset.（很長的問答任務） XSum : (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.（新聞抽象式摘要任務） ConvAI2 : (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.（對話生成文本任務） CNN/DM : (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences.（同為新聞摘要任務） ResultsComparing with pretrain objects我只是站得夠高而已！！！ 每個方法都是獨立的在每個任務(language model best at ELI5, bad at SQUAD) Token masking 很重要 (4,5的表現都差了一點 在BART上) Left-to-right pre-training improves generation BART achieves the most consistently strong performance.(除了ELI5以外都是最好的 Result:他的設定基本都跟Roberta一樣，large model with 12 layers in each of encoder and decoder，use a batch size of 8000, and train the model for 500000 steps,mask 30% of tokens in each document, and permute all sentences.並且為了更好的fitting, 最後的10% step是不使用dropout的. Future work 破壞文檔的預訓練的新方法 針對特別文檔的演算法","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}]},{"title":"Discourse-Aware Neural Extractive Text Summarization","slug":"Discourse-Aware Neural Extractive Text Summarization","date":"2020-07-08T18:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/07/08/Discourse-Aware Neural Extractive Text Summarization/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/07/08/Discourse-Aware%20Neural%20Extractive%20Text%20Summarization/","excerpt":"","text":"Discourse-Aware Neural Extractive Text Summarizationtags: references NLP論文BERT 的sentence-based extractive model 通常extracte出多餘的或沒有訊息的summary 且 long dependency on document 也是一個問題(since bert pretrain is sentence-based) DISCOBERT extracts 一些子句的discourse unit(而非整個句子)更精細的來選出較好結果而long dependency on discourse unit的問題 透過使用RST tree and coreference mentions(Graph Convolutional Networks) 解決通常summary 分成兩種 extractive summary : Directly selects sentences from the document to assemble into a summary. abstractive summary : Genterated word-by-word after encoding whole document 各自優點 extractive 有更好的呈現，和較快 abstractive summary 更靈活，產生較少多餘文章 Hybrid -&gt; pipeline or candidate 原目的是丟棄所選句子中沒有信息的短語 但是會受到pipeline分開產生而產生的語意斷層 EDU(elementary discourse unit): 而在這個paper使用EDU當成基礎單元而非原來的句子，來進行提取壓縮和減少句子間的冗言贅字 Here is the example of document透過使用EDU可以減少使用多餘的細節，保留了更多的包含主要概念或事件的資訊，從而產生更簡潔，內容更豐富的摘要。此外使用已知的知識來finetune 句子內的資訊，透過使用2種graphic model (都是base on EDU) RST graph $G_R$ Coreference graph $G_C$ 有點像是BFS+cluster:找定一點(core) explore 與其互相影響的 other point(event/concepts) ，亦用來讓model找long-dependency最後使用 Graph Convolutional Network (GCN)(bse on EDU) 來解決long-range interaction問題 主要的貢獻有三 提出discourse base 的summarization model 生成簡潔且較少冗句的摘要 結構化兩種類型的 graphic model 在summary 上贏過bertDiscourse Graph Construction Initial with disconnection on all point Discourse focus on unit間的關係 In the RST framework, document can be segmented into (1)contiguous,(2)adjacent and (3)non-overlapping text spans(=EDU,其通常標記為 1.Nucleus or 2.Satellite) Nucleus : 通常位於中心位置 Satellite : 位於外圍，且這類在內容和語意相依性不太重要。 Note : EDU 存在相依性，代表著它們互相的修飾關係，但是當考慮修飾關係會使得summary more ambiguous，因此修飾關係不在model的考量當中。 RST Graph當選擇句子作為提取摘要的候選者時，假設每個句子在語法上是獨立的。 但是對於EDU，有些額外限制以確保語法。在圖中 [2]EDU是完整句子而[3]不是。只看那些不完整EDU-&gt;需要了解EDU之間的依賴性，以確保所選組合的語法合法。We use the converted dependency version of the tree to build the RST Graph G_R, by initializing an empty graph and treating every discourse dependency from the i-th EDU to the j-th EDU as a directed edge, i.e., G_R[i][j] = 1 Coreference Graph Text summarization 通常會有’position bias’ 問題：即為關鍵訊息聚集在特定地方。i.e.,在新聞中大部分關鍵訊息都是在一開始就描述的。 Coreference meaning 演算法首先使用Stanford CoreNLP來檢測文章中的所有coreference clusters。 對於每個coreference clusters，將提及同一cluster的所有語篇單元link在一起。 在所有coreference clusters上重複此過程，以創建最終的Coreference Graph。 但是，在文檔的中間或末尾仍然散佈了大量的信息，匯總模型通常會忽略這些信息。 經過觀察大約25％的 oracle sentence 出現在CNNDM數據集中的前10個句子之外。 此外，在長篇新聞文章中，整個文檔中經常有多個核心人物和事件。但是，現有的神經模型在建模這樣的長篇上下文時效果不佳，尤其是當存在多個模棱兩可的共指關係(兩種詞共指同物)要解析時。 在圖一中也顯示構造圖$G_C$時，由於提到了“ Pulitzer prizes”，因此1-1、2-1、20-1和22-1之間的邊都連接在一起。 DISCOBERT Model首先使用BERT encode整個篇章，使用BERT得到的hidden state表示，每個EDU內部做selfattentive span extractor得到新的EDU的表示，由得到的EDU表示和兩個矩陣表示$G_R$和$G_C$，做GCN得到EDU新的表示， 通過MLP預測EDU是否被抽取出來做EDU（0-1序列標註）[ where each EDU $d_i$ is scored by neural networks]。而在生成過程中，需要進一步考慮語篇依賴性，以確保輸出摘要的連貫性和語法性。Note :Insert &lt;CLS&gt; and &lt;SEP&gt; tokens at the beginning and the end of each &quot;sentence&quot;, respectively Document Encoder Self-Attentive Span ExtractorNote : all the W matrices and b vectors are parameters to learn,$h^S = {h^S_1,…h^S_n} \\in R^{d_h*n}$ Graph Encoder LN(·) represents Layer Normalization, $N_i$ denotes the neighorhood of the i-th EDU node. For different graphs, the parameter of DGEs are not shared. If we use both graphs, their output are concatenated with:(合2圖的結果再丟進2分類器)) Loss:BCE losswe sort y^ in descending order, and select EDUs accordingly. Note that the dependencies between EDUs are also enforced in prediction to ensure grammacality of generated summariesExperimentsDatasets NYT(New York Times) CNNDM(CNN and Dailymail) Toolkit See et al.用於extract summaries Stanford CoreNLP for sentence boundary detection,tokenization and parsing Detail Theedges in $G_C$ are undirected, while those in $G_R$ are directional. AllenNLP as the code framework DGL as the implementation of graph encoding Datasets training validation test CNNDM 287226 13368 11490 NYT 137778 17222 17223 Compare with State-of-the-art Baselines Extractive Models: BanditSum is a policy gradient methods NeuSum is a seq2seq architecture, where the attention mechanism scores the document and emits the index as the selection Compressive Models: JECS(BiLSTM as the encoder)The first stage is selecting sentences, and the second stage is sentence compression by pruning constituency parsing tree BERT-based Models: BertSum(model re-implementation as baseline) PNBert proposed a BERT-based model with various training strategies, including “reinforcement learning” and “Pointer Networks” HiBert is a hierarchical BERT-based model for document encoding, which is further pretrained with unlabeled data Hardware NVIDIA P100 card Mini-batch 6 length 768 BPEs pre-trained bert-base-uncased Iter 80000 evaluation metrics ROUGE validation criteria R-2 Note:EDU之間的依賴性對於所選EDU的語法至關重要。 這是學習依賴關係的兩個步驟：head inheritance andtree conversion。Head inheritance:為每個有效的非終止樹定義Head node。 對於每個葉節點，頭部都是自身。 我們根據非終止樹的Nucleus數確定其Head nodeex: If model selects “[5])(N) being carried … Liberia.” as a candidate span, we will enforce the model to select “[3](N) and shows … 8,” and “[2](N) This … series,” as well. 通過篇章分析，可以在篇章上構造得到一棵樹，樹的葉子節點是EDU，樹上的邊代表的是對應子節點的重要性程度，N代表主要，S代表次要，可以認為S是N的補充。相鄰兩個子節點可以有三種關係，N-N,N-S,S-N。 作者提出假設：S依賴N,所以存在一條路徑從S指向N；如果兩個節點都是N，就認為是右N依賴做N。 根據這個假設，可以將RST discourse tree轉成成RST dependence graph。 ResultsCNNDM The second section lists the performance of baseline models, including non-BERT-based and BERTbased variants By a significant margin (0.52/0.61/1.04 on R-1/-2/-L on F1).NYT DISCOBERT provides 1.30/1.29/1.82 gain on R-1/-2/-L over the BERT baseline. However, the use of discourse graphs does not help much in this case. Grammatical problem 由於句子的分割和部分選擇，我們模型的輸出在語法上可能不如原始句子。 需要手動檢查並自動評估模型輸出，並觀察到總體而言，考慮到RST依賴樹限制了EDU之間的修辭關係，所生成的摘要仍然是語法上的。 一組簡單而有效的後處理規則在某些情況下有助於完成EDU。 Automatic Grammar Checking Human Evaluation Examples &amp; Analysis : We notice that a decentamount of irrelevant details are removed from theextracted summary.In this example “[‘Johnny is believed to have drowned,]1 [but actually he is fine,’]2 [the police say.]3”, only selecting the second EDU yields a sentence “actually he is fine”, which is not clear who is ‘he’ mentioned here. 發現錯誤主要源自RST依賴關係解析和語篇解析的解析錯誤。RST依賴關係的錯誤分類和手工製定的依賴關係解決規則損害了生成輸出的語法和連貫性。 常見的標點符號問題包括逗號過多或缺失以及引號缺失。 Conclusion 在本文中，我們提出了DISCOBERT，它使用EDU作為最小選擇基礎來減少摘要冗句，並利用兩種類型的圖model作為來捕獲EDU之間以及長期依賴性。 在兩個摘要生成數據集上驗證了所提出的方法，並觀察到相對於baseline模型的一致改進。 在以後的工作中，我們將探索更好的圖形編碼方法，並將圖model應用於需要長文檔編碼的其他任務。 忘了做圖 之後再++ 南部網路真滴慢","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"}],"author":"Kohpo"},{"title":"Is Attention Interpretable?","slug":"Is Attention Interpretable_ (1)","date":"2020-07-01T13:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/07/01/Is Attention Interpretable_ (1)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/07/01/Is%20Attention%20Interpretable_%20(1)/","excerpt":"","text":"Is Attention Interpretable?by Chih-Chi Wu 2020.06.29論文 tags: ‘NLP’, ‘paper’, ‘NLP study group’ Q:此篇論文的目標？想探討「attention可偵測出模型中認為重要的訊息」這件事的正確性。 可解釋性可以理解為，Attention權重的高低應該與對應位置信息的重要程度正相關；高權重的輸入單元對於輸出結果有決定性作用。本文的主要研究方法是中間表示擦除，主要邏輯在於越重要的權重對輸出結果的影響越大，將它置零就會對結果有直接的影響。https://www.cnblogs.com/bernieloveslife/p/12748433.html Q:這篇和上一篇的差異？上一篇論文(Attention is not explanation)較頃向從整體來看，譬如：整組的attention weight換掉是否對結果有差？而此論文則著重在attention到底有沒有抓到重點？（此段文字頃向自己的解讀，若有理解錯歡迎糾正…）論文中在講和之前的差異的內文如下： One point worth noting is the facet of interpretability that our tests are designed to capture. By examining only how well attention represents the importance of intermediate quantities, which may themselves already have changed uninterpretably from the model’s inputs, we are testing for a relatively low level of interpretability. So far, other work looking at attention has examined whether attention suffices as a holistic explanation for a model’s decision (Jain and Wallace, 2019), which is a higher bar. We instead focus on the lowest standard of interpretability that attention might be expected to meet, ignoring prior model layers. Q:簡短敘述這篇論文？為了探討「attention可找出模型中認為重要的特徵訊息」這件事的正確性，這篇論文設計了一個實驗驗證架構：使用擦掉某些attention weight的方式（在此篇論文中假設weight較大表示較重要），去看看對模型預測的影響。結論是attention weight雖有稍微的反應出特徵（feature）的重要性，但並沒有非常直接的相關。 註：這裡的feature指的是input經過encoder後的向量。 論文中使用的任務、資料集 topic classification (dataset: Yahoo answers) review rating (datasets: IMDB, Amazon, Yelp) 模型架構主要使用Hierarchical Attention Network(HAN)架構（一種text classification的模型），HAN的特色在先對「詞」做attention，再對「句子」做attention，請看圖一（之前小勛講的那篇,忘記可以回顧一下）。 在此篇論文中僅對句子那層的attention去做測試，attention則是使用Bahdanau et al. (2015)提出的架構如下（就是之前討論過的第一篇attention )： 除此之外，此論文也探討兩個HAN的變化模型： flat attention networks (簡稱FLAN)：僅考慮一層attention，也就是不像HAN分詞、句子去做attention，而是不分句子，把所有句子的詞攤平一起看。 將原本HAN模型裡的word encoder的雙向GRU結構改成convolutional encoder 所以，此篇論文總共考慮的模型有：HANrnns (bi-GRU), HANconvs, HANnoencs, FLANrnns, FLANconvs, FLANnoencs HAN（圖一）圖出處 convolutional encoder的FLAN示意圖 圖出處 如何透過實驗驗證目標？實驗驗證架構此論文透過擦掉某些重要的attention weight的方式，去觀察結果的差異。因此，實驗在Part 1 of model（即做完attention、還沒跑linear classification的部分）之後分兩部分去探討： attention weight不變，直接放到Part 2 of model（即最後一層linear classification的部分） 擦掉某些重要的attention weight，再放入最後一層linear（這裡為什麼要做renormalize？是為了避免擦去較大的attention weight之後，document representation的地方趨近於0，使擦去後的representation和原始training放入最後一層linear classification的representation差太多） 驗證方式此論文分兩部分來探討： 擦掉一個attention weight(將最大的attention設為0的意思) 擦掉一些attention weights 針對上述兩點，論文進行了不同的驗證實驗。 1. 擦掉一個attention weight(將最大的attention設為0的意思)驗證方式1 比較： 最高attention weight 隨機選取的attention weight x軸：上述兩項的attention weight的差距;y軸：原模型分別對其的JS divergence差距如預期的，大部分的attention weight差距越大，$\\triangle JS$也大多有變大的現象。|是負值的部分，也大都趨近於0 x軸：-$\\triangle JS$; y軸：數量註：若較高的attention weight影響力比較高，那麼$\\triangle JS$差值應該大於0。 註：但我們其實不知道多大的差距可以詮釋attention weight重要性。 上圖的意思是：分別去看在「remove 最大attention weight」及「remove 隨機attention weight」時是否造成翻轉決策？從上圖可發現，大部分都是兩個都沒有影響結果。雖然remove最大attention weight仍比remove隨機的attention weight造成翻轉決策較多，但並沒有高很多。（這裡只放rnn based的結果，但其他模型架構結果差不多，有興趣可以看論文） 2. 擦掉一些attention weights上述的方法（僅去掉其中一個attention weight），會看到只擦去一個attention weight大多不會翻轉決策、$\\triangle JS$趨近於0等問題，因此本論文又再做更進一步的探討。 如何探討呢？想法是：若attention weight真的呈現出feature重要性，那麼應該擦去越小的集合，就能夠翻轉決策。 某項token越重要，那麼mask它使模型分類錯誤的可能性也就越大，所以找到的mask集合越小，這個集合中的token越重要。https://www.cnblogs.com/bernieloveslife/p/12748433.html 也就是透過不同的方法，去對feature重要性去做ranking，並按ranking結果依序擦去feature，直到會翻轉決策。而擦去的集合大小也某種程度呈現此方法到底好不好？（feature importance的概念），而此論文使用四種方式： 隨機mask 按attention weight大小做ranking，優先mask掉attention weight較大的。 透過分析最後一層classification layer的decision function的gradient去做ranking 透過直接「attention weight*上述的gradient值」去做ranking 從上圖（boxplot）可看到，attention*gradient（第4種方式）的方法，雖然整體而言最小（較好），但其實和gradient方法（第3種方法）差不多。而attention weight(第2種方法)的方法則更差（大部分的測試資料都需要擦去非常多feature,才能達到翻轉決策的效果）。 值得一提的是，rnn based的模型架構，相較convolution based的模型，通常需要擦去較多的attention weights,這也和RNN based的模型做完encoder後會有訊息遷移這件事相吻合。 limitations 這篇主要著重在探討text classification，若此方法應用在其他的task（如翻譯，不只幾類，而是有許多預測結果）就不一定適用。 因為分類結果中有很多類別，但這裡只考慮出來結果機率最大的，其他的都沒有考慮。（可能需要針對每個類別都去找最重要的token是什麼） 還有許多其他的模型架構結論attention weight雖有稍微的反應出特徵的重要性，但並沒有非常直接的相關。 future work 除了擦去重要的attention weight的方法，還有許多方法可以探討attention weight的解釋性議題。 期望轉換成「找出一個更加好的ranking方式」 思考問題 為什麼要設計FLAN?（可能是因為只有一層;HAN有兩層） Related workhttps://www.cnblogs.com/bernieloveslife/p/12748433.html","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Puchi"},{"title":"Attention is not not explanation","slug":"Attention is not not explanation","date":"2020-06-24T15:12:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/06/24/Attention is not not explanation/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/06/24/Attention%20is%20not%20not%20explanation/","excerpt":"tags: study paper DSMI labpaper: Attention is not not explanation (不太喜歡這篇的表達方式，很多雙重否定@@，而且結構有點亂) Introduction這篇是基於”Attention is not explanation”(以下簡稱 ==J&amp;W==)這篇論文的探討。目的不是在說attention有解釋性，而是在說”attention 不一定沒有解釋性”。文中有點出幾個在”Attention is not explanation”的實驗和論述裡面，潛在的文問題。並且提出一些比較好的分析方式(雖然我沒有完全被說服)。","text":"tags: study paper DSMI labpaper: Attention is not not explanation (不太喜歡這篇的表達方式，很多雙重否定@@，而且結構有點亂) Introduction這篇是基於”Attention is not explanation”(以下簡稱 ==J&amp;W==)這篇論文的探討。目的不是在說attention有解釋性，而是在說”attention 不一定沒有解釋性”。文中有點出幾個在”Attention is not explanation”的實驗和論述裡面，潛在的文問題。並且提出一些比較好的分析方式(雖然我沒有完全被說服)。 Main Claim Attention Distribution is not a Primitive The base attention weights are not assigned arbitrarily by the model, but rather computed by an integral component whose parameters were trained alongside the rest of the layers; the way they work depends on each other. Attention是有參與模型訓練的，並不是獨立於模型存在。J&amp;W的把attention random permute 的實驗不是那麼適當。在製造adversary的時候應該要retrain model. Existence does not Entail Exclusivity We hold that attention scores are used as providing an explanation; not the explanation. 找到另一個可解釋的方式不代表本來的方式沒有解釋性。解釋性不具有唯一性。尤其對於binary classification task，input是很多個字(維度很大)，output是0~1，在降維的過程中容易有比較大的彈性。 Defining ExplanationJ&amp;W 沒有清楚的定義甚麼是explanation，其實在過去的文獻對於explanation有不同定義。提到AI的可解釋性，常會出現下列三個名詞。 transparency: 找到model中可以令人理解的部分。 Attention mechanisms do provide a look into the inner workings of a model, as they produce an easily-understandable weighting of hidden states. explainability 可以提供決策 可以模擬人類從過去所發生的事情中進行推斷的能力 interpretability relationship between input and output (類似回歸裡面的regressor) 需要專家幫忙鑑定 ExperimentsDataset跟J&amp;W做一樣的實驗，但只有做 binary classification，沒有做QA。 Diabetes: whether a patient is diagnosed with diabetes from their ICU discharge summary Anemia: hether the patient is diagnosed with acute (neg.) or chronic (pos.) anemia IMDb: positive or negative sentiment from movie reviews SST:positive or negative sentiment from sentences AgNews:the topic of news articles as either world (neg.) or business (pos.) 20News:the topic of news articles as either baseball (neg.) or hockey (pos.) Modelsame as J&amp;W single-layer bidirectional LSTM with tanh activation attention layer softmax prediction hyperparameters are set to be same as J&amp;W Uniform as the Adversary Attention is not explanation if you don’t need it 每個字給相同的權重在某些task上面其實跟attention一樣好，那在這些task上面，attention的確沒什麼用。 Variance within a Model(其實我不太懂這個實驗到底要幹嘛) Test whether the variances observed by J&amp;W between trained attention scores and adversarially-obtained ones are unusual. Train 8 models with different initial seeds Plot the distribution of JSD(Jensen-Shannon Divergence) attention weights by the models IMDB, SST, Anemia are robust to with seed changes. (e): J&amp;W 所產生出的advesary attentions 確實和本來的model很不一樣 (d):negative-label instances in Diabetes dataset subject to relatively arbitrary distributions from the different random seeds.因此對於分布差很多的attention weights，效果可能還是不錯 (f): 所以J&amp;W 所產生出的advesary attentions 不是足夠adversarial Training an Adversary所以要怎麼樣才能建立合理的Adversary呢?上面有提到attention weight 不是獨立於model而存在的，所以每換一組attention weight都應該train a new model Given a base model $M_b$, train a model $M_a$ which can provide similar prediction scores its distribution of attention weights should be very different from $M_b$ Loss function: $L(M_a, M_b)=\\sum^N_{i=1}TVD(\\hat{y}_a^i,\\hat{y}_b^i)-\\lambda KL(\\alpha_a^i||\\alpha_b^i)$ $TVD(\\hat{y}_a^i,\\hat{y}_b^i)=\\frac{1}{2}|\\hat{y}_a^i-\\hat{y}_b^i|$ 以下的圖: 曲線越convex 表示attention weight 越可以被操控。 x軸是某個model和$M_b$的attention JSD 圖例: 三角形: 固定$\\lambda$ (不同dataet自己最好的$\\lambda$)，使用不同random seeds 所train 出來的model 正方形: uniform weight model 加號: J&amp;W’s adversary model 點點: 不同$\\lambda$所train 出來的model Diagnosing attention distribution by guiding simpler models使用RNN系列的 model，會有前後字的影響，其實很難排除前後字是不是會影響解釋性，所以這篇使用MLP(non-contextual model，不能看左右鄰居)來診斷if attention weights provide better guides. 選擇一組pretrained attention weight (有對比MLP自己學) train MLP 實驗結果有發現用本來的model還是最好的(so attention provides better guide) Conclusion 首先要定義好explanation是甚麼 J&amp;W 的實驗有不少漏洞，我們提供了比要合理的實驗方式 從MLP的實驗可以看到attention is somehow meaningful Future work: 擴展實驗到QA tasks、不是英文的語言、請專家鑑定 Reference https://zhuanlan.zhihu.com/p/84490817 https://medium.com/@yuvalpinter/attention-is-not-not-explanation-dbc25b534017","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Darcy"},{"title":"Emotion-Cause Pair Extraction - A New Task to Emotion Analysis in Texts","slug":"Emotion-Cause Pair Extraction - A New Task to Emotion Analysis in Texts","date":"2020-06-24T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/06/24/Emotion-Cause Pair Extraction - A New Task to Emotion Analysis in Texts/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/06/24/Emotion-Cause%20Pair%20Extraction%20-%20A%20New%20Task%20to%20Emotion%20Analysis%20in%20Texts/","excerpt":"GoalEmotion cause extraction (ECE) 最原始的資料及論文來源其實在Event-Driven Emotion Cause Extraction with Corpus Construction這篇論文所提出的。 資料長相如下圖：","text":"GoalEmotion cause extraction (ECE) 最原始的資料及論文來源其實在Event-Driven Emotion Cause Extraction with Corpus Construction這篇論文所提出的。 資料長相如下圖： 原始目標 (過去本資料集相關論文的目標)： 給定emotion, 找出此emotion的cause (甚麼原因導致這個emotion) 本論文目標： 給一段落，找出數個pair的(emotion, cause)，意即沒有預先給定型emotion的資訊。本文稱此task叫做ECPE。 本目標也為此論文主打優點，畢竟可以不用”提供模型emotion”資訊，這在實際問題的應用上是非常重要的。 模型架構他的模型主要分為兩個階段。第一階段為”抽取emotion, cause”兩者的資訊，而第二階段為”將這些資訊組合及過濾”。 第一階段 - 抽取資訊我們先來看第一階段”抽取資訊”的模型架構，本文提出共三種架構： 第一種架構稱為”Indep”,想法很直觀，即將emotion, cuase分為兩個分支而成的模型，如下圖： 計算y及loss 另外兩種架構基本上是一樣的，只是他的”前提”不一樣，如下圖： 第二階段 - 組合與過濾 先將emotion, cause找出目前為候選人的短句兩兩配對組成P_all 每一組兩兩配對的組合，我們可以計算它的embedding s以及兩兩之間的距離 最後經由簡單的regression，得到此組合是否應該刪除或留下 剩餘的pairs即為最終的模型結果 upper bound對於Inter-CE, Inter-EC，其實有他的upper bound，也就是萃取的y直接換成ground truth。換句話說，在知道cause 的前提下預測emotion；或者相反。 Experimental results針對本篇論文提出的方法所整理的結果，其研究指出，”cause extraction”比起emotion困難許多，因此在已經給予cause的ground truth，emotion extraction的效果自然比較好；反之，則效果較差。另外，缺少ground truth前提的Inter-CE, Inter-EC表現自然也比bound系列差許多： 與其他方法的比較：(話說這裡我有個疑問，其他人都是給予”emotion”的前提，去預測”cause”，作者既然都做了Inter-EC-bound，怎麼不把它也拿進來比一下呢?) Reference this paper","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Jeff"},{"title":"Attention is not Explanation","slug":"Attention is not Explanation (2)","date":"2020-06-10T09:30:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2020/06/10/Attention is not Explanation (2)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/06/10/Attention%20is%20not%20Explanation%20(2)/","excerpt":"Attention is not Explanation","text":"Attention is not Explanation 原始論文在此 tags: ‘NLP’, ‘paper’GoalLi et al. (2016)曾提到：attention對neural models提供了一個重要的解釋依據。也曾有多篇論文對此提出了佐證。 Li et al. (2016) summarized this commonly held view in NLP: “Attention provides an important way to explain the workings of neural models”. 而本篇論文，目標是希望評估attention weights是否真的這麼有解釋性。 In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. Introduction and motivation本篇作者認為，若attention真的這麼有解釋性，那麼應該可以看到以下兩個性質： attention weights應該要和其他feature importance measures（如:gradient-based measures, leave one out feature importance）的結果有高度相關。 打亂attention結構（如：把attention weight和其他人配對），應該要對模型在預測上造成相對應的影響。 因此本篇主要針對上述兩點提出一些相對應的驗證實驗，主要為以下兩方法：（後面會再詳細提） correlation between attention and feature importance measures（以下簡稱correlation） conterfactural attention wetghts （以下簡稱conterfactural） Assuming attention provides a faithful explanation for model predictions, we might expect the following properties to hold.(i) Attention weights should correlate with feature importance measures (e.g., gradient-based measures);(ii) Alternative (or counterfactual) attention weight configurations ought to yield corresponding changes in prediction (and if they do not then are equally plausible as explanations). We investigate whether this holds across tasks by exploring the following empirical questions. To what extent do induced attention weights correlate with measures of feature importance – specifically, those resulting from gradients and leave-one-out (LOO) methods? Would alternative attention weights (and hence distinct heatmaps/“explanations”) necessarily yield different predictions? Dataset &amp; Tasks此篇論文透過以下三個tasks（各task都有許多不同的datasets）來看在introduction中提到的兩件事（correlation及conterfactural）。 binary text classification natural language inference question answering 註：用attention較常見的task是seq2seq的翻譯，但在此篇沒有探討這件事（他們當作future work) Dataset:針對上述提到的三個tasks，此篇論文分別使用了以下dataset。1. binary text classification:Stanford Sentiment Treebank (SST), IMDB Large Movie Reviews Corpus, Twitter Adverse Drug Reaction dataset, 20 Newsgroups, AG News Corpus (Business vs World), MIMIC ICD9 (Diabetes), MIMIC ICD9 2. natural language inference:SNLI dataset 3. question answering:CNN News Articles, bAbI Experiments使用的模型架構：model architecture:T: sentence size|V|:dataset text size $x \\in R^{T \\times|V| }$: one hot encoded word at each position $x_e \\in R^{T \\times d }$: dense (d dimensional) token representations (word embedding) encoder(h=Enc($x_e$))$\\in R^{T\\times m}$: m-dimensional hidden states produced by consumeing the embedded tokens in order attention weight $$\\hat{\\alpha}=softmax(\\phi(h,Q)),$$ $\\phi(h,Q)={\\bf v^T}tanh({\\bf W_1 h+W_2Q}),$$\\phi(h,Q)=\\dfrac{hQ}{\\sqrt{m}}$ $Q\\in R^m$ is a query to sclar scores. A similarity function $\\phi$ maps h and a query $Q\\in R^m$ (e.g., hidden representation of a question in QA, or the hypothesis in NLI) to scalar scores, … $\\hat{y}=\\sigma(\\theta\\cdot h_\\alpha)\\in R^{|y|},$ where |y| denotes the label set size. $\\hat{h_\\alpha}=\\sum_{t=1}^T\\hat\\cdot h_t$ 1. correlation between attention and feature importance measures在此，作者分別計算attention與下列兩方法的correlation(先分別以leave one out、gradient based的方式算出一個代表feature importance的值，再計算Kendall τ) (1) gradient based measures of feature importance ($τg$)(2) differences in model output induced by leaving features out ($τ{loo}$). 整個計算流程如下： 註: 計算流程中, 會先將disconnect attention的計算圖, 也就是, 這邊並不考慮attention這部分的計算 Total Variation Distance (TVD): kendall τ Result 從下圖可看到，使用較簡單的feed-forward projection，意即比較簡單的encoder（Average），結果比BiLSTM相對好一點。（越靠近1表示相關度越高） 從下圖可以看出，Average(feed-forward projection,就是比較簡單的encoder)應比BiLSTM好。（越靠近1表示相關度越高） gradient和LOO（leave one out）的correlation比LOO與attention的correlation高（平均來看） gradient和LOO（leave one out）的correlation比gradient與attention的correlation高（平均來看) Average(feed-forward projection)的attention與LOO的correlation比BiLSTM的attention的correlation高（平均來看） 2. Conterfactural attention wetghts透過更換attention weight的方式，去看看對整個結果的影響。若更換attention weight並沒有對結果帶來太大影響，表示原始attention weight的大小並無法表示該位置的重要性。 Under the assumption that attention weights are explanatory, such counterfactual distributions may be viewed as alternative potential explanations; if these do not correspondingly change model output, then itis hard to argue that the original attention weights provide meaningful explanation in the first place. 此部分使用兩種方法去看： random permute attention weights 使用設計後的最佳化來計算出一組新的attention weights2.1 Attention permutation將attention weight隨機置換後，去看看最後output的差別。（將attention weight random permute 100次後，去計算和原始的output $\\hat{y}$的TVD。並取Median做為代表。 從下圖可看到，有許多case，當attention weight很大時，Median($\\triangle \\hat{y^p}$)仍然很小。但像diabetetes dataset，只有potitive的有影響。 2.2 Adversarial attention期望找出一組新的attention weight，使得 結果的差異小於某個「自行定義的很小的值 $\\epsilon$」 使得Jenseb-Shannon Divergence越大越好。(希望兩分不差越多越好)註：針對$\\epsilon$，在此篇論文中，text classification的task,作者設定$\\epsilon=0.01$;對QA dataset則設為0.05 也就是計算以下最佳化： 演算法如下： 從下圖結果可以看出，JSD其實都滿高的。值得一提的是，Diabetes datasets在positive的結果JSD比較低，這和figure4, figure6的結果一致。 conclusion 此篇論文主要針對attention有沒有解釋性提出兩個判斷依據： 去看attention weight和其他feature importance measures的correlation。（如果有解釋性的話，應該要有高的correlation） 去看counterfactual attention distribution對模型在預測上造成的影響。（若attention weight對原始位置有解釋性的話，應該要對結果有高的影響） 從這篇論文的實驗數據來看，correlation並沒有太高 counterfactual attention distribution對模型預測的差異並不大。 當使用較複雜的encoder時，attention weight的解釋力有更低的現象（依此論文的論點來看）。 這篇論文並沒有討論attention最常見的seq2seq task 這篇論文仍有許多限制: correlation的值要多好才能算好？ 論文中使用的其他feature importance measures的方法並不能表示絕對是最理想的方法 irrelevant features可能會對Kendall τ 造成干擾（但在figure 5中可看到，「BiLSTM和Average的attention」是呈較高相關的，且「gradient和LOO」的結果也成較高相關，因此干擾應不會造成太多影響） 從countergactual attention experiments可看到很重要的一點：我們不應該模型的特定input與特定的prediction有關。 以下為擷取幾段覺得滿重要的段落： We have reported the (generally weak) correlation between learned attention weights and various alternative measures of feature importance, e.g., gradients. We do not intend to imply that such alternative measures are necessarily ideal or that they should be considered ‘ground truth’. While such measures do enjoy a clear intrinsic (to the model) semantics, their interpretation in the context of nonlinear neural networks can nonetheless be difficult for humans (Feng et al., 2018). An additional limitation is that we have only considered a handful of attention variants, selected to reflect common module architectures for the respective tasks included in our analysis. We have been particularly focused on RNNs (here, BiLSTMs) and attention, which is very commonly used. It is also important to note that the counterfactual attention experiments demonstrate the existence of alternative heatmaps that yield equivalent predictions; thus one cannot conclude that the model made a particular prediction because it attended over inputs in a specific way. However, the adversarial weights themselves may be scored as unlikely under the attention module parameters. Furthermore, it may be that multiple plausible explanations for a particular disposition exist, and this may complicate interpretation: We would maintain that in such cases the model should highlight all plausible explanations, but one may instead view a model that provides ‘sufficient’ explanation as reasonable. 延伸閱讀Attention is not not Explanation","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"attention","slug":"attention","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/attention/"}],"author":"Puchi"},{"title":"Leetcode - Jump Game","slug":"Leetcode - Jump Game","date":"2020-06-09T14:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/06/09/Leetcode - Jump Game/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/06/09/Leetcode%20-%20Jump%20Game/","excerpt":"","text":"key idea : 既然能走到K, 那1~(K-1)都能走到。 Sol : DPPPPPPPPP SOL : DPPPPPPPP~","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Chris"},{"title":"Leetcode - Rotation","slug":"Leetcode - Rotation","date":"2020-06-04T14:37:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/06/04/Leetcode - Rotation/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/06/04/Leetcode%20-%20Rotation/","excerpt":"tags: Leetcode DSMI labRotate Array","text":"tags: Leetcode DSMI labRotate Array 觀察: 假設len(nums)=5，rotate 6次和rotate 1次的效果是一樣的=&gt;rotate k times is equivalent to rotate k%len(nums) Solution 1123456789class Solution(object): def rotate(self, nums, k): optimize_step&#x3D;k%len(nums) for i in range(optimize_step): first&#x3D;nums[-1] nums.insert(0,first) nums.pop() return nums Solution 2123456789class Solution(object): def rotate(self, nums, k): n &#x3D; len(nums) a &#x3D; [0] * n for i in range(n): a[(i + k) % n] &#x3D; nums[i] nums&#x3D; a return nums Solution 3123456class Solution: def rotate(self, nums, k): k &#x3D; k % len(nums) nums[k: len(nums)], nums[:k] &#x3D; nums[: len(nums) - k], nums[-k:] return nums Rotate List Define a linked list used in leetcode 123456789101112131415161718192021class ListNode: def __init__(self, val&#x3D;0, next&#x3D;None): self.val &#x3D; val self.next &#x3D; next def print_list(self): figure&#x3D;str(self.val)+&quot;-&gt;&quot; curr&#x3D;self.next while curr: figure&#x3D;figure+str(curr.val)+&quot;-&gt;&quot; curr&#x3D;curr.next figure&#x3D;figure+&quot;NULL&quot; print(figure) # 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULLLN_list&#x3D;[ListNode(5,None)]for i in [4,3,2,1]: LN_list.append(ListNode(i, LN_list[-1]))Input&#x3D;LN_list[-1] Input.print_list() Solution1234567891011121314151617181920212223242526272829303132 class Solution(object): def rotateRight(self, head, k): &quot;&quot;&quot; :type head: ListNode :type k: int :rtype: ListNode &quot;&quot;&quot; if (k&#x3D;&#x3D;0 or head is None): return head if head.next is None: #Linked List 長度為1 return head #計算有幾個node: count&#x3D;head length&#x3D;0 while count: count&#x3D;count.next length+&#x3D;1 #開始轉 optimize_steps&#x3D;k%length #轉到第length 步的時候就根本來的一樣了 for i in range(optimize_steps): curr&#x3D;head while (curr.next.next): curr&#x3D;curr.next temp&#x3D;curr.next #本來的最後一個node curr.next&#x3D;None #本來的倒數第二個node接None temp.next&#x3D;head head&#x3D;temp #本來的最後一個node接到最前面 return head Rotate Image Solution123456789101112class Solution(object): def rotate(self, matrix): size&#x3D;len(matrix) history&#x3D;&#123;&#125; for i in range(size): for j in range(size): history[(j,size-1-i)]&#x3D;matrix[j][size-1-i] if (i,j) in history: matrix[j][size-1-i]&#x3D;history[(i,j)] else: matrix[j][size-1-i]&#x3D;matrix[i][j]","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Darcy"},{"title":"Leetcode - Single Number","slug":"Leetcode - Single Number (1)","date":"2020-05-28T20:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/28/Leetcode - Single Number (1)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/28/Leetcode%20-%20Single%20Number%20(1)/","excerpt":"Leetcode - Single Numbertags: ‘meeting’, ‘Leetcode’","text":"Leetcode - Single Numbertags: ‘meeting’, ‘Leetcode’ 136. Single Number - Easy題目給一串integers，其中有一個數字只出現一次，其他的數字皆出現兩次。請找出只出現一次的數字。 程式123456789class Solution(object): def singleNumber(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; a&#x3D;2*(sum(set(nums)))-sum(nums) # print(a) return(a) 其他解法解法1 解法2 解法3 137. Single Number II - Medium題目給一串integers，其中有一個數字只出現一次，其他的數字皆出現三次。請找出只出現一次的數字。 程式123456789class Solution(object): def singleNumber(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; a&#x3D;3*(sum(set(nums)))-sum(nums) # print(a) return(a&#x2F;2) 260. Single Number III - Medium題目給一串integers，其中有兩個數字只出現一次，其他的數字皆出現兩次。請找出只出現一次的數字們（output的排序不影響正確與否）。 程式123456789101112131415161718192021222324class Solution(object): def singleNumber(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; a&#x3D;sorted(nums) c&#x3D;len(a) b&#x3D;[] if a[0]!&#x3D;a[1]: b.append(a[0]) for i in range(1,len(a)-1): if a[i]&#x3D;&#x3D;a[i-1]: continue elif a[i]&#x3D;&#x3D;a[i+1]: continue else: b.append(a[i]) if a[c-1]!&#x3D;a[c-2]: b.append(a[c-1]) return(b)","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Puchi"},{"title":"Adversarial Extra","slug":"Adversarial Extra","date":"2020-05-27T23:20:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2020/05/27/Adversarial Extra/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/27/Adversarial%20Extra/","excerpt":"","text":"AdversarialAdversarial example in some norm在之前的ZOO可以看到我們創造的是$norm_2$的adversarial example在JSMA創造的是$norm_\\infty$在EDA創造的是$norm_1$的例子， Adversarial examples in more type The use in CNN+RNN : The use in speech recognition The use in NLP with Sentiment Analysis,Fake-News Detection, Spam Filtering. The improve of the ZOO -&gt; AUTOZOO (Auto encoder based Z eroth Order Optimization Method for Attacking Black box Neural Networks)主要做的有兩點： 使用了更新的gradient算法:The Old oneThe New one : $b \\in (1, dimensions)$The Sucessful rate in iterations Use the dimension reduction:Here is the two methodLeft : AutoencoderRight: Biliner STRUCTURED ADVERSARIAL ATTACKADMM used to solve the subproblem‘ostrich’ is the original label, and ‘unicycle’ is the misclassified label. 將圖片分成一些區塊，並且將那些區塊做loss處理，並且用ADMM將方程式做處理，處理過後的方程式就不列上了，有點長連結在此","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"},{"name":"cv","slug":"cv","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/cv/"}],"author":"Yi-Wei"},{"title":"Adversarial - Black Box with Zero Order Optimization","slug":"Adversarial-Black-Box-with-Zero-Order-Optimization","date":"2020-05-21T17:20:00.000Z","updated":"2021-12-17T10:05:22.896Z","comments":true,"path":"2020/05/21/Adversarial-Black-Box-with-Zero-Order-Optimization/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/21/Adversarial-Black-Box-with-Zero-Order-Optimization/","excerpt":"","text":"Adversarial - Black Box with Zero Order OptimizationWhat is Adversarial attack?Original paper : EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLESIan J. Goodfellow, Jonathon Shlens &amp; Christian Szegedy Google Inc., Mountain View, CA ICLR 2015 Attack settings : White or BlackWhite box : 你知道他的結構以及可以使用他中間的參數, 可以使用BP去計算Black box : 你並不知道他是用什麼的結構去training, 只知道他最後的output的機率分佈gray box : 就是介於白盒和黑盒之間(例如：你可能知道一半的架構或是只知道一半的機率分佈等等…)restricted box : 以圖片來說可能你根本什麼都無法知道（我也不知道這是什麼鬼，沒研究） The Introduction White Box Attack (最有名的幾個例子) Fast gradient sign method (FGSM): Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.2014. Explaining and harnessing adversarial examples.Use the backpropagation from the Target DNN, stepwised adding a little noise.這裡給一個知乎的詳解，有興趣的可以點開來詳細觀看 Jacobian-based Saliency Map Attack (JSMA) : Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarialsettings.Constructing a Jacobian-based saliency map for characterizing the input-output relation of a targeted DNN.知乎 DeepFool : Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.Deepfool: a simple and accurate method to fool deep neural networks.Callculate the Euclidean distance by the dicision boundary for update same with FGSM. Carlini &amp; Wagner (C&amp;W) Attack : Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP).這個是最重要的一個 C&amp;WThe most strong white box attack with logit layer and L2 norm. 有攻擊就有防守 Defend method Detection-based defense : shown to be effective in detecting adversarial examples by projecting an image to a confined subspace(e.g., reducing color depth of a pixel) to alleviate the exploits from adversarial attacks.(C&amp;W attacked) Gradient and representation masking : using distillation based on the original confidence scores for classification (also known as soft labels) and introduced the concept of “temperature” in the softmax step for gradient masking.And the representation masking aims to replace the internal representations in DNNs (usually the last few layers) with robust representations to alleviate adversarial attacks.(C&amp;W attacked Gradent masking). Adversarial training : adversarial examples are jointly used to stabilize training, known as the data augmentation method.(Good at defense FGSM and C&amp;W). Hint : model capacity increasing can make the model strong. The Black Box attack論文：(ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute) 通常來說黑盒攻擊通常會比白盒攻擊來得差，但這篇提出了ZOO的演算法讓黑盒能夠做的跟白盒一樣好。Main abstract: Use an effective black-box attack that is also only has access to the input images and the output with the confidence scores of the targeted DNN ZOO is to directedly estimate the gradients of the targeted DNN for generating adversarial examples. In the experiment results on MNIST, CIFAR-10, ImageNet show the ZOO attack is as effective as the state-of-the-art white box attack. C&amp;W attack (white box)最主要的部分就是他會提取倒數第二層來使用（logit層） The Black box attack為了讓黑盒接近白盒，故讓其maximize的function 也一致化。但我們需要做兩件事情 更換logit層, 在黑盒攻擊的時候並不會知道logit層的長相，只會有最後的機率分佈，故我們加上一個log讓他趨近於白盒的樣子。 BP solving 方法 ： 由於我們不知道其網路架構內到底有什麼，所以我們需要使用另外一個演算法去推估該隊哪些點去做gradient.(ZOO algorithm) Zero Order OptimizationThe Black Box’s Optimization 啊！ 每個點都要微分一次 那麼假設圖片是$64⋅64⋅3$那麼我每一個epoch的微小變動就要跑 $2⋅64⋅64⋅3$ = 24596次/epoch這樣實在是太昂貴了，所以我們需要使用 Stochastic coordinate descent每次的 iteration, 都變成是隨機取點去minimize the objective function.（本篇文章是使用 batch_size = 128 去優化） 當然這裡的optimization 也可以使用ADAM去做優化。由於是一般的gradient你也可以選擇使用最簡單的Newton’s去做迭代。這裡附上兩個演算法，應該不會有人沒看過ADAM吧ADAM快過Newton就跟Classification不再用SVM一樣 本篇會使用到的技術 Dimension reduction :Q:圖片太大導致運算太慢怎麼辦？ 前面有講到圖片的大小，若圖片太大也不好處理，而且會導致迭代速度慢，那我們就希望將輸入圖片維度降維Let $△x=x−x_0∈ℝ^𝑝，𝐷(𝑦)∈ ℝ^𝑚，𝑚&lt;𝑝$Replace the $𝐷(𝑦)$ into $△x$ Hierarchical Attack(分層攻擊)：Q: 該在哪個維度下使用？ 在高維度的情況下，你可能需要很長的時間才能找到Adversarial Example. 在低維度的情況下，你可能會根本找不到在這個有限的空間內。 應用降維技術可以將$229⋅229⋅3$的圖片變成$32⋅32⋅3$，但會導致訊息可能不夠，故應用分層攻擊，漸漸地增加你的維度，在你的loss的下降已經很不明顯的時候，可以看到下圖維度增加的時候變化也幾乎都在貝果的周圍附近。 Important Sampling:Q : 是否在迭代時做了很多多餘的迭代？由於迭代每個點是很貴的，那為什麼我們不去迭代那些重要的點就好？例如：貝果來說，我們想要改變的應該是貝果本身或附近的點，對於角落的背景應該是沒有任何影響的，但若我們隨機迭代，很有可能會選到很差的點。 將圖片分割成$8*8$的區塊，然後去計算他的重要性質，若比較重要的點，他的改變率就會較高，若不重要那他的改變率就會比較低。 我們對每個區域中的絕對像素值變化進行max pooling，向上採樣至所需尺寸，然後對所有值進行normalized，機率總和為1。 每進行幾次迭代，我們就會根據最近的變化來更新這些採樣概率 。 The reset ADAM states : ADAM states only when $c⋅f (x,t)$ reaches 0 for the first time. 你真的好棒棒時間 Attack on MNIST and CIFAR-10Result : Success Rate and the $𝐿_2−𝑑𝑖𝑠𝑡𝑜𝑟𝑡𝑖𝑜𝑛$ is close to the C&amp;W and have a reasonable avg time.FGSM ($L_∞−𝑑𝑖𝑠𝑡𝑜𝑟𝑡𝑖𝑜𝑛$) use with substitute model. ImageNetRunning 1500 iteration, attack base on $32⋅32⋅3$(original image $229⋅229⋅3$), and don’t use the Hierarchical Attack. The average $𝐿_2−𝑑𝑖𝑠𝑡𝑜𝑟𝑡𝑖𝑜𝑛$ is 3 times than white box, but adversarial images are still visually indistinguishable. 3.The importance of the Technique 黑色曲線的下降非常緩慢，這表明分級攻擊對於加速攻擊非常重要(左圖) 重要性採樣也有所不同，尤其是在10,000次迭代之後(右圖) 上上圖清楚顯示了重置ADAM狀態的好處，如果不重置狀態，最終失真和損耗會明顯增加。 result: ZOO攻擊成功地將原始類別的概率降低了160倍以上（從97％降低到約0.6％），同時將目標類別的概率提高了1000倍以上（從0.0006％提高到超過0.6％) The Future work 持續減少計算時間。 用黑盒製造對抗性資料拿去當training有助於robustness。 生成對抗性資料不一定只能在CNN上，RNN或是其他的應用都可以使用到。","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"},{"name":"cv","slug":"cv","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/cv/"}],"author":"Yi-Wei"},{"title":"Hierarchical Attention Networks for Document Classification","slug":"Hierarchical Attention Networks for Document Classification","date":"2020-05-20T00:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/20/Hierarchical Attention Networks for Document Classification/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/20/Hierarchical%20Attention%20Networks%20for%20Document%20Classification/","excerpt":"參閱網站: 論文速速讀: Hierarchical Attention Networks for Document Classification","text":"參閱網站: 論文速速讀: Hierarchical Attention Networks for Document Classification","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"john"},{"title":"Leetcode - Parentheses","slug":"Leetcode - Parentheses","date":"2020-05-18T15:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/18/Leetcode - Parentheses/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/18/Leetcode%20-%20Parentheses/","excerpt":"","text":"20. Valid Parentheses (easy)Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. An input string is valid if: Open brackets must be closed by the same type of brackets.Open brackets must be closed in the correct order.Note that an empty string is also considered valid. example 1Input: “()”Output: true example 2Input: “()[]{}”Output: true example 3Input: “(]”Output: false example 4Input: “([)]”Output: false example 5Input: “{[]}”Output: true Solution Stack!!!!!! 22. Generate Parentheses (medium)Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()”] Solution Backtracking!!!!!! x : 紀錄’(‘的個數l : 紀錄這x個’(‘中，還有幾個沒有match到 ‘)’ 32. Longest Valid Parentheses (Hard)Given a string containing just the characters ‘(‘ and ‘)’, find the length of the longest valid (well-formed) parentheses substring. example 1Input: “(()”Output: 2Explanation: The longest valid parentheses substring is “()”example 2Input: “)()())”Output: 4Explanation: The longest valid parentheses substring is “()()” Solution 還是stack!!!!!! stack_list : 紀錄’(‘的index。","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Chris"},{"title":"Leetcode - Dynamic Programming","slug":"Leetcode - Dynamic Programming","date":"2020-05-14T21:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/14/Leetcode - Dynamic Programming/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/14/Leetcode%20-%20Dynamic%20Programming/","excerpt":"Dynamic Programming動態規劃 Dynamic Programming :idea : 動態規劃(Dynamic Programming)是指將一個較大的問題定義為較小的子問題組合，先處理較小的問題並將結果儲存起來(通常使用表格)，再進一步以較小問題的解逐步建構出較大問題的解。","text":"Dynamic Programming動態規劃 Dynamic Programming :idea : 動態規劃(Dynamic Programming)是指將一個較大的問題定義為較小的子問題組合，先處理較小的問題並將結果儲存起來(通常使用表格)，再進一步以較小問題的解逐步建構出較大問題的解。 Easy solution in Fibonacci Squence or Leetcode(70 :Climbing Stairs.) First we can do it in recusive part, but no memory save.The time limit exceeded! Because we do more time in the repeated work, and the Time Complexity is $O(2^n)$. we can do in the memory part, recursive.Time Complexity is O(n). we can do the DP(Dynamic programming). 經典動態規劃問題 Shortest path problem in weighted directed graph (negative edge allowed but no negative cycles) Question : Find the minimum path in th graphapproch : (forward or backward)Here is the forward DPlet $f(k)$ is the shortest path to the k point.$f(k) = 0$$W(k-1,k)$ is the distance from $k-1$ to $k$recursive with $f(k-1) = min{f(k) + W(k-1,k)}$ Time Complexity : O(|V|+|E|) The Longest Common Subsequence (LCS) Problem:Question : Give two Sequences $X = &lt;x_1,x_2,….x_n&gt;, Y = &lt;y_1,y_2,…y_m&gt;$ find the Longest common subsequence(words not need to consecutive)solution :example for LCS","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Yi-Wei"},{"title":"Overview for Semi-Supervised Learning","slug":"Overview for Semi-Supervised Learning","date":"2020-05-14T15:40:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/14/Overview for Semi-Supervised Learning/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/14/Overview%20for%20Semi-Supervised%20Learning/","excerpt":"What is Semi-Supervised Learning?When you counter some real problem…","text":"What is Semi-Supervised Learning?When you counter some real problem… 我們先來快速看一下supervised 和semi-supervised 的差別 self-training最常用且最簡單的概念及手法，任何模型都可以很easy的套用 Teacher-Student co-training常用的概念及手法之二，只要你有多個feature就可以使用 S3VM前身為TSVM，是一個以SVM為基礎建構的半監督式學習手法。 Generative Models藉由沒有label的data調整Gussian分布 深度學習中，最早由Google所提出Deep Generative Model，但其實效果沒有到很好。而後，許多論文開始依照這樣的概念著手，Semi-Supervised Learning with Ladder network便是半監督式學習中，曾經風靡一時的論文。 Clustering Space偷偷推廣我的論文😛藉由自定義loss function已告訴模型分群空間中的分群好壞，強制network學習一個好的分群空間。這裡就不贅述了，大家有興趣之後可以參加我的口試囉😎 Follow up - 新時代的SSL (Self-Supervised Learning)⚠️這不是self-training⚠️⚠️這不是self-training⚠️⚠️這不是self-training⚠️#很重要所以說三遍 Then, what is self-supervised learning?????? Self-Supervised Learning的前提假設是沒有任何的labeled data！！！那麼…這要怎麼訓練模型呢…? 既然沒有任何的labeled data…自己的data自己標🙃🙃🙃 思考出發點就像剛出生的嬰兒，沒有人教導前，僅藉由”觀察”進行學習。在這樣的學習中，或許他認得”手”也辨別得出來”手”，但他並不知道他所看到的就是”手”。而Self-Supervised Learning便是用這樣的概念所建構的方法。希望模型能從毫無label的狀況下，”觀察”並”學習”。如此一來，未來只要稍加提點，或許就能得到很好的效果。 經典例子 BERT CV Reference Semi-supervised Learning 簡介 - self-supervised learning的近期發展 100篇self-training Deep Generative Model Semi-Supervised Learning with Ladder network 三顾 Semi-supervised Learning with Ladder Network Facebook Believes in Omni-Supervised Learning","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"semi-supervised","slug":"semi-supervised","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/semi-supervised/"}],"author":"Jeff"},{"title":"BERT - Bidirectional Encoder Representations from Transformers","slug":"BERT - Bidirectional Encoder Representations from Transformers","date":"2020-05-13T11:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/13/BERT - Bidirectional Encoder Representations from Transformers/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/13/BERT%20-%20Bidirectional%20Encoder%20Representations%20from%20Transformers/","excerpt":"","text":"Main concept of BERT利用不同的任務來pre-train模型，當使用者需要使用時，可以直接把BERT串接到自己的任務上，再稍微針對自己的模型訓練一下(fine tune)即可。這就像是先學了一堆基礎的先輩知識，再去學高微時你就覺得輕鬆自在。 Training tasks of BERTBERT的pre-training有兩項主要的訓練方式。hugging face已經幫我們預訓練好各國語言的模型(含中文)，因此大家可以直接下載他們訓練好的模型參數直接使用。 BERT 模型架構採用transformer Training task 1 - Predicting mask words隨機遮罩15%的字，模型需要預測這些是那些字。 Training task 2 - next sentence classifier給予兩句話，模型需要判斷這兩句話是否為前後關係。 How to use BERTBERT input formulataggings首先要先了解瞭面的tagging所代表的意義 [CLS]：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr. [SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔 [UNK]：沒出現在 BERT 字典裡頭的字會被這個 token 取代 [PAD]：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算 [MASK]：未知遮罩，僅在預訓練階段會用到 input embedding在BERT的使用中，你需要有三種embedding，最後把三種embedding合併起來當作完整的input token embedding : 代表識別每個 token 的index，用 tokenizer 轉換即可 segment embedding : 用來識別句子界限。第一句為 0，第二句則為 1。另外注意句子間的 [SEP] 為 0 position embedding : 用來界定自注意力機制範圍。1 讓 BERT 關注該位置，0 則代表是 padding 不需關注 (通常使用狀況為：在補上[PAD]tagging時，我們會在該位置將此embedding設置為0，其他句子的部分則設置為1) Four main usage of BERTBERT提供了四種fine tune模型的使用方式給大家，使用時也不需要自己重新架設及串接，你只需要指定你要接哪一種，你的output class要幾類，他其實都幫你寫好囉😍！(當然，如果這四種都不是你想要使用的，你就必須自己刻一下了) 使用方式a : sentence pair classifier 使用方式b : single sentence classifier 使用方式c : Question Answering *這裡要注意的是，output為paragraph的start index及end index，若end inddex比start還前面，則答案會回傳此題無解。 使用方式d : sentence tagging 沒有足夠強大的gpu來fine tune怎麼辦？ 抽取word embedding 直接把文字餵進BERT取得embedding，再用這些embedding做你的任務！ 使用colab，雖然跑比較慢，但至少能+-用 BERT 實驗結果 Follow up - How to use BERT (for code)大家可以參考進擊的BERT：NLP界的巨人之力與遷移學習，不過這篇有段時日了，部分使用BERT的指令及方法有更新過，因此不能完全直接使用他所提供的code。我有空再整理我的code更新到這裡給大家@@ Reference BERT 論文 進擊的BERT：NLP界的巨人之力與遷移學習 陳縕儂教授 - Contextual Word Embeddings ELMo BERT 解析","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Jeff"},{"title":"Data argumentation method - Attentive Cutmix & Cutmix","slug":"Data argumentation method - Attentive Cutmix & Cutmix","date":"2020-05-12T00:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/12/Data argumentation method - Attentive Cutmix & Cutmix/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/12/Data%20argumentation%20method%20-%20Attentive%20Cutmix%20&%20Cutmix/","excerpt":"介紹兩篇對於data argumentation的技巧，Attentive CutMix和CutMix: 論文速速讀: Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification 論文速速讀: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features","text":"介紹兩篇對於data argumentation的技巧，Attentive CutMix和CutMix: 論文速速讀: Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification 論文速速讀: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"cv","slug":"cv","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/cv/"}],"author":"John"},{"title":"NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE","slug":"NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE","date":"2020-05-06T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/06/NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/06/NEURAL%20MACHINE%20TRANSLATION%20BY%20JOINTLY%20LEARNING%20TO%20ALIGN%20AND%20TRANSLATE/","excerpt":"NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE","text":"NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE 論文原文在此。 Remark:“LEARNING TO ALIGN AND TRANSLATE” means “attention”. target task in this paperMachine translation system (English-to-French translation) Let $x= [ x_1, x_2, …, x_{T_x} ]$ and $y= [ y_1, y_2, …, y_{T_y} ],$ where $T_x$ and $T_y$ denote the length of source sentence and target translation sentence, respectively. Goal: $arg \\max_y p(y|x)$ contribution“Attention!” They proposed a novel architecture for Neural Machine Translation (NMT) model called RNNsearch. 此篇論文有提到“soft alignment”，以前的模型通常會需要相對應的位置，但此模型可以放寬一點 What’s the main issue in this paper?在過去的機器翻譯領域，encoder-decoder模型是普遍的模型架構。而過去的架構，都是將原input句全部的資訊放進encoder，生成一個fixed-length vector，並使用同一個fixed-length vector，傳往decoder。這類模型，在碰到長句的input時，模型output出的結果通常不太好。 在此篇論文中，作者認為只使用固定的vector作為decoder的輸入，是造成模型成效不彰的關鍵。 當input sequence過長的時候，無法確保context vector的dimension夠大使得有足夠的能力記住所有information 以往的模型長相（以下圖片來自原作者的演講投影片）： encoder: decoder: （圖片來自此投影片） How do the authors solve this issue in this paper?引入attention的概念。They extract the most relevant information from the original input sentence rather than the whole sentence. Model architecture in this articleThey use a Bidirectional RNN as their encoder and extend attenction mechanism to the decoder. Encoder:使用Bidirectional RNN，並將兩個方向得到的 hidden states: $\\overrightarrow{h_i}$, $\\overleftarrow{h_i}$ 直接 concatenate起來。(i.e. $h_i= [ \\overrightarrow{h_i} ; \\overleftarrow{h_i} ]$) (圖片來自原作者的演講投影片） Alignment model (圖片來自原作者的演講投影片） 這裡的$v$, $W$, $V$是weight matrix（要train的參數）。 (圖片來自原作者的演講投影片） Decoder: (圖片來自原作者的演講投影片） (圖片來自原作者的演講投影片） This can be understood as having a deep output (Pascanu et al., 2014) with a single maxout hidden layer (Goodfellow et al., 2013). Experiment We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model. 使用minibatch stochastic gradient descent、Adadelta。並使用Beam search去得出最佳的翻譯。 dataset: WMT’14實驗內容：與RNN Encoder-Decoder (RNNencdec)比較。兩個模型（RNNencdec、RNNsearch）皆分別使用「句子長度少於30字的資料」、「句子長度少於50字的資料」的去train模型。 從下圖可知，此論文提出的模型-RNNsearch，結果不論是以30字訓練的模型（RNNsearch-30）或50字訓練的模型（RNNsearch-50）都比RNNencdec來得好。且RNNsearch-50幾乎不受input句子長度影響結果。 )(圖片引自原論文) 此論文提出的模型，下面的圖可以看到相較於hard-alignment，soft-alignment的優點–可抓出不同語言中句構的特性（如：英文與法文的名詞、形容詞排序相反） (圖片來自原作者的演講投影片） 作者列出幾個實際英翻法的翻譯例子，指出RNNencdec在長句翻譯容易在後面失焦，而本篇提出的RNNsearch則否。 )(圖片引自原論文) 補充：此方法的限制若換個task，input太長，此方法會造成計算量太大。 Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words. However, this may limit the applicability of the proposed scheme to other tasks. Attention時序表 ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE ACL 2015 Effective Approaches to Attention-based Neural Machine Translation NIPS 2017 Attention Is All You Need 補充：其實這篇論文和acl的這篇，方法非常相近，且發表時間也很近。但acl這篇比較慢一點。在acl這篇，作者也有特別提到他們的方法和iclr這篇的差異主要有三點Effective Approaches to Attention-based Neural Machine Translation Reference https://www.youtube.com/watch?v=wyQBfi6uOHk https://aisc.ai.science/static/slides/20181018_XiyangChen.pdf https://arxiv.org/pdf/1212.5701.pdf","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"},{"name":"language model","slug":"language-model","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/language-model/"}],"author":"Puchi"},{"title":"Enriching Word Vectors with Subword Information","slug":"Enriching Word Vectors with Subword Information","date":"2020-05-06T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/05/06/Enriching Word Vectors with Subword Information/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/05/06/Enriching%20Word%20Vectors%20with%20Subword%20Information/","excerpt":"Enriching Word Vectors with Subword Information","text":"Enriching Word Vectors with Subword Information 前情提要:傳統上在訓練word embeddings時是用distinct vector來表示一個vocabulary。而這樣會忽略word的internal structure (e.g. 某些語言有相當豐富的詞形變化，像是西班牙文就高達40種) 這篇主要就是考慮internal structure 後再去做訓練 subword Model將每個word $w$ 用 bag of character $n$-gram 來表示 add special boundary symbols &lt; and &gt; at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences. include the word $w$ itself in the set of its $n$-gramexample (n = 3) word character $n$-grams &lt;where&gt; &lt;wh, whe, her, ere, re&gt; 、&lt;where&gt; represent a word by sum of the vector representation of its $n$ -gram ExperimentHuman similarity judgementword similarity model and baseline: baseline: skipgram(sg)、cbow model: 針對在DataSet內有出現，但沒在Training Data 出現的字(i.e. out-of-vocabulary) , 採以下兩種方式: model name 說明 sisg- use null vectors for these words sisg taking the sum of its n-gram vectors evaluate function利用 Spearman’s rank 計算 human judgement 和 cosine similarity between the vector representations 的相關性 Result: 第一列為 語言 第二列為 DataSet 還有跟一些其他的model做比較: 前四個model和後兩個model的Training Data 不同 Word analogy tasks$A$ is to $B$ as $C$ is to $D$ Result : Accuracy of word analogy tasks 第一列為 語言 第二列為 DataSet 在 syntactic tasks 有大大的進步在 semantic questions 幾乎沒什麼進步 Language Modelmodel:用LSTM架構來訓練，LSTM的細節請參考論文5.6 Result: Test perplexity on the language modeling task計算test perplexity 時有幾種不同的方法 列表如下: model name 說明 LSTM model without using pre-trained word vectors sg model with word vectors pre-trained without subword information sisg model with our vectors Effect of the size of the training data Effect of the size of n-grams Nearest neighbors Character n-gram and morpheme想要檢驗 一個字中最重要的n-gram 是否為語素 Result:","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Chuck"},{"title":"Transition-Based Parsing for Deep Dependency Structures","slug":"Transition-Based Parsing for Deep Dependency Structures","date":"2020-04-29T16:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/29/Transition-Based Parsing for Deep Dependency Structures/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/29/Transition-Based%20Parsing%20for%20Deep%20Dependency%20Structures/","excerpt":"","text":"NLP : Transition-Based Parsing for Deep Dependency StructuresURL : https://www.aclweb.org/anthology/J16-3001.pdf?fbclid=IwAR2cuvWAgeuFM5IoQE8Rouzx9VE_pImvFS46so7a51DFTJMadzpmzdJpBc8 前景提要: Greedy Transition-Based Parsing https://zhuanlan.zhihu.com/p/59619401?fbclid=IwAR1GlFEfpDfUj-BbvZPhWPslp-K4UXeJDOomfTvBD764B_lUuss5Yq6qndQ https://blog.csdn.net/kunpen8944/article/details/83349880?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-12-1 Notations A dependency graph G = (V, A) is a labeled directed graph, such that $x = w_{1}, \\cdots, w_{n}$ the following holds: $V = {0,1,2,\\cdots, n}$ $A \\subseteq V\\times R \\times V$ $0$ represents a virtual root node $w_{0}$, and all others correspond to words in $x$. An arc $(i,r,j)\\in A$ is a dependency relation $r$ from head $w_i$ to dependent $w_j$. Assume $R$ is singleton (consider unlabeled prasing) ($A \\subseteq V \\times V$). Define a transition system for dependency parsing as a quadruple $S = (C, T, c_{s}, C_{t})$ where, $C$ is a set of configurations, each of which contains a buffer $\\beta$ of remaining words and a set $A$ of dependency arcs. T is a set of transitions, each of which is a function $t : C \\rightarrow C$. $c_{s}$ is an initialization function, mapping a sentence $x$ to a configuration with $\\beta$ = [1,$\\cdots$, $n$] $C_{t} \\subseteq C$ is a set of terminal configurations. Oracle Sequence Given a sentence $x = w_{1}, \\cdots, w_{n}$ and a graph $G=(V, A)$ on it, if there is a sequence of transitions $t_{1}, \\cdots, t_{m}$ and a sequence of configurations $c_{0}, \\cdots, c_{m}$ such that $c_{0} = c_{s}(x)$, $t_{i}(c_{i-1}) = c_{i}(i = 1,\\cdots, m)$, $c_{m} \\in C_{t}$, and $A_{C_{m}}=A$. $\\bar{A_{c_{i}}} = A - A_{c_{i}}$ the arcs to be built of $c_{i}$. Denote a transition sequence as either $t_{1,m}$ or $c_{0,m}$. 2-2 Naive Spanning and Locality Choice of $Link$: adding the arc $(i, j)$ for $(j, i)$ adding no arc at all. Time complexity : $\\theta(n^{2})$ 2-3 Online Re-ordering Handle crossing arc. Idea : Using SWAP transition. The system $S_{S} = (C, T, c_{s}, C_{t})$, where a configuration $c = (\\sigma, \\beta, A) \\in C$ $\\sigma$ : stack, $\\beta$ : buffer, $A$ : dependency relations. Set the initial configuration for a sentence $x = w_{1}, \\cdots, w_{n}$ to be $c_{s}(x) = ([], [1, \\cdots, n], {})$, and take $C_{t}$ to be the set of all configurations of the form $c_{i} = (\\sigma, [], A)$ for any $\\sigma$ and any $A$. Transitions SHIFT (sh) removes the front from the buffer and pushes it onto the stack. LEFT/RIGHT-ARC (la/ra) updates a configuration by adding $(j, i)$/$(i, j)$ to $A$ where $i$ is the top of stack, and $j$ is the front of the buffer. POP (pop) updates a configuration by poping the top of stack. SWAP (sw) updates a configuration with stack $\\sigma|i|j$ moving $i$ back to the buffer. Theoretical Analysis Given a sentence $x = w_{1}, \\cdots. w_{n}$ and a graph $G = (V, A)$ on it. Initial configuration $c_{0} = c_{s}(x)$ On the $i$-stap, let $p$ be the top of $\\sigma_{c_{i-1}}$, $b$ be the front of $\\beta_{c_{i-1}}$. Let $L(j)$ be the ordered list of nodes connected to $j$ in $\\bar{A}{c{i-1}}$ for any node $j \\in \\sigma_{c_{i-1}}$. Let $\\mathcal{L}(\\sigma_{c_{i-1}}) = [L(j_{0}), \\cdots, L(j_{l})]$ if $\\sigma_{c_{i-1}}=[j_{l}, \\cdots, j_{0}]$. If there is no arc linked to p in $\\bar{A}{c{i-1}}$, then we set $t_{i}$ to pop. If there exists $a \\in \\bar{A}{c{i-1}}$ linking $p$ and $b$, then we set $t_{i}$ to la or ra correspondingly. If there is any node $q$ under top of $\\sigma_{c_{i-1}}$ such that $L(q)$ precedes $L(p)$ by the lexicographical order, we set $t_{i}$ to sw. Otherwise, we set $t_{i}$ to sh. Let $C_{i} = t_{i}(c_{i-1})$; we continue to compute $t_{i+1}$ until $\\beta_{c_{i}}$ is empty. 2.4 Two-stack-Based System Handle crossing arcs. Idea : Temporarily move nodes that block non-adjacent nodes to an extra memory module. The System $S_{2S} = (C, T, c_{s}, C_{t})$, where a configuration $c = (\\sigma, \\sigma^{\\prime}, \\beta, A) \\in C$, contains a primary stack $\\sigma$ and a secondary stack $\\sigma^{\\prime}$. $c_{s}(x) = ([], [], [1, \\cdots, n], {})$ for the sentence $x = w_{1}, \\cdots, w_{n}$. $C_{t}$ to be the set of all configurations with empty buffers. Transitions MEM (mem) pops the top element from the primary stack and pushes it onto the secondary stack. RECALL (rc) moves the top element of the secondary stack back to the primary stack. Theoretical Analysis la, ra, pop is same as Online Re-ordering. After that, let $b$ be the front of $\\beta_{c_{i-1}}$. we see if there is $j \\in \\sigma_{c_{i-1}}$ or $j \\in \\sigma_{c_{i-1}}^{\\prime}$ linked to $b$ by an arc in $\\bar{A}{c{i-1}}$. If $j \\in \\sigma_{c_{i-1}}$, then we do a sequence of mem to make $j$ the top of $\\sigma_{c_{i-1}}^{\\prime}$. If $j \\in \\sigma_{c_{i-1}}^{\\prime}$, then we do a sequence of rc to make $j$ the top of $\\sigma_{c_{i-1}}$. When no node in $\\sigma_{c_{i-1}}$ or $\\sigma_{c_{i-1}}^{\\prime}$ is linked to $b$, we do sh. 2-5 ExtensionGraph with Loops Extend the system to generate arbitary directed graphs by adding a new transition. SELF-ARC: adds an arc from the top element of the primary memory module ($\\sigma$) to itself, but does not update any stack nor buffer. 3-1 Transition Classification A transition-based parser must decide which transition is appropriate given its parsing environment (i.e., configuration). A discriminative classifier is utilized to approximate the oracle function for a transition system $S$ that maps a configuration $c$ to a transition $t$ that is defined on $c$. Find the transition sequence $c_{0, m}$ that maximizes the SCORE: SCORE($c_{0,m}$) = $\\sum^{m-1}{i=0}SCORE(c{i}, t_{i+1})$ $SCORE(c_{i}, t_{i+1})=\\theta^{T}\\phi(c_{i}, t_{i+1})$where $\\phi$ defines a vector for each configuration-transition pair, $\\theta$ is the weight vector for linear combination. beam search to find $\\phi$.https://www.youtube.com/watch?v=RLWuzLLSIgw (Deeplearning.ai 吳恩達) Averaged perceptron algorithm update estimate parameters $\\theta$. https://www.dropbox.com/s/tkngkky93hdy2y7/SC_final_SVM.pdf?dl=0 （我＆祥瑄科導期末報告 p.5） 3-2 Transition Combination Problem : A majority of features for predicting an ARC transition will be overlapped with the features for the sucessive transition, this property significantly decreases the parsing accuracy. Solution : Combine every pair of two successive transitions starting with ARC and transform the proposed two transition systems into two modified one. Two cycle problem: The number of edges between any two words could be at most two in real data. If there are two edges between two word $w_{a}$ and $w_{b}$, it must $w_{a} \\rightarrow w_{b}$ or $w_{b} \\rightarrow w_{a}$. We call these tow edges a two-cycle. In our combined transitions, a LEFT/RIGHT-ARC transition should be apper before a non-ARC transition. Two Strategies: Add a new type of transitions to each system, which consist of a LEFT-ARC transition, a RIGHT-ARC transition, and any other non-ARC transition. (e.g., LEFT-ARC-RIGHT-ARC-RECALL for $S_{2S}$). Use a non-directional ARC transtion. We propose two algorithms, namely, ENCODELABEL and DECODELABEL. An ARC transition may add one or two edges depend on its label. If there are total $K$ possible labels in training data. Using strategy 1: Add additional transitions to handle the two-cycle condition. Based on experiments, the performance decreased when using more transitions. Must add $K^{2}$ transitions to deal with all possible types. Using strategy 2: Change the original edges’ labels and use the ARC(label)-non-ARC transition instead of LEFT/RIGHT-ARC(label)-non-ARC. Not only do we encode two-cycle labels, but also LEFT/RIGHT-ARC labels. Labels that do not appear only contribute non-negative weights while training, we can eliminate them without any performance loss. [THMM 出處:Titov, Ivan, James Henderson, Paola Merlo,and Gabriele Musillo. 2009. Online graphplanarisation for synchronous parsingof semantic and syntactic dependencies.](https://www.ijcai.org/Proceedings/09/Papers/261.pdf) Discussion:因該是說，原本的無法解決有交叉情況的relation。他就用了一個方法是能夠產生出交叉情形的relation。『還有就是他在尋找最好的transition sequence，他發現上下連續的feature會差不多，所以就合併兩成成一個』 In this experiment, we distinguish parsing models with and without transition combination.他好像有做六種可能的combine2s : two stackT : THMMstd : which do not combine an ARC transition (No transition combine)TC : transition combine{std , TC } X {T, S, 2S}[有無方法？] Ｘ [不同的transition?]恩恩對 就是他現在把兩個transition合其來變一個，比如(lr+pop）當作一個transition方式。std 就是lr pop就是個別兩種不同的transition方式。 這個把她留著ＸＤ 明天可能比較好講解ＸＤ 4 Tree Approximation:Induce tree backbones from deep dependency graphs, tree backbones can be utilized to train a tree parser which provides pseudo path features.idea : assign heuristic weights to all ordered pairs of words(all pairs of words), and then find the tree with maximum weights(finding the maximum spanning tree).nodes : $V$each possible edge : $edge(i,j), i,j \\in V$assign heuristic weight : $w(i,j)$all tree : $T$, maximum spanning tree $w(i,j) = A(i,j) + B(i,j) + C(i,j)$ the tree is informative only when the given graph is dense enough. Fortunately, this condition holds for semantic dependency parsing. 5 Conclusion總之就是在說明 使用tranistion combination &gt; none 使用2-stack &gt; stack $S^{rev} &gt; S$, $S^{rev}_{x}$ means processing a sentence with system $S_x$ but in the right-to-left word order. transition-based : produce general dependency graphs directly from input sequences of words, in a way nearly as simple as tree parsers. transition combination and tree approximation for statistical disambiguation state-of-the-art performance on five representative data sets for English and Chinese parsing.","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"}],"author":"Chi-Yu, Yi-Wei"},{"title":"How to use clustering to improve a language model","slug":"How Use clustering to improve a language model","date":"2020-04-23T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/23/How Use clustering to improve a language model/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/23/How%20Use%20clustering%20to%20improve%20a%20language%20model/","excerpt":"","text":"前情提要:question:在做 speech recognizer 時President Kennedy 和 precedent Kennedy 的發音很像該如何讓speech recognizer 知道答案為President Kennedy? answer Notation Meaning e.g. $H$ hypothesis President Kennedyprecedent Kennedy $D$ speech signal $P(H)$ language model 用 noisy channal model使找到的解答滿足: $\\hat H = \\underset{H}{\\arg\\max} P(H|D) = \\underset{H}{\\arg\\max} \\frac{P(D|H)P(H)}{P(D)} = \\underset{H}{\\arg\\max} P(D|H)P(H)$ 要如何優化 $P(H)$ , 使之更 generalization 便是今天要探討的問題 using clustering improving a language model模型假設 bigram model Markov assumption Notation Meaning Remark L corpus L = $w_1,…w_N$ $\\pi$ function assign word to cluster 目標find function $\\pi$s.t minimize $H(L, \\pi)$(白話翻譯: 找到一個分類函數 使之最小化 對下個字預測的不確定性) Question: 要如何最小化$H(L, \\pi)$?$H(L, \\pi) = -\\frac{1}{N}logP(w_{1,…N})$經過 課本 510-511頁一連串的推導後可得$H(L, \\pi) \\approx H(W)-I(c_1; c_2)$ Remark: $I(c_1; c_2)$: mutual information between adjacent clusters 因此 最大化 $I(c_1; c_2)$ 即可使 $H(L, \\pi) 最小$ Question: 要如何分群使$I(c_1; c_2)$的值最大?用bottom-up algorithm來分群，用mutual information loss 當作loss MI-loss($c_i$, $c_j$) = $\\sum_{c_k \\in C\\setminus{c_i, c_j}} I(c_k;c_i) + I(c_k;c_j) - I(c_k;c_i\\cup c_j)$ 每次迭代將loss最小的兩群做合併持續迭代到剩下k群為止(k為給定的參數 課本是給1000) Remark: 此演算法不能保證找到最佳解 Is cluster-based language model work?課本給出的結果如下: model perplxity cluster-based language model 277 word-based model 244 linear interpolation between the word-based and the cluster based model 236","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"clustering","slug":"clustering","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/clustering/"},{"name":"language model","slug":"language-model","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/language-model/"}],"author":"Chuck"},{"title":"Support vector machine (SVM)","slug":"Support vector machine (SVM)","date":"2020-04-22T20:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/22/Support vector machine (SVM)/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/22/Support%20vector%20machine%20(SVM)/","excerpt":"","text":"Support vector machine (SVM)author:Puchidate:2020.04.22 tags: ‘SVM’, ‘study group’SVM (Hard margin) SVM training時間遠低過DNN。 SVM可以將linear extend到non-linear（運用kernel trick） 有統計理論support目標（問題解釋）： 給定一個training dataset$$S={(x^i,y_i)|x^i\\in R^n, y_i\\in{-1,1},i=1,…,l}$$$$x_+\\in A_+\\quad iff\\quad y=1 \\quad and \\quad x_-\\in A_-\\quad iff \\quad y=-1.$$想要從training data的資訊找出一個function f:R^n-&gt;R 使得$f(x)&gt;0 \\rightarrow x\\in A_+$ and $f(x)&lt;0 \\rightarrow x\\in A_-$目標：預測新進來的data的label 最原始的SVM在解的最佳化問題：$$min_{ {\\bf x},\\gamma}\\dfrac{1}{2}|{\\bf w}|^2_2,$$$$s.t.\\quad y_i({\\bf w}^T{\\bf x}_i+b)&gt;=1,\\quad i=1,…,n.$$ Primal form and Dual formPrimal form:$$min_{ {\\bf x},\\gamma}\\dfrac{1}{2}|{\\bf w}|^2_2,$$$$s.t.\\quad y_i({\\bf w}^T{\\bf x}_i+b)&gt;=1,\\quad i=1,…,n.$$ Dual form:$$max_\\alpha\\quad 1^T\\alpha-\\dfrac{1}{2}{\\bf\\alpha}^TDAA^TD{\\bf\\alpha}$$$$s.t.\\quad 1^TD{\\bf\\alpha}=0,\\quad {\\bf\\alpha}\\geq 0.$$ Dual form推導：$$L(w,b,\\alpha)＝\\dfrac{1}{2}w^Tw+\\alpha^T(1-D(Aw+1b)),\\alpha\\geq 0$$$$\\dfrac{\\partial}{\\partial w}L(w,b,\\alpha)=w-ATD\\alpha=0$$$$\\dfrac{\\partial}{\\partial b}L(w,b,\\alpha)=a^TD\\alpha=\\sum_{i=1}^ly_i\\alpha_i=0$$ Recall Dual form : $\\max_{\\alpha} \\min_{w,b} L(w,b,\\alpha) \\quad s.t. \\quad \\alpha \\geq 0.$ So, we have Dual form:$$max\\quad 1^T\\alpha-\\dfrac{1}{2}\\alpha^TDAA^TD\\alpha$$$$s.t.\\quad 1^TD\\alpha=0,\\quad \\alpha\\geq 0.$$ 值得注意的是：$w=A^TD\\alpha=\\sum_{i=1}^{l}y_i\\alpha_iA_i^T$，所以原 linear classifier $f$ 可以寫$f(x)=\\sum_{i=1}^n\\alpha_iy_i{\\bf x_i}{\\bf x}^T_i+b \\equiv \\sum_{i=1}^nu_i{\\bf x_i}{\\bf x}^T_i+b$，且我們也可知用KKT condiction把b算回去。因此我們只要算出dual問題，就可帶回原本的問題。 為什麼要用Dual form去求解最佳化？因為Dual的objective function整個是一個concave function,且限制條件只有一個，計算起來容易很多。（注意：$max$ $\\theta(\\alpha)$ iff $-min$ $-\\theta(\\alpha)$，解concave,convex問題是一樣的） support vector的意義$\\alpha$與$w^Tx+b$相垂直，且$\\alpha_i\\geq 0$。當$\\alpha&gt;0$代表這些限制條件是active constraint。對整個限制條件才有影響。$\\alpha_i=0$是inactive constraint,也就是$w^Tx+b$嚴格大於0，因此有這個限制或沒有限制都一樣。透過$\\alpha_i$正負可知道。$\\alpha_i=0$的對整個結果沒有影響。support vector: $\\alpha_i&gt;0$，也就是在線上的。 (圖片來自老師SVM的投影片) Remark一個好的model應該要考慮到：model bias, model variance都應該小 Structural Risk Minimizationtraining error (衡量model bias) +VC error bound （衡量 model variance）實驗誤差和$|w|_2$ 和VC bound成正比$min$ VC bound iff $min$ $\\dfrac{1}{2}|w|_2^2$ iff max Margin KKT condiction SMO (Sequential Minimal Optimization) 由Microsoft的人提出，是早期的一種非常知名的加速計算SVM的方法，當然也可以用quadratic programming的方法來計算，但比較慢。(quadratic programming方法包含：Newton method, steepest descent, …等) 若對SMO詳細推倒有興趣，可參考這篇論文。 回想我們原本的最佳化問題(引用李育杰老師的投影片)： SMO核心想法：每次只挑出兩個 $\\alpha_i, \\alpha_j$ 搭配做更新，其餘固定不動。為什麼是兩個呢？因為需要有人來搭配，一個增加、一個減少，去固定改變的量(以下圖片引用李育杰老師的投影片) 經過一連串轉換後，原本的問題就轉變為以下(詳細數學推導可以去看論文，以下圖片引用李育杰老師的投影片)： 演算法流程：以下演算法引用自此投影片 Soft-Margin SVM (nonseparable case)為什麼需要Soft-margin SVM?因為有些問題，他不是linearly separable，也就是找不到$w,b$使得資料完全被分乾淨。（也就是說，primal problem是infeasible，Dual problem是umbounded） 因此我們給每一個training data都給一點點調整量$\\xi_i$，即可。$$y_i(w^Tx^i+b)\\geq 1-\\xi_i,\\quad \\xi_i\\geq 0, \\forall i.$$ 但仍希望調整量越少越好（除此之外，也記得：原始問題是讓margin越大越好） 1-norm soft margine: Primal form:$$\\min \\quad \\dfrac{1}{2}|w|^2+C\\sum_{i=1}^n\\xi_i,$$$$s.t.\\quad y_i(w^Tx_i+b)\\geq 1-\\xi_i,$$ $$\\qquad \\xi_i\\geq 0, i=1,…,n.$$ Remark:$1^T\\xi$ (1 norm measure of error vector) 就是training error Dual form:$$\\max \\sum_{i=1}^n\\alpha_i-\\dfrac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_j y_i y_j &lt;x_i,x_j&gt;$$$$s.t.\\quad 0\\leq\\alpha_i\\leq C, i=1,…,n,$$ $$ \\sum_{i=1}^n\\alpha_i y_i=0.$$ We call $\\xi_i\\geq 0$ the slack variable. 2-norm soft margin Primal form:$$\\min \\quad \\dfrac{1}{2}|w|^2+\\dfrac{C}{2}\\sum_{i=1}^n\\xi_i^2,$$$$s.t.\\quad y_i(w^Tx_i+b)\\geq 1-\\xi_i.$$ Dual form: （詳細推導可看此）Remark我們稱 $C$ 為 weighted varialbe。當我們做SVM的最佳化時，其實我們希望： 讓margin分越開越好 讓slack variable不要太大（不希望讓太多太遠的也跑進來） 當 $C$ 調大時，只要有一點的$\\xi_i$，就會讓目標函數放很大。亦即「很看重training error」，因此若有overfitting的現象，須將 $C$ 調小。 Remark:1-Norm SVM:$$min\\quad |w|_1+C1^T\\xi$$$$D(Aw+1b)+\\xi\\geq 1 $$$$\\xi\\geq 0$$ 等價於$$min \\quad 1s+C1^T\\xi$$$$D(Aw+1b)+\\xi\\geq 1 $$$$-s\\leq w\\leq s$$$$\\xi\\geq 0$$ How to tune $\\gamma$ and $C$?在李育杰老師的Model selection for support vector machines via uniform design這篇論文，提到可用uniform design的方法來tune。並提到：SVM Ｃ建議的range在$10^{-2}$及$10^4$，但RSVM需要大一點的Ｃ，約$10^0$到$10^{6}$。http://www.math.hkbu.edu.hk/UniformDesign/ 以下截圖自Model selection for support vector machines via uniform design： Non-linear SVM （圖片來自李育杰老師的投影片） 有些資料，在原始空間可能沒辦法很容易的用linear classifier去分開。Non-linear SVM的想法來自將原資料映射到某個較高維度的空間，在那個高維度空間去做分類（把資料轉換後，再做SVM的意思）。 藉著 $\\phi$ 將原資料做完映射後，classifier長相：classifier for primal form: $$f(x)=(\\sum_{j=1}^? w_j\\phi_j(x))+b$$ dual form:$$f(x)=(\\sum_{i=1}^l\\alpha_iy_i&lt;\\phi(x^i),\\phi(x)&gt;)+b=(\\sum_{i=1}^l\\alpha_iy_iK(x^i,x))+b$$ Kernel 此圖來自李育杰老師投影片 我們可以把kernel想像成「用不同的方式去描述資料」，kernel matrix裡的每k個row，就是第k筆資料與其他資料之間的相似度。（有n筆training資料,kernel matrix的size就是nxn） 常見的kernel function (以下截圖自此): RBF kernel （Radial basis or Gaussian) $k(x,y)=e^{-\\beta|x-y|^2}=e^{-\\frac{|x-y|^2}{2\\sigma^2} }, \\sigma\\in \\mathbb R\\setminus{0}.$ $\\beta$大小 v.s. RBF kernel:[註]：圖片來源在此 距離與kernel值的關係: [註]：圖片來源在此 可想像成用不同方式描述一筆資料（用和其他筆資料的相似度）。 Mercer condiction如果kernel function滿足Mercer condiction,就一定有相對應的non linear map。 SSVM (Smooth Support Vector Machine)SSVM在解的最佳化問題：$$\\min_{(w,b)\\in R^{n+1} }\\dfrac{C}{2}|p((1-D(Aw+{\\bf 1}b),\\beta))|^2_2+b^2$$重點：SSVM使用一個二次可微分的函數去近似原本的plus function，因此可以使用newton method去解這個最佳化問題，且SSVM是一個strongly convex的問題，因此有唯一解。 式子推導 constraint 轉成 unconstraint我們可透過以下方法將原本的constraint的問題轉成unconstraint問題。 Remark: 加上b只是為了數學上確保是strongly convex function (有unique solution).$\\xi=0$ or 小於$0$: 函數不用調整因為少了constraint，變數從原本的n+1+l變成n+1 原本不是二次可微分的的plus function 轉換成二次可微分的p函數 從上圖可以清楚看到 Plus function $(*)_+$ 不是二次可微分(紅色的圈圈點，不可微分)，所以不能用newton method去計算這個最佳化問題。我們會想去找一個二次可微的函數去近似plus function，讓我們能夠用newton method去解這個問題。 事實上，在訊號處理的領域常用sigmoid function去近似step function。而step function的積分就是plus function。因此希望透過sigmoid function的積分來近似plus function。 Remark:Sigmoid function: $\\dfrac{1}{1+e^{-\\beta x} }$Sigmoid function的積分：$$p(x,\\beta)\\equiv x+\\dfrac{1}{\\beta}log(1+e^{-\\beta x})$$ SSVM在做的事就是用$p(x,\\beta)$這個二次可微分的函數來取代原本的plus function。（這裡一直強調二次可微，主要是因為牛頓法需要二次可微）。 在此使用 Newton Armoijo Algorithm （之後再詳細寫） Remark:原本牛頓法其實就是$\\lambda=1$時。從$\\lambda=1$開始試，直到滿足Armijo rule為止。Armijo rule直觀的意思就是保證從原本的點走到新的點要下降一定的量。 此方法保證：不論起始值多少，都可在有限步找到解。且一定會在有限步求出解。（通常只要 6-8 iterations 就可收斂。） 我們現在知道SSVM是一個好算的方法，那我們再回頭去看要怎麼用SSVM去解non-linear SVM呢？先看一下原始的SVM: 注意：在這裡講的Dual指的是用 dual variable去替換primal variable。但我們仍然是在解primal problem。 extend到non linear:我們其實就是把$AA^T$用kernel去做替換（實際上原本的SVM就是linear kernel。） 我們可以仔細觀察一開始的SVM和經過dual varible替換後，式子的差別，事實上我們只差在input，因此我們解最佳化時，其實我們只需要改變我們的input就好。（求出classifier的參數後，記得代入的點也要先經過kernel轉換。） RSVM (Reduced Support Vector Machine)從上面我們知道，nonlinear SVM 的 classifier就是：$$f(x)=\\sum_{i=1}^l \\alpha_ik(x,A_i)+b，$$其實這個classifier就像是一個由$\\beta={1}\\bigcup{k(.,x^i)}_{i=1}^l$這組basis做線性組合而成的函數（注意$\\alpha_i$個數就是training data的個數） 因此RSVM在想的事情就是用subset of $\\beta$，其實也就是從原本的training data裡面抽出一些點，來從kernel看相似度。 (李育杰老師投影片) 註：當$l$越多，VC dimention就越大，因為你可以用更多的組合來組出classifier，但也可能造成overfitting。 Different tools for SVM可直接參考這篇文章：https://blog.csdn.net/weixin_43746433/article/details/97808078 台大林智仁教授libsvm -用SMO sklearn.svm.SVC sklearn.svm.NuSVC sklearn.svm.LinearSVC Reference https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-73003-5_299 林軒田教授機器學習技法 Machine Learning Techniques 第 3 講學習筆記 R筆記 – (14)Support Vector Machine/Regression(支持向量機SVM) sklearn-1.4. Support Vector Machines 支持向量机通俗导论（理解SVM的三层境界） 我所理解的 SVM（支持向量机）- 1 支持向量机(SVM)是什么意思？ 機器學習-支撐向量機(support vector machine, SVM)詳細推導 Support Vector Machines 簡介 CS229 Lecture notes paper Model selection for support vector machines via uniform design A Practical Guide to Support Vector Classification support vector machine Reduced Support Vector Machines: A Statistical Theory paper list:about loss function: Statistical behavior and consistency of classification methods based on convex risk minimization Statistical analysis of some multi-category large margin classification methods SMO: Sequential Minimal Optimization：A Fast Algorithm for Training Support Vector Machines","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SVM/"}],"author":"Puchi"},{"title":"Sequence to Sequence Learning with Neural Networks","slug":"Sequence to Sequence Learning with Neural Networks","date":"2020-04-22T15:12:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/22/Sequence to Sequence Learning with Neural Networks/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/22/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/","excerpt":"tags: study paper DSMI labpaper: Sequence to Sequence Learning with Neural Networks Abstract and Introduction DNN cannot be used to map sequences to sequences because they can only work when dimensionalty of input/output is fixed and known Task: English to French translation task from the WMT’14 dataset Proposed method 的優點: Minimal assumptions on the sequence structure: 甚麼樣的sequence 架構都可以處理 Sensitive to word order Does well on long senetances","text":"tags: study paper DSMI labpaper: Sequence to Sequence Learning with Neural Networks Abstract and Introduction DNN cannot be used to map sequences to sequences because they can only work when dimensionalty of input/output is fixed and known Task: English to French translation task from the WMT’14 dataset Proposed method 的優點: Minimal assumptions on the sequence structure: 甚麼樣的sequence 架構都可以處理 Sensitive to word order Does well on long senetances The model Goal: given an input sentance $(x_1,x_2,…,x_T)$ and its corresponding output sentacne $(y_1, y_2, …,y_{T’})$ (where $T$ need not equal to $T’$), want to estimate $p(y_1, y_2, …,y_{T’}|x_1,x_2,…,x_T)$ $p(y_1, y_2, …,y_{T’}|x_1,x_2,…,x_T)=\\Pi_{t=1}^{T’}p(y_{t}|v,y_1,…,y_{t-1})$, where $p(y_{t}|v,y_1,…,y_{t-1})$ is each distribution is represented with a softmax over all the words in the vocabulary 把 input sequence 倒過來餵進去效果比較好 Experiment Dataset: WMT’14 English to French dataset 12M sentences Vocabulary: 160000 most frequently used English words and 80000 most frequently used Frech words 沒有在vocabulary出現的字用”UNK”代替 test set (for evaluation): 1000-best lists generated by SMT system(baseline) Objective: maximizing the log probability of a correct translation $T$ given the source sentence $S$: $\\hat{T}=\\mathop{argmax}\\limits_{T}p(T|S)$ Left-to-Right beam search Reverse the source sentence: Imporoves the performance, but they don’t have a complete explanation XDD 當input sentence 被反過來之後，input sentence 的前幾個字和output sentence的前幾個字更近了，有助於output sentence 在一開始就有更精準的生成，後面生成的也會比較準 (類似好的開始就是成功的一半的概念?) Trianing details 1000 dimensional word embedding (但他沒有說word embedding是怎麼做的) LSTM: 4 layers, 1000 cells in each layers Parameter initialization from uniform distribution between -0.08 to 0.08 平行化計算: 總共使用8個GPU，訓練10天 Experimental Results最好的結果是ensemble不同random initialization 的LSTM 所得到的 Model analysis: 能夠分辨使用相同字但不同排序的句子，以及相同意思但使用不同文字表達的句子 對於長句的表現仍然良好(左圖: x軸是句子長度) 句子中如果有出現很多不常用的字，表現也能維持一定的水準(右圖: x軸是句子裡面出現的字的詞頻在整個vocabulary中的排名的平均) 補充 SMT system: Statistical Machine translation 通過對大量的平行語料進行統計分析，構建統計翻譯模型 不需要依靠語法規則，所以容易推廣到不同語言的翻譯工作 Word-based translation: 一個一個字翻 Phrase-based translation: 視情況將幾個字組起來變成詞彙來翻 Syntax-based translation: 使用句法分析(例如parsing tree)作為翻譯的依據 Hierarchical phrase-based translation: a combination of pharse-based and syntax based Beam search: 演算法細節再這裡","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Darcy"},{"title":"Effective Approaches to Attention-based Neural Machine Translation","slug":"Effective Approaches to Attention-based Neural Machine Translation","date":"2020-04-22T15:12:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/22/Effective Approaches to Attention-based Neural Machine Translation/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/22/Effective%20Approaches%20to%20Attention-based%20Neural%20Machine%20Translation/","excerpt":"tags: study paper DSMI labpaper: Effective Approaches to Attention-based Neural Machine Translation Introduction Neural Machine Translation (NMT) requires minimal domain knowledge and is conceptually simple NMT generalizes very well to very long word sequences =&gt; don’t need to store phrase tables The concept of “attention”: learn alignments between different modalities image caption generation task: visual features of a picture v.s. text description speech recognition task: speech frames v.s. text Proposed method: novel types of attention- based models global approach local approach","text":"tags: study paper DSMI labpaper: Effective Approaches to Attention-based Neural Machine Translation Introduction Neural Machine Translation (NMT) requires minimal domain knowledge and is conceptually simple NMT generalizes very well to very long word sequences =&gt; don’t need to store phrase tables The concept of “attention”: learn alignments between different modalities image caption generation task: visual features of a picture v.s. text description speech recognition task: speech frames v.s. text Proposed method: novel types of attention- based models global approach local approach Neural Machine Translation Goal: translate the source sentence $x_1, x_2,…,x_n$ to the target sentence $y_1, y_2,…,y_m$ A basic form of NMT consists of two components: Encoder: compute the representation $s$ for each sentence Decoder: generates one target word at a time$p(y_j|y_{&lt;j},s)=softmax(g(h_j))$, where $g$ is a transformation function that outputs a vocabulary-sized vector, $h_j$ is the RNN hidden unit. Traning objective: $J_t=\\sum_{(x,y)\\in D}-log(p(y|x))$, $D$ is the parallel training corpus.Attention-based model Global AttentionDifference compared with Bahdanau: Bahdanau uses bidirectional encdoer Bahdanau uses deep-output and max-out layer Bahdanau uses a different alignment funciton (but ours are better): $e_{ij}=v^T tanh(W_ah_i+U_a\\hat{h_j})$ Local Attention Global attention is computational costy when the source sentence is long. Input-feeding approach make the model fully aware of previous alignment choices create a very deep network spanning both horizontally and vertically. Experiments Training Data: WMT’14 4.5M sentence pairs. 116M English words, 110M German words vocabularies: top 50K modst frequent words for both languages. Model: stacking LSTM models with 4 layers each layers with 1000 cells 1000 dimensional embeddings Results: English-German reuslts German-English results Analysis Sample Translations baseline model的問題: 人名翻錯 雙重否定翻錯 Reference github in pytorch: https://github.com/AotY/Pytorch-NMT slides: https://slideplayer.com/slide/7710523/","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Darcy"},{"title":"BLEU - a Method for Automatic Evaluation of Machine Translation","slug":"BLEU -- a Method for Automatic Evaluation of Machine Translation","date":"2020-04-16T17:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/16/BLEU -- a Method for Automatic Evaluation of Machine Translation/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/16/BLEU%20--%20a%20Method%20for%20Automatic%20Evaluation%20of%20Machine%20Translation/","excerpt":"這篇文章主要是提供一個對於NLP翻譯後的預測結果的一個快速評估方法。","text":"這篇文章主要是提供一個對於NLP翻譯後的預測結果的一個快速評估方法。 paper: BLEU: a Method for Automatic Evaluation of Machine Translation Abstract由於人工評估的方法通常是昂貴而且費時。我們希望有一個能夠快速替代”人工評估”翻譯好壞的一個方法BLEU := bilingual evaluation understudy Related work n-gram modified unigram precision =$p_n$ 5 systems (2個人工翻譯H1, H2, 3個電腦翻譯S1~S3) Main issue簡單來說 : 只要與人工翻譯的程度越相似則越好實際上就是我們去判斷兩個句子的相似程度直接將標準人工翻譯與我的機器翻譯的結果作比，如果相似，那就是成功主要希望能夠與機器翻譯越相似越好，故需要具備兩個條件: 翻譯相似度矩陣 高質量的人工翻譯資料庫Main idea只觀看1-gram，會有以下例子，也就是常用詞干擾，讓1-gram機率為1，故需要使用modified unigram precision，去避免常用詞導致相似度太高。當然modified一樣有問題，句子的長度一樣會影響 Main model$BLEU$$w_n$ : 加權$p_n$ : n-gram 精度$c$ : 翻譯長度$r$ : 語料庫長度Table 1 : 500 sentencesTable 2 : 20 blocks of 25 sentencesT-value with 95% significant BLEU vs The Human Evaluation2組人工判決，他們將每個翻譯的評分從1（非常差）到5（非常好） monolingual group consisted of 10 native speakers of English.（原生語言是英文） bilingual group consisted of 10 native speakers of Chinese who had lived in the United States for the past several years.（原生語言中文但在美國住很久的人）Figure5 的線性回歸顯示有0.99的高相關係數，很好的追蹤了人類的判斷。Figure6 的回歸結果也有相關係數為0.96。 high correlation between the BLEU score and the monolingual group small difference between S2 and S3 and the larger difference between S3 and H1.(機器翻譯和人類翻譯的差距) bilingual group was very forgiving in judging H1 relative to H2 ReferenceBLEU文章 重點網路介紹，但裡面程式執行有點怪怪的，但基本觀念是對的","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"yiwei"},{"title":"Leetcode - Two Sum","slug":"Leetcode - Two Sum","date":"2020-04-14T00:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/14/Leetcode - Two Sum/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/14/Leetcode%20-%20Two%20Sum/","excerpt":"在面試時，如果順利解完白板題了，面試官可能會接著問: “這樣的時間複雜度是多少?” “還能更快嗎?” 所以，我們試著從簡單的問題來學習如何分析時間複雜度xD Time complexity試圖用個不嚴謹的方法來理解他，想像有一個計數變數count = 0在你的程式裡 每執行一次你的程式count就會+1，請問執行完後count會等於多少?","text":"在面試時，如果順利解完白板題了，面試官可能會接著問: “這樣的時間複雜度是多少?” “還能更快嗎?” 所以，我們試著從簡單的問題來學習如何分析時間複雜度xD Time complexity試圖用個不嚴謹的方法來理解他，想像有一個計數變數count = 0在你的程式裡 每執行一次你的程式count就會+1，請問執行完後count會等於多少? 這!就是時間複雜度 接下來考慮幾個Case: variable assign: $O(1)$ 12x = 0count += 1 loop: O(N) 12for _ in range(N): count += 1 nested loop(2 level): $O(N^2)$ 123for _ in range(N): for _ in range(N): count += 1 nested loop(2 level) + loop: $O(N) + O(N^2) \\rightarrow O(N^2)$ upper bound12345for _ in range(N): for _ in range(N): count += 1for _ in range(N): count += 1 常見的還有sort通常時間複雜度為O(NlogN)、tree-based recursive的時間複雜度為O(2^N)…不過大概有個概念就好 接下來進到本週Leetcode，來探討不同解法的時間複雜度，以及他們對於效能上的差異 LeetCode 1. Two SumSoultion 1. double loop Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. 這題要返回index，所以不應該對list做任何變動 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): for j in range(i+1, len(nums)): if nums[i]+nums[j] == target: return (i, j) 時間複雜度: $O(N^2)$ 12Runtime: 6196 ms, faster than 5.34% of Python3 online submissions for Two Sum.Memory Usage: 14.9 MB, less than 12.55% of Python3 online submissions for Two Sum. Solution 2. Dict(hash table)先把所有的值跟index都記錄下來，然後掃一次list，如果紀錄中存在target - nums[i]，則這就是一組解 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: keys = &#123;&#125; for i in range(len(nums)): if target - nums[i] in keys: return (keys[target - nums[i]], i) if nums[i] not in keys: keys[nums[i]] = i 時間複雜度: $O(N)$ 12Runtime: 44 ms, faster than 91.83% of Python3 online submissions for Two Sum.Memory Usage: 15.1 MB, less than 5.34% of Python3 online submissions for Two Sum. python的dictionary: hash table?這裡用到了一個資料結構: dictionary，或者，在其他語言裡面稱作hash table查詢最快可以是$O(1)$ 怎麼做到的? 空間換時間 透過一個hash function把data轉換成index，然後存到array[index]上 如果array[index]上已經有資料? 在該位置上串linked list 之後在linked list上搜尋就需要遍歷一次list 好的hash function很重要，考慮以下hash function: F(data)=0，也就是任意的資料都給予0的index，此時hash table查找的時間複雜度就會變O(N) 因為等同於在一個linked list上查找 結論: python dict()背後用到的資料結構是hash table，查找時間複雜度通常是$O(1)$ 類似題型15. 3Sum Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. 這題不是要回傳index，而是要回傳可以湊到0的數字集合 18. 4Sum Given an array nums of n integers and an integer target, are there elements a, b, c, and d in nums such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. 這題不是要回傳index，而是要回傳可以湊到target的數字集合 總結針對不同情境、應用使用不同的資料結構、演算法能夠大大提升效能 簡單的針對程式來分析時間複雜度是個還不錯的好習慣 不過Python提供太多lib了，有時候有些東西不好分析，就大概有個概念就好 Reference 初學者學演算法｜從時間複雜度認識常見演算法 白話的 Hash Table 簡介","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"John"},{"title":"Introduction of time series models","slug":"Introduction of time series models","date":"2020-04-09T09:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/09/Introduction of time series models/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/09/Introduction%20of%20time%20series%20models/","excerpt":"tags: references本篇文章介紹常用的時間序列model和相關概念，希望以後大家有遇到時序分析的問題的時候，稍微知道他們在幹嘛XD Notations:$r_t$: a time series, $a_t$: a white noise series, $\\rho_l$: lag-$l$ autocorrelation,$\\gamma_l:Cov(r_t,r_{t-l})$,$\\sigma^2$: variance of $a_t$, $B$: back-shift operator 名詞解釋: Stationary: Strict: distribution is time-invariant (基本上不可能達到) Weak: first 2 moments are time-invariant (平均、標準差、共變數不隨著時間變)","text":"tags: references本篇文章介紹常用的時間序列model和相關概念，希望以後大家有遇到時序分析的問題的時候，稍微知道他們在幹嘛XD Notations:$r_t$: a time series, $a_t$: a white noise series, $\\rho_l$: lag-$l$ autocorrelation,$\\gamma_l:Cov(r_t,r_{t-l})$,$\\sigma^2$: variance of $a_t$, $B$: back-shift operator 名詞解釋: Stationary: Strict: distribution is time-invariant (基本上不可能達到) Weak: first 2 moments are time-invariant (平均、標準差、共變數不隨著時間變) Trend Stationary: $p_t=\\beta_0+\\beta_1t+r_t$,$r_t$ is a stationary time series with mean=$\\mu_r$ $E(p_t)=\\beta_0+\\beta_1t+\\mu_r\\Rightarrow$ time dependent $Var(p_t)=Var(r_t)$ 把$\\beta_0+\\beta_1t$拿掉就變平穩啦~ ACF (Autocorrelation function) $\\rho_l=\\frac{Cov(r_t, r_{t-l})}{Var(r_t)}=\\frac{\\gamma_l}{\\gamma_0}$ Sample autocorrelation function: $\\hat\\rho_l=\\frac{\\sum_{t=1}^{T-l}(r_t-\\bar{r})(r_{t-l}-\\bar{r})}{\\sum_{t=1}^{T}(r_t-\\bar{r})}$ $\\rho_0=1$ 如何檢測autocorrelation 顯著異於0: Ljung–Box test 圖中的直線如果在兩條橫線的範圍之內，表示$\\rho_l$不顯著異於0，反之則是顯著異於0 White noise: Definition: {$a_t$} is a sequence of iid random variables ACFs are 0 Random walk: $p_t=p_{t-1}+a_t$ 他是一個AR(1)，但$\\phi_1=1$ 隨著時間，$a_t$會逐漸累積，如果哪天遇到一個很大的$a_t$ 就回不去了。 如何檢定序列是不是random walk: Unit root test(Augmented Dickey-Fuller test) Autoregressive modelAssume $r_t$ is stationary(這很重要!!): $E(r_t)=\\mu$, $Var(r_t)=\\gamma_0$ AR(1): $r_t=\\phi_0+\\phi_1r_{t-1}+a_t$ by stationary assumption, $E(r_t)=\\phi_0+\\phi_1\\mu=\\mu$, $\\mu=\\frac{\\phi_0}{1-\\phi_1}\\Rightarrow$, $\\phi_1\\neq1$ $Var(r_t)=\\phi_1^2Var(r_{t-1})+\\sigma^2$, since $Var(r_t)=Var(r_{t-1})=\\gamma_0$, $(1-\\phi_1^2)\\gamma_0=\\sigma^2, \\gamma_0=\\frac{\\sigma^2}{1-\\phi_1^2} \\Rightarrow$$\\phi_1^2\\leq1$ $r_t-\\mu=\\phi_0+\\phi_1r_{t-1}+a_t-\\mu=(1-\\phi_1)\\mu+\\phi_1r_{t-1}+a_t-\\mu=\\phi_1(r_{t-1}-\\mu)+a_t$$\\Rightarrow r_i-\\mu$ 和 $a_j, i\\neq j$ 是不相關的 ($Cov(r_{t-1},a_t)=0$) ACF decays exponentially at rate $\\phi_1$: $\\rho_0=1, \\rho_l=\\phi_1^l$ (有興趣可以自己證明看看) AR(2):$r_t=\\phi_0+\\phi_1r_{t-1}+\\phi_2r_{r-2}+a_t$ $E(r_t)=\\phi_0+\\phi_1\\mu+\\phi_2\\mu=\\mu$, $\\mu=\\frac{\\phi_0}{1-\\phi_1-\\phi_2}\\Rightarrow$, $\\phi_1+\\phi_2\\neq1$ $r_t-\\mu=\\phi_0+\\phi_1r_{t-1}+\\phi_2r_{r-2}+a_t-\\mu=(1-\\phi_1-\\phi_2)\\mu+\\phi_1r_{t-1}+\\phi_2r_{t-2}+a_t-\\mu$$=\\phi_1(r_{t-1}-\\mu)+\\phi_2(r_{t-2}-\\mu)+a_t$ ACF: $\\rho_l=\\left{\\begin{aligned}\\frac{\\phi_1}{1-\\phi_2} &amp; &amp; l=1\\\\phi_1\\rho_{l-1}+\\phi_2\\rho_{l-1} &amp; &amp; l\\geq 2 \\\\end{aligned}\\right.$ ACF satifies the second-order difference equation: $(1-\\phi_1B-\\phi_2B^2)\\rho_l=0$, $B\\rho_l=\\rho_{l-1}$ characteristic equation: $(1-\\phi_1x-\\phi_2x^2)=0$ 的根$\\frac{1}{w_1}, \\frac{1}{w_2}$會影響ACF的長相。而基於穩太假設，$w_1, w_2$的長度不能超過單位圓。 AR( p ):$r_t=\\phi_0+\\phi_1r_{t-1}+\\phi_2r_{r-2}+…+\\phi_pr_{t-p}+a_t$ $E(r_t)=\\phi_0+(\\phi_1+\\phi_2+…+\\phi_p)\\mu=\\mu$, $\\mu=\\frac{\\phi_0}{1-\\phi_1-\\phi_2-…-\\phi_p}$$\\Rightarrow(\\phi_1+\\phi_2+…+\\phi_p)\\neq1$ characteristic equaiton: $(1-\\phi_1x-\\phi_2x^2-…-\\phi_px^p)=0$ If all the roots of the characteristic equation $\\geq1$, $r_t$ is stationary Moving average model AR 有點不太實際，因為當order 很高的時候，要估的參數就會很多$\\Rightarrow$使用一個參數來簡化model: $r_t=\\phi_0-\\theta_1r_{t-1}-\\theta_1^2r_{t-2}-…+a_t$ $|\\theta_1|&lt;1$ 不然整個series會爆掉，不符合statioary $\\theta_1^i\\rightarrow0$ as $i \\rightarrow\\infty$, 當$i$很大的時候 $r_{t-i}$的貢獻就很小 MA(1): $r_t+\\theta_1r_{t-1}+\\theta_1^2r_{t-2}+…=\\phi_0+a_t$ $-)$ $\\theta_1r_{t-1}+\\theta_1^2r_{t-2}+…=\\theta_1\\phi_0+\\theta_1a_{t-1}$ $\\Rightarrow r_t=\\phi_0(1-\\theta_1)+a_t-\\theta_1a_{t-1}$, let $c_0=\\phi_0(1-\\theta_1)$ $E(r_t)=c_0$ $Var(r_t)=(1+\\theta_1^2)\\sigma^2$ ACF: $\\rho_l=\\left{\\begin{aligned}1 &amp; &amp; l=0\\\\frac{-\\theta_1}{1+\\theta_1^2}&amp; &amp; l=1 \\0 &amp; &amp; l&gt;1\\\\end{aligned}\\right.$ MA models are always weakly stationary because they are finite linear combination of a white noise sequence. MA(2): $r_t=c_0+a_t-\\theta_1a_{t-1}-\\theta_2a_{t-2}$ $E(r_t)=c_0$ $Var(r_t)=(1+\\theta_1^2+\\theta_2^2)\\sigma^2$ ACF: $\\rho_l=\\left{\\begin{aligned}1 &amp; &amp; l=0\\\\frac{-\\theta_1+\\theta_1\\theta_2}{1-\\theta_1^2-\\theta_2^2}&amp; &amp; l=1 \\\\frac{-\\theta_2}{1-\\theta_1^2-\\theta_2^2} &amp; &amp; l=2\\0 &amp; &amp; l&gt;2\\\\end{aligned}\\right.$ MA(q): $r_t=c_0+a_t-\\theta_1a_{t-1}-\\theta_2a_{t-2}-…-\\theta_qa_{t-q}$ $E(r_t)=c_0$ $Var(r_t)=(1+\\theta_1^2+\\theta_2^2+…+\\theta_qa_{t-q})\\sigma^2$ ACF: $\\rho_l=0$ for $l&gt;q$, lag超過$q$之後ACF的圖就會斷掉(series只和有限個lag有關$\\Rightarrow$finite memory) Autoregressive moving-average model ARMA(1,1): $r_t-\\phi_1r_{t-1}=\\phi_0+a_t-\\theta_1a_{t-1}$ $\\theta_1\\neq\\phi_1$ 不然會退化回AR $E(r_t)-\\phi_1E(r_{t-1})=\\phi_0$, $(1-\\phi_1)\\mu=\\phi_0\\Rightarrow$$\\mu=\\frac{\\phi_0}{1-\\phi_1}\\Rightarrow$, $\\phi_1\\neq1$ (跟AR一樣欸) $Var(r_t)=\\frac{1-2\\phi_1\\theta_1+\\theta_1^2}{1-\\phi_1^2}\\sigma^2\\Rightarrow|\\phi_1|&lt;1$ ACF: $\\rho_l=\\left{\\begin{aligned}1 &amp; &amp; l=0\\\\phi_1-\\frac{\\theta_1\\sigma^2}{Var(r_t)}&amp; &amp; l=1 \\\\phi_1\\rho_{l-1} &amp; &amp; l&gt;1\\\\end{aligned}\\right.$ Exponential decay starts with lag2 ARMA(P,Q): $r_t=\\phi_0+\\sum_{i=1}^{p}\\phi_ir_{t-i}-\\sum^{q}{i=1}\\theta_ia{t-i}+a_t$ 上式可寫成$(1-\\phi_1B-…-\\phi_pB^p)r_t=\\phi_0+(1-\\theta_1B-…-\\theta_pB^p)a_t$ $(\\theta_1,…,\\theta_q)$ 和 $(\\phi_1,…,\\phi_p)$ 完全不能一樣，不然order 會下降 characteristic equation: $(1-\\theta_1x-…-\\theta_px^p)$ 如果這個方程式的解全大於0 $r_t$ 則符合若平穩 $E(r_t)=\\mu=\\frac{\\phi_0}{1-\\phi_1-…-\\phi_p}$ Autoregressive integrated moving average差分之後是ARMA就是ARIMA了!! ARIMA(0,1,0): random walk $r_t-r_{t-1}=a_t$ ARIMA(P,1,Q): $c_t=r_t-r_{t-1}$, $c_t$是ARMA(P,Q) 則 $r_t$就是ARIMA(P,1,Q) ARIMA(P,d,Q): $r_t$差分d次之後是ARMA(P,Q) Seasonal differencing如果時間序列看起來有週期性，可以做不是lag 1的差分","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Time series","slug":"Time-series","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Time-series/"}],"author":"Darcy"},{"title":"Leetcode - 687. Longest Univalue Path","slug":"Leetcode - 687. Longest Univalue Path","date":"2020-04-08T18:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/08/Leetcode - 687. Longest Univalue Path/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/08/Leetcode%20-%20687.%20Longest%20Univalue%20Path/","excerpt":"tags: leetcode687. Longest Univalue Pathhttps://leetcode.com/problems/longest-univalue-path/","text":"tags: leetcode687. Longest Univalue Pathhttps://leetcode.com/problems/longest-univalue-path/ 題目: Given a binary tree, find the length of the longest path where each node in the path has the same value. This path may or may not pass through the root.The length of path between two nodes is represented by the number of edges between them. example 3: 1 / \\ 1 1 / \\ \\ 1 1 1 /\\ \\ 1 2 1example 4: 1 / \\ 1 1 / \\ \\ 1 1 1 [1 1 1 1 1 1] 左邊的路徑、右邊的路徑 路徑怎麼決定？如果能接過來（右邊的值＝當前的值）： 右邊的路徑＋1如果不能：0 Remark http://www.csie.ntnu.edu.tw/~u91029/BinaryTree.html 12345678910111213141516171819202122232425262728293031323334353637# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val &#x3D; x# self.left &#x3D; None# self.right &#x3D; Noneclass Solution(object): def longestUnivaluePath(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; self.ans&#x3D;0 def maxlength(node): if not node: return 0 else: left_node&#x3D;node.left right_node&#x3D;node.right left&#x3D;maxlength(left_node) right&#x3D;maxlength(right_node) if left_node and node.val&#x3D;&#x3D;left_node.val: mlength_left&#x3D;left+1 else: mlength_left&#x3D;0 if right_node and node.val&#x3D;&#x3D;right_node.val: mlength_right&#x3D;right+1 else: mlength_right&#x3D;0 self.ans&#x3D;max(self.ans,mlength_right+mlength_left) return(max(mlength_right,mlength_left)) maxlength(root) return self.ans","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Puchi"},{"title":"GloVe -- Global Vectors for Word Representation","slug":"GloVe Global Vectors for Word Representation","date":"2020-04-08T13:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/08/GloVe Global Vectors for Word Representation/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/08/GloVe%20Global%20Vectors%20for%20Word%20Representation/","excerpt":"本文章簡述了GloVe論文中較為重要的概念，部分細節並無詳細闡述，若有疑問，請參考原始論文。 Related work LSA (Latent Semantic Analysis) HAL (Hyperspace Analogue to Language) word2vec with CBOW word2vec with skip-gram","text":"本文章簡述了GloVe論文中較為重要的概念，部分細節並無詳細闡述，若有疑問，請參考原始論文。 Related work LSA (Latent Semantic Analysis) HAL (Hyperspace Analogue to Language) word2vec with CBOW word2vec with skip-gram Main issue過去的word representation的發法有些問題： LSA, HAL : 詞頻高的詞彙影響這些模型太嚴重，導致不重要的詞彙，例如”a, the, …”，讓模型學到一些不必要(錯誤)的詞彙關係。[Matrix Factorization Methods] word2vec 僅考慮一定window內，附近詞彙之間的關係。當兩詞彙有關連但相距較遠，word2vec將難以學會這些詞彙之間的關聯。[Shallow Window-Based Methods] 本論文希望能同時改善上述問題。 Main idea1Glove希望能找到更多詞彙之間的有用關聯 Notations: $X_{ij}= word,j 出現在word,i上下文的次數$ $X_{i}= \\sum_{k}{X_{ik}}$ $P_{ij}=P(j|i)=\\dfrac{X_{ij}}{X_i}$，此公式可對應到Main idea的表格 找出文字之間關聯 由上圖可以發現，solid出現在ice附近的機率遠大於steam，而gas出現在steam附近的機率大於ice。而water(對於兩詞彙皆有關聯)及fashion(對於兩詞彙皆無關聯)對於ice及steam兩者間的比例則接近於1。 對於一字多義：則將同一字彙標註為多個，Ex: bank1, bank2, ...。 Main modelProcess for finding cost function:首先，Glove希望能凸顯Main idea所體現的文字關係，換句話說，對於任意三個不同的字，希望能找到function F，以計算他們之間的關係：換句話說，也可以想成是，將兩文字之間相差多大，以及第三者當作輸入(如果想不通，建議可以想想main idea的example)：為求實際運算上方便，將公式轉換為向量間的乘法： - Eqn. (3)假設F是homomorphism，則可得到並推得以下：將$log(X_{i})$以b替換掉，$\\hat b_k$ 是額外加入的bias for $\\hat w_k$：最後乘上weight $f(X_{ij})$並當作cost function j:其中$f(X_{ij})$可思考一下，他必定不會&gt;1；但當有設置$x_{max}$時，仍需給予條件設置，另外也給予一個$\\alpha$值作為模型調整：以下為$X_{ij}$與$f(X_{ij})$之間的關係圖： Related to other Models就結論而言，會推導出與Main model中eq(8)一樣的cost function，詳細過程請參考GloVe原文。 complexity總之他說他比別人快的樣子(看網路是這樣說)，好像是最差的結果是$O(|V|^2)$, 但可以證明其速度可以到$O(|C|^{0.8})$, window-base是$O(|C|)$。詳細細節請參考GloVe原文。 Experiments主要分為兩個部分，測量詞彙向量本身做答題作為測量(內部測量)，以及將文字向量拿去訓練其他問題(外部測量) Word similarity semantic questions (回答相對地名或人名) Athens is to Greece as Berlin is to _____ ? syntactic questions (找出對應詞彙型態，包含動詞及形容詞) dance is to dancing as fly is to _____ ? bad is to worst as good is to _____ ? Named entity recognition (NER)Dataset : CoNLL-03 Data : Reuters newswire articles with four entity types: person(人), location(地), organization(機構), and miscellaneous(其他). Goal : 標記這些entity Result: More results:他想說越多資料，模型就會越準。就我看來，這應該是拿來湊頁數用的。 如何使用Glove這裡有篇使用教學文，歡迎大家使用看看。 Reference [paper] https://nlp.stanford.edu/pubs/glove.pdf [source code] https://nlp.stanford.edu/projects/glove/","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Jeff"},{"title":"Efficient Estimation of Word Representations in Vector Space","slug":"Efficient Estimation of Word Representations in Vector Space","date":"2020-04-08T03:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/04/08/Efficient Estimation of Word Representations in Vector Space/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/04/08/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space/","excerpt":"論文網址: Efficient Estimation of Word Representations in Vector Space 4 model architectures:NNLM; RNNLM; CBOW; Skip-Gram為了比較模型好壞，先定義接下來訓練深度模型的複雜度皆為:$$O = E\\times T\\times Q$$ E: 迭代次數 T: 訓練集的詞個數 Q: 模型參數 old: NNLM, RNNLM What’s NNLM (Feedforward Neural Net Model)?(上圖是原始paper(A Neural Probabilistic Language Model)的圖，annotation可能會跟下面word2vec的對不起來，下面使用原本word2vec的annotation) 有4層: input, projection, hidden, output","text":"論文網址: Efficient Estimation of Word Representations in Vector Space 4 model architectures:NNLM; RNNLM; CBOW; Skip-Gram為了比較模型好壞，先定義接下來訓練深度模型的複雜度皆為:$$O = E\\times T\\times Q$$ E: 迭代次數 T: 訓練集的詞個數 Q: 模型參數 old: NNLM, RNNLM What’s NNLM (Feedforward Neural Net Model)?(上圖是原始paper(A Neural Probabilistic Language Model)的圖，annotation可能會跟下面word2vec的對不起來，下面使用原本word2vec的annotation) 有4層: input, projection, hidden, output input layer: 前N words使用one-hot encoding成V維的向量，V是(vocab size) 注意這裡是用前N words，而不是用所有words來訓練，這是和word2vec中CBOW投影層的差異!! projection layer: input(NxV)會使用同一個projection matrix(VxD)投影到projection layer P(NxD) D是投影後的維度 共用一個projection matrix，所以這裡的cost還算低 hidden layer: 隱藏層來計算整個word的機率，有H個neuron output layer有V個neuron 所以整體的模型參數量是$$Q=N×D+N×D×H+H×V$$ 其中output layer的HxV最重要 有一些優化的方法，例如hierarchical softmax，使用binary tree representations of the vocabulary(Huffman tree)，可以降到$\\log_2(V)$ 所以其實主要的計算量在hidden layer What’s RNNLM (Recurrent Neural Net Language Model)? Remark: $y(t)$ produces a probability distribution over words 只有input, hidden, output層，訓練複雜度是$$Q=D×H+H×V$$ D和隱藏層H有相同的維度 使用hierarchical softmax + huffman tree，H×V可以降低為H×$\\log_2V$，所以大部分的複雜度來自D×H new: CBOW (Continuous Bag-of-Words Model), Skip-Gram (Continuous Skip-Gram model) overview (from cs224n) (圖片來自[7]） What’s CBOW? cs224n note:http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdfloss function: 和NNLM相似，但刪除了hidden層，並且投影層是所有的words共用(NNLM是前N words共用) 所有的单词都投影到同一个位置（所有向量取平均值） 这样不考虑单词的位置顺序信息，叫做词袋模型 詞的順序對於不影響投影 会用到将来的词，例如如果窗口 windows 为 2，这样训练中心词的词向量时，会选取中心词附近的 4 个上下文词（前面 2 个后面 2 个） 整體的模型參數量為:$$Q=N×D+D×log(V)$$ log(V)是用到了hierarchical softmax + huffman tree What’s Skip-Gram? cs224n note:http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf loss function: 跟CBOW相似，不過是根據中心的詞去預測上下文 通過實驗發現，windows越大效果越好(但cost也越大) 距離較遠的word通常關聯性較小，所以透過抽取較少的樣本(降低機率)來降低對距離較遠word的權重 整體複雜度:$$Q=C×(D+D×log(V))$$ C是window size的2倍，也就是要預測的word個數 也就是說預測每個word所需的參數量是D+D×log(V) 看到logV就知道用到了hierarchical softmax Experiment Result1. Experiment task 1:–主要： There are totally 8869 semantic questions and 10675 syntatic questions.(They create the correct word pairs manually, and then randomly connecting two words to form the word pair question.) –在table 3中多了一個MSR task: SemEval-2012: Semantic Evaluation Exercisese.g. Singular/Plural: year:years law:?[4] Evaluation: Accuracy (預測的字，必須剛好是正確的word pair，才算對！預測的僅是相似字也不算對) Training data: 不同實驗用不同training data,主要為Google News corpus (6B tokens)。 Testing data:5 types of semantic questions &amp; 9 types of syntatic questions Experiment result &amp; conclusion: 雖然paper跑的實驗很多，但主要結論是CBOW＆skip-gram準確率高且花的時間短（需要運算量較低）。另外，增加training data也須增加維度 We have to increase both vector dimensionality and the amount of the training data together. (使用1 CPU去train,CBOW花一天;skip gram花約三天) 從表格可看出NNLM花的時間明顯多於其他兩個模型。（使用mini-batch asynchronous grandient decent and Adagrad (a adaptive learning rate procedure) ） MSR: SemEval-2012: Semantic Evaluation Exercisese.g. Singular/Plural: year:years law:?[4]MSR的training set: LDC corpora（320M words, 82K vocabulary). 2. Experiment task 2: Microsoft Sentence Completion ChallengeThe Microsoft Research Sentence Completion Challenge Training data: many novelshttps://www.kaggle.com/c/mlsd-hw3/data Testing data: https://www.kaggle.com/c/mlsd-hw3/data – Skip-gram model作法： – Skip-gram + RNNLMs: weighted combination Result 補充 DistBeliefhttps://www.iotone.com/term/distbelief/t187 dataset –LDC corpora–SemEval-2012: Semantic Evaluation Exercises–Microsoft Sentence Completion Challenge (延伸討論)Word2vec implement detailWord2vec hidden layer沒有activation function在input-hidden層，沒有非線性變換，而是簡單地把所有vector加總並取平均，以減少計算複雜度 Hierarchical Softmax v.s. Negative sampling論文：Distributed Representations of Words and Phrasesand their Compositionality實際上，input layer有CBOW和Skip-gram兩種版本，output也有Hierarchical Softmax和Negative sampling兩種版本 Hierarchical Softmax以下引用[6] 一般正常input-hidden-output的model如果套用在embedding training，因為output層的softmax計算量很大(要去算所有詞的softmax機率，再去找機率最大值) 透過huffman tree + Hierarchical Softmax，tree的根節點是每一個word，透過一步步走到leaf來求得softmax的值 從root到leaf只需要log(V)步 如此可以大幅的加快求得softmax的速度 缺點: 但是如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了 於是有了Negative Sampling Negative sampling定義window內的為正樣本，window外的為負樣本，如此就可以不用把全部的word拿進來一起train 只需要window內的所有正樣本 + sampling一定數量的負樣本就足夠訓練模型 透過Unigram distribution來模擬負樣本(不在window內的word)被選中的機率 設計這個分佈時希望詞被抽到的機率要跟這個詞出現的頻率有關，出現在文本中的頻率越高越高越有可能被抽到 公式為:$$P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right ) }$$$f(w_i)$代表$w_i$出現次數(頻率)，3/4是實驗try出來的數據 例如：有一個詞編號是 100，它出現在整個文本中 1000 次，所以 100 在 unigram table 就會出現 1000 ^ 0.75 = 177 次 至於要選幾個詞當 negative sample，paper 中建議如下 Our experiments indicate that values of k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as small as 2–5. Subsampling of frequent words英文中 “the”, “a”, “in”，中文中的「的」、「是」等等這種詞，其實在句子中並沒有辦法提供太多資訊但又常常出現，對訓練沒有太大幫助，所以就用一個機率來決定這個詞是否要被丟掉，公式如下$$P(f_i) = (\\sqrt{\\frac{f(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{f(w_i)}$$ idea table 2(3 training epochs; stochastic gradient decent and backpropogation; learning rate=0.025 and decreased it linearly)Reference https://myndbook.com/view/4900 https://arxiv.org/pdf/1411.2738.pdf http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf https://www.aclweb.org/anthology/N13-1090.pdf Word2vec Tutorial word2vec原理(二) 基于Hierarchical Softmax的模型 引起你對 Word2Vec 基本概念","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"}],"author":"John, Puchi"},{"title":"Basic Overview of Natural Language Processing","slug":"Basic Overview of Natural Language Processing","date":"2020-03-31T14:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/31/Basic Overview of Natural Language Processing/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/31/Basic%20Overview%20of%20Natural%20Language%20Processing/","excerpt":"Basic Overview of Natural Language Processing本篇文章粗略介紹了從古(?)至今(2020)NLP重要的發展史及技術，希望能讓大家對NLP有一些基礎的概念與了解🧐。","text":"Basic Overview of Natural Language Processing本篇文章粗略介紹了從古(?)至今(2020)NLP重要的發展史及技術，希望能讓大家對NLP有一些基礎的概念與了解🧐。 遠古時期 (?~19xx)ruled based - 利用建立大量規則回答問題 傳統機器學習 (19xx~2013)機率模型 – 標註任務Given sequence Y(sentence), we want to find sequence X(tags) CRF (Conditional Random Field) HMM (Hidden Markov Model) MEMM (Maximum-Entropy Markov Model) 運用情境： 標註詞性 標註主要內容 n-gram 模型 (2001)利用前n個字(詞)，去觀察(統計)下一個字(詞)出現的機率。 運用情境： 下一個字(詞)預測 斷詞 大NN時代 (2013~)總而言之，言而總之，我們想要把文字放進NN，要先想個辦法把文字用向量表示才行🤔。 one-hot encoding最直接的方式就是一個字給他一個代碼~! 當你拿到的文本是百科全書🙃： word2vec (號稱NLP界的哥爾羅傑，開啟了大NLP時代)好用程度真的沒話說😏： word2vec兩種不同的訓練方式： 重要突破點： 能將文字轉為任何長度的向量 運用情境： 需要將文字轉為向量時 你拿到的資料嚴重不足，導致DL訓練效果極差時 你的時間不夠用，需要速成且堪用的模型時 RNN, LSTM相信大家經過交大課程的歷練，已經很熟悉了。放幾張圖留個紀念🕶 重要突破點： 我想大家對他們的優點在課堂上都很熟悉了 使用情境： 任何NLP相關的問題也許都可以try try看 Attention is all you need在Deep Learning的世界，了解Attention is all you need為Deep Learning的世界開啟了一片新大陸😎。 下圖為Attention計算的核心概念： Transformer有了Attention的概念之後，實際使用時，特別設計了transformer的架構，如下圖： 重要突破點： 提出了注意力機制，讓模型能忽略不重要的字詞 不僅運用在NLP，CV也有極多運用 重要應用： 把所有DL相關的paper重新刷一遍，發paper就是如此簡單愜意！ BERT (人人都能變巨人 - NLP界的巨人之力)萬分感謝hugging face所公開的模型。 學會BERT讓你一步登天😀 BERT的出現，讓NLP界意識到了pre-training的重要性。對於文字的任務，大多時候要能有好的模型，是需要很多先備知識的。而pre-training正是賦予模型基礎知識的技巧，讓往後的每個目標任務都無須從剛出生的嬰兒開始學起。 利用bertviz可以視覺化模型學習到的注意力機制結果。 重要突破點： 同一組參數，多種任務訓練 預訓練模型的概念，並將訓練好的模型公開給大家使用 使用情境： QA classifier 立場分析 閱讀測驗 把所有NLP相關的paper重新刷一遍，發paper就是如此簡單愜意！ GPT-2BERT馬上就失戀了😕 當然，效果更好的代價就是…參數更多 GPT-2訓練時的主要目標為預測下一個字。並且在做Attention時，只會往前看。 下圖能看出BERT和GPT-2的主要差異 不過GPT-2到近期才開放部分訓練好的模型供大家使用，如果你覺得空虛寂寞覺得冷，不妨也可以到Talk to transformer跟模型聊聊天。 重要突破點： 不同於BERT，採用了transformer的Decoder 使用了更大的模型，更多的參數，成功讓大家了解到自己的設備資源竟是如此不足 使用情境： QA classifier 立場分析 閱讀測驗 當你想要打敗BERT的時候(?) 把所有NLP相關的paper重新刷一遍，發paper就是如此簡單愜意！ Reference https://nlp.stanford.edu/~wcmac/papers/20140716-UNLU.pdf https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html https://zhuanlan.zhihu.com/p/48508221","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"}],"author":"Jeff"},{"title":"Overview of Meta Learning","slug":"Overview of Meta Learning","date":"2020-03-30T16:28:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/30/Overview of Meta Learning/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/30/Overview%20of%20Meta%20Learning/","excerpt":"Meta LearningIntroductionMeta Learning = Learn to learn 讓機器學習如何去做學習這件事。 實際例子：當我們接觸到一個裝糖果的玻璃罐時，我們察覺玻璃罐與保特瓶相似的本質，因而有辦法套用既往的知識快速的移轉到新的任務上，而Meta learning便是在學這個過程，在遍覽多種任務後，學習一組對任務敏感的參數，當新任務進來時能快速的將先驗知識移轉到新任務中。","text":"Meta LearningIntroductionMeta Learning = Learn to learn 讓機器學習如何去做學習這件事。 實際例子：當我們接觸到一個裝糖果的玻璃罐時，我們察覺玻璃罐與保特瓶相似的本質，因而有辦法套用既往的知識快速的移轉到新的任務上，而Meta learning便是在學這個過程，在遍覽多種任務後，學習一組對任務敏感的參數，當新任務進來時能快速的將先驗知識移轉到新任務中。 假如在過去我們過去叫機器學了task1(影像辨識)、task2(語音辨識)等等這些事，那他在學習task100(文字辨識)可以學習的更快。雖然影像辨識、語音辨識跟文字辨識沒什麼太大關係，但機器是學到了怎麼去學習這件事，所以未來有新的任務就可以學得更快。 這跟life long learning挺像的。但很大的差異是，life long learning主要是希望當機器學了那麼多的task，那在新的task上仍然做得很好，希望同個模型能夠學習不同的技能。但在Meta learning中，不同的任務仍有不同的模型，但我們期待機器能夠從過去的學習經驗，學到一些東西使得未來要train一個新的模型可以學得更快更好。 在過去的machine learning，你給他一些training data假設任務是要辨識貓狗的圖片，透過gradient descent、back propagation這些algorithm我們可以得到CNN裡頭的參數並形成 $f^{}$接著要把test的image丟進這個$f^{}$就可以去預測這個image是貓還狗。 那在meta learning上，可以把learning algorithm也想像成一個function $F$，這個$F$吃進去的是訓練資料$D_{train}$ ，吐出來的是另外一個 function $f^{*}$，在例子下就是吐出一個 function $f^{＊}$ 可以拿來把貓狗進行影像辨識。 Meta learning的主要點就在於如何讓機器自動找到這個 Learning Algorithm $F$。 Machine Learning vs. Meta Learning Machine Learning 的目標是$f^{*}$分類器。Meta Learning 的目標則是 $F$ learning algorithm。 How to do Meta Learning ?不同人為設計的Network、initial $\\theta^{0}$、update method 以及 learning rate都是不同的learning algorithm。 Meta Learning主要希望紅色框框的一些人為設計，可以取代為機器自己學。 再來，我們需要一個評量準則來決定Learning Algorithm $F$ 的好壞。 在machine learning，我們需要準備的就是很多training的資料以及testing的資料。在meta learning，我們則是需要準備training的task跟testing的task，每一個task裡有training set跟testing set。測試的task當然要跟訓練的任務不太一樣，有時候我們也會從training set切出一個validation set來調整參數。 meta learning往往都搭配few-shot learning。few-shot learning指的是，假設我們現在要做分類任務的時候，每一種類別只給你很少很少的資料，希望機器可以看很少的例子就可以來分類。因為我要檢查一個learning algorithm好不好的時後，我就必須跑完整個訓練的流程，才知道我的learning algorithm好不好。假設跑完一個訓練流程要一天，那這樣的task根本無法做。所以說，往往在做meta learning的時候，都會假設我們今天是few-shot learning，這些task的訓練資料都很少，比較能快跑出訓練結果。 那在meta learning中，往往會把每個task的training set叫做support set，testing set叫做query set。因為真正的training set指的會是task，所以必須要給它們其他一個名稱。 Meta Learning的流程:最後的結果 $l$ 就是整個Meta Learning這個方法的好壞。 那在Meta Learning中有一個benchmark的dataset，如同在影像辨識的mnist dataset。Omniglot（歐 ㄋ一 嘎）裡頭總共有1623個符號，每一種符號會有20張範例。下面就是一堆不同的符號。 Omniglot 往往被設計成一個 few-shot classification的task。 N-ways K-shot classification: In each training and testing tasks, there are N classes, each has K examples. 20-ways 1-shot : 總共有20個不同符號，每一個類別只有一張圖片，希望機器就可以學出一個20類的分類系統。如果直接用普通方法去學一定爆炸，但就希望meta-learning能幫助我學出一個演算法，只要看過一個example就可以分類的很好。 Split Train &amp; TestSplit your characters into training and testing characters(1623 char. $\\rightarrow$ 1200 train char., 423 test char.) Sample N training characters, sample K examples from each sampled characters $\\rightarrow$ one training task. Sample N testing characters, sample K examples from each sampled characters$\\rightarrow$ one testing task. MAML (Model Agnostic Meta-Learning for Fast Adpation of Deep Networks, 2017)缺點：Network structure要一樣 想法：只focus在每個weight的初始值 $\\phi$ MAML vs. Model Pre-training在做transfer learning時會用Model Pre-training的方法。現在我要做的某個任務data很少，但另外一個相關認為data數量很多，把model pre-train在多量資料的任務上，fine tune在少量的資料的任務上。 在MAML，$l^{n}$是用$\\phi$訓練過後的model $\\hat{\\theta^{n}}$計算出來的。在Model Pre-training，$l^{n}$拿的是現在手上的model去評估他現在在我拿來要做Pre-training的任務，表現怎麼樣。 MAML的主要想法:找到一個初始值$\\phi$，然後利用這個$\\phi$去訓練過後的model $\\hat{\\theta^{n}}$在各自task表現要好。 Model Pre-training的主要想法:現在我們要找到一個$\\phi$，同時讓在每個task表現很好。 MAML : Fine $\\phi$ achieving good performance after training. (model 潛力如何) Model Pre-training : Fine $\\phi$ achieving good performance. (model 現在表現如何) MAML 在實做時的 Assumption:Training Algorithm 只會做一次參數update。也就是說，$\\phi$是機器自到的初始值，計算一次gradient，updata後得到的$\\hat \\theta$就是最後的模型結果。 可能的理由： Fast … Fast …. Good to truly train a model with one step. Few-shot learning has limited data. (updata太多容易overfitting) ＊＊ 但在testing Task 可以update很多次參數 Warning of Math $\\frac{\\partial \\hat \\theta_{j}}{\\partial \\phi_{i}}$ using first-order approximation. 所以 $\\nabla_{\\phi}L(\\phi) = \\nabla_{ \\phi} \\sum_{n=1}^{N} l(\\hat \\theta) = \\sum_{n=1}^{N} \\nabla_{\\phi}l(\\hat \\theta) = \\sum_{n=1}^{N} \\nabla_{\\hat \\theta}l(\\hat \\theta)$ Real Implement:初始化初始值 $\\phi^{0}$，假如我們只做1-batch，現在要train task m時，我們就把$\\theta^{0}$初始值去traim task m model並update一次得到$\\hat \\theta^{m}$。接著再update一次得到update的方向，把它用來當作update 初始值$\\phi^{0}$的方向到$\\phi^{1}$。 那pre-training是長怎樣？ Reptile (2018)不限制只能update一次，可以很多次。 MAMO vs. Pre-training vs. Reptile More issue :","categories":[{"name":"Learning Note","slug":"Learning-Note","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/Learning-Note/"}],"tags":[{"name":"TL","slug":"TL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/TL/"}],"author":"Chris"},{"title":"Introduction of Transfer Learning","slug":"Introduction of Transfer Learning","date":"2020-03-29T12:41:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/29/Introduction of Transfer Learning/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/29/Introduction%20of%20Transfer%20Learning/","excerpt":"Transfer Learning (TL) Similar domain, Different task Different domain, Similar task Q : 能不能在有些不相干的data情況下，來幫助我們現在要做的task.","text":"Transfer Learning (TL) Similar domain, Different task Different domain, Similar task Q : 能不能在有些不相干的data情況下，來幫助我們現在要做的task. Why?ex1. 台語語音辨識（data少），但英文、中文的語音相對容易取的。ex2. 醫療影像辨識（有無腫瘤），實際其他圖片的data網路上一堆。ex3. 分析法律文件，網路上其他文件是否有幫助？ TL Example in real life:漫畫家 -&gt; 責編 -&gt; 畫分鏡 -&gt; 投稿jump研究生 -&gt; 指導教授 -&gt; 跑實驗 -&gt; 投稿期刊 我們只會介紹 Source Data 有 label 的部分。 Labelled Source &amp; Labelled Target DataModel Fine-tuneTask description:target data : $(x^{t}, y^{t}) \\rightarrow$ very littlesource data : $(x^{s}, y^{s}) \\rightarrow$ A large Amount One-shot learning: only a few examples in target domain Example : Speaker Adaptiontarget data: audio data and its transriptions of specific user.source data: audio data and transriptions from many speakers. Main Idea :Training the model by source data, then fine tune the model by target data.**Challenge : Only limited target data, so be careful about overfitting. How to prevent overfitting ?1. Conservarive Learning假設現在有大量的source data假設在語音辨識裡他就是很多不同speaker的聲音都有transciption，就可以拿來train一個語音辨識的neural network。接下來，你有target data，可能只有五句或十句這麼多，如果直接把這些target data去train一個model想必因該是不可行的。這時候在train的時候就可以設定一些限制，讓train完的新model跟舊model不要差太多，那就是新的model跟舊的model看到同一筆data的時候，他們的output越接近越好。或者也可以把constraint放在兩者model他們之間的weight L2 norm越接近越好。 2. Layer Transfer source data只需考慮很少的參數，就可以避免overfitting的情形。 當source data量一多也可以考慮fine-tune整個model. Q : Which layer can be transferred (copy)?Speech : usually copy the last few layers.每一個人，他用同樣的發音方式，因為口腔結構略有差異，得到的聲音不同。前面幾層layer主要在看語者說話的發音方式，再根據發音方式得到現在説的詞彙，達到辨識結果。所以從發音方式到辨識結果後面幾層layer是跟語者沒關係的可以被copy，不一樣在於從聲音訊號到發音方式這一段，可能是每個人都不一樣。 Image: usually copy the first few layers. 因為CNN前面幾層所做的就是detect最簡單的pattern，像是橫線、直線，或者有沒有簡單的幾何圖形。所以在CNN前幾層learn的東西，是可以transfer到其他task上。而最後幾層learn的東西往往比較具體，他就無法transfer到其他task上 Multi-task learning與fine-tune不同，fine-tune在意target domain做得好不好，不管source domain. Multi-task learning是考慮能不能同時做好。 好處在於，taskA 與 taskB 前面幾層是共用的。前面幾層是用比較多data訓練的，所以可能會有比較好的performance。前提：這兩個task必須是有共通性的，是不是可以共用前面幾個layer。 另外，input無法share。兩種不同的input都用不同的神經網路把它transfrom到相同domain上，在apply不同的神經網路，一個做taskA，另一個做taskB。如果你覺得這兩個task中間有共同地方的話。 Example: 一大堆不同語言data。可以訓練一個模型同時辨識五種不同語言。前面幾個layer共用參數，後面幾個layer每一個語言會有自己的參數。雖是不同語言，但都為人類說的，前面幾個layer就可以share相同資訊，可以共用相同的參數 Q: Transfer 是否會造成負面效果？有可能，如果兩個task不相近就會有可能造成這樣的結果。但總是思考能不能transfer, try &amp; error太浪費時間的。 Progressive neural netwok task 2 他的每一個hidden layer都接前面task 1 某一個hidden layer的output。所以他的好處在於，就算task 1與task 2不像也能夠進行training。 首先，task 2的model不會去動到task 1的model，所以task1一定不會變得比較差。再來，task 2去借用task 1的參數，但是可以把他借來的參數直接設成0，這樣也不會影響task 2 的performance。最糟的情況就是跟自己train的performance是差不多的。 所以說如果有5個task不就要接前面4個task嗎？那這篇作者自己也覺得很怪等待別人提出想法～～～～ Labelled Source &amp; Unlabelled Target DataTask description: Source data : $(x^{s}, y^{s}) \\rightarrow$ training dataTarget data : $(x^{t}) \\rightarrow$ testing data Main problem:Training data and testing data are mismatch.如何在Source data上的model直接apply在別的Target data上也可以運行呢？ Domain Adversarial training 不應該是兩個不同domain的feature分成兩群，而是不同domain的feature要被混雜再一起。把不同domain的特性給消除。Domain classifer主要就在辨別feature $f$ 是在哪一個domain裡面。前面feature extractor有點類似GAN generator的output，然後domain classifier就是descriminator的結果，使得整個架構很像GAN。但在這邊，你只是把feature混在一起，根本無法進行數字分類，因此整體的Domain Adversarial training長得像這樣: 藍色的部份希望把class分得越精準越好，粉紅色的部分則是希望能準確預測input x 屬於哪一個domain。那feature extractor要做的事情就是同時imporve label predictor的accuray，同時也要minimize domain classifer的accuracy，希望把類別分類做到最好，同時也要移除掉不同domain的特性。 如何移除domin的特型，在做back propagation的時候，多加一個gradient reversal layer把domain classifier的gradient取負號回傳給前面的feature extractor就以簡單做到了。$\\textbf但重要的問題來了$: traget data根本沒label那要怎麼train label classifier? 根據paper我覺得是，target data不會參與label classifier的training。也就是說在train model時，target data的作用只會在domain classifier 的loss上。（這是我看他sudo code覺得因該是這樣，因為他沒細講ＱＱ） Zero-shot LearningZero-shot learning對於domain的要求更嚴苛一點，他的define是source data 和 target data的task是不一樣的，不像先前是對不同doamin下的數字進行辨識。 example :Source data $(x^{s}, y^{s})$ 都是貓和狗圖片。Target data $(x^{t})$ 是草泥馬的圖片，在Source data中是沒看過的。 語音上很常遇到這樣的問題。假設，把不同的word都當作是一個詞彙一個class，本來在training和testing中就很長有可能看到不同的詞彙。在語音上的做法是，不要直接判斷這一段聲音屬於哪一個word，而是判斷這一段聲音屬於哪一個phoneme(音位，是人類語言中能夠區別意義的最小聲音單位，是音位學分析的基礎概念)，之後在根據人類的知識在做一個phoneme與文字對應關係的table。 在辨識的時候，只要辨識出phoneme就好，再去查表看看這一段phoneme對應到哪一個word。 因此就算有些word在training data沒看過，只要他在你建的lexicon(詞典)有出現過，model就可以辨識聲音屬於哪一個phoneme的話。 那在圖片上，我們可以把每一個class用他的attribute來表示。每一個class最好有獨一無二的attribute。 在training的時候，我們output的是圖片的特徵而不是類別。 所以當有一個從沒在training set出現過的圖片，我們就透過model找出他的特徵，並且查表來找哪一個和output的最接近(不一定會一樣)，那那個動物就是要找的。 有時後，我們產生的attribute可能很複雜，dimension可能很大，這時候我們就能夠做attribute的embedding。我們就把每一個training data的image都透過一個transform變成embedding space上的一個點，然後也把每個image對應的attribute也都變成embedding space上得點。$f$和$g$可以想像成是兩個不同的NN。在training時會希望$f(x^{n})$和$g(y^{n})$越接近越好。如此一來在testing時，當有一個不在training data上的圖片，我們就把這個image的attribute轉變至embedding space看哪個動物在embedding space和他最相近。 但在這邊會牽涉到一個問題，如果我根本沒有database紀錄動物的attribute呢？A: 借用word vector word vector的給個dimension就代表這個word的某種attribute，所以不一定需要一個database跟你說每一個動物的attribute是什麼。假設你知道每個動物他對應的word的word vector(例如説從很大量的corpse wikipedia train出來)，那就可以直接把attribute直接換成word vector，再做剛剛的embedding就ＯＫ了。 那在learn zero-shot的時候需要注意一些事情。如果我們單純只用第一個式子只要求$f(x^{n})$跟$g(y^{n})$越近越好，解出來你就會發現最後的結果是 $\\forall n, f(x^{n})=g(y^{n})$，那顯然這不是我們要的結果。 所以在定loss function時我們因該還要考慮$x^{n}$跟$y^{m}$如果不是同一個pair，那麼他們再embedding space得距離因該要越大越好，於是就有了第二個式子。這裡的$k$代表的是margin，再training時必須事先設定好。那我們來看看後面summation什麼時候會是zero-loss呢，只有當後面那一長串小於0的時候。那我們把後面那一長串小於0做一下整理，我們便可以清楚知道這一長串的含義是什麼: “當$f(x^{n})$與$g(y^{n})$他的inner product都大於$f(x^{n})$與其他任一$g(y^{m}) k$個單位” 因此，如果定這樣一個loss function，不只把同一個pair起來的attribute跟image拉近，同時也要把不成pair的拆開。 其實還有一個更簡單的zero-shot learing方法。這個方法就是說，我們也不需要任何的learning。假設現在有一個現成的Imagenet model跟word vector，把一張圖丟到NN他覺得有0.5的機率是lion，0.5的機率是tiger，就把lion跟tiger的word vector比例用1:1混合，在看哪個word和word vector的混合結果最近。那可以發現最接近的就是liger(獅虎)。 之前我們都是舉圖片的例子，我們再舉一個speech在zero-shot learning的例子。下面這是google做的一個實驗，這是在做machine translation。machine看過如何把英文翻成韓文，也知道怎麼把韓文翻成英文，也知道怎麼把英文翻譯成日文，也知道怎麼把日文翻成英文，有這些data。但machine從沒看過日文翻韓文跟韓文翻日文，但是他卻可以翻。 為什麼zero-shot learning在這樣的task是可行的？因為如果用同一個model做了不同語言之間的translation，他可以學到不同語言的句子project到相同的space上，而在這個space上他們是language independent的，在這space上，只跟句子的semantic有關。 這個是paper説根據learn好的translator。translator會有一個encoder，會把input轉換成一個vector，一個decoder根據這個vector解回翻譯的結果。如果把不同語言都丟進這個encoder裡頭，讓它變成vector的話，這些不同語言的不同句子，在這個space上的分佈關西會有什麼關係？ 比如說圖上有三個句子分別是英文、韓文以及日文的句子，這三個句子講的都是同樣的事情，是一樣的意思。通過encoder，可以發現它們都在space上都在差不多的位置。左邊(ａ)圖不同顏色代表是說這些句子都來自相同意思，但可能來自不同語言。machine做到的事情就是，他發現一種sequence language，對每一個語言，都先轉換成只有machine自己知道的 sequence language，就算是有某一個task，input跟output是machine沒看過的，它也可以透過自己學出的sequence language來做translation。 Q &amp; A感謝觀看～～～","categories":[{"name":"Learning Note","slug":"Learning-Note","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/Learning-Note/"}],"tags":[{"name":"TL","slug":"TL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/TL/"}],"author":"Chris"},{"title":"Leetcode -- linked list","slug":"Leetcode -- linked list","date":"2020-03-26T21:00:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/26/Leetcode -- linked list/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/26/Leetcode%20--%20linked%20list/","excerpt":"Leetcode – linked list🧠幫助大家回憶並練習linked list的概念","text":"Leetcode – linked list🧠幫助大家回憶並練習linked list的概念 講解題目1 solution of question 1大家先想想看🤔，敬請期待👀直觀解法： 自迴圈解法： 講解題目2 solution of question 2大家先想想看🤔，敬請期待👀直觀解法： 隱藏版進階題 solution of question 3有足夠時間再講解🧐🧐🧐直觀解法：","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"}],"author":"Jeff"},{"title":"Basic Overview of Convolutional Neural Network","slug":"Basic Overview of Convolutional Neural Network","date":"2020-03-26T13:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/26/Basic Overview of Convolutional Neural Network/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/26/Basic%20Overview%20of%20Convolutional%20Neural%20Network/","excerpt":"這篇文章主要是提供一個CV domain的overview，帶領大家快速回顧CNN的發展及應用","text":"這篇文章主要是提供一個CV domain的overview，帶領大家快速回顧CNN的發展及應用 Convolution in CNN傳統影像處理的技術會使用Image filter的技術來處理圖像，透過不同的濾波器可以對圖片進行convolution而得到不同的結果: low pass filter high pass filter 在以往這些filter的權重是透過研究而得出的，也就是一堆人都在研究各式各樣不同的filter。但是有沒有辦法讓電腦自己學習什麼樣的filter對我們的task最好呢? 這就是Convolutional Neural Network, CNN在做的事情。 CNN in Computer Vision domain常見的CV領域應用如下，接下來會快速go through一遍每個領域，最後聚焦在Object Classification的各種模型。 Object Classification Object Detection and Localization Object Segmentation Object Identification Object tracking + ActionRecognition Object Detection and Localization 除了判斷物體類別外，還要知道物體的bounding box Issue探討: 是先找到物體在去框box，還是先找box在來判斷物體? 當透過模型得出一個物體有多個可能符合的box，如何找到最佳的box? 找box跟classification看似是兩個不同的task，是否可以一起做(end-to-end)? Related research:R-CNN, SPPnet, Fast R-CNN, Faster R-CNN, YOLO, SSD, YOLOv2, YOLO9000, FPN, YOLOv3 Object Segmentation Segmentation可以說是 pixel-wise的classification，能夠提供更有效的語意資訊 Issue探討: 核心概念: patch + sliding window 將圖片分成許多小patch，針對每個patch去預測center pixel的label Issue1: high computational cost? Issue2: accuracy/localization trade off? Unsampling(deConvolution)如何對應回原本的點? Pooling具有平移不變性(translation invariance)，對於classification較robust，但對於segmentation則不好，如何解決? Semantic Segmentation (category-wise) v.s. Instance Segmentation (object-aware) Related research:FCN, DeepLab, SegNet, Enet, ICNet, DeepMask, SharpMask, InstanceFCN, R-FCN, MNC, Mask R-CNN Object IdentificationIssue探討: 臉部辨識技術? 如何有有效率的用一個向量來表達一張圖片? 應用在圖像檢索(image retrieval) Binary descriptor + Hamming distance Related research:Deepface, FaceNet, Binary Hashing Code, DeepBit Object tracking + ActionRecognitionConv有1d, 2d, 那Conv3d到底用在什麼地方? 就是這裡了! 辨識影片中某時刻的動作: 追蹤影片中的物體: Object ClassificationImageNet Large Scale Visual Recognition Challenge(ILSVRC) ImageNet dataset 最後一屆2017年，Why? 已經比人眼辨識率還低了你還想怎樣QQ LeNet for MNIST digit recognition 2 Conv(Convolution layer) + 2 FC(Fully Connected layer) AlexNet 2 GPU to train 2012 ILSVRC Winner(top 5 error rate: 16.4%) 8 layers architecture first use “ReLU” as activation function use data augumentation for training 從此之後開始，正式進入大調參時代 ZFNet 2013 ILSVRC Winner(top 5 error rate: 11.7%) Same to AlexNet, but use smaller kernel size &amp; increase kernel numbers VGG 2014 ILSVRC 2nd place(top 5 error rate:7.3%) more deeper! 19 layers use 3x3 Conv &amp; 2x2 Maxpooling only: 目的是用多層的較小的filter達到一個大的filter包含的資訊。舉例來說2個3×3的Conv可以和1個5×5的Conv涵蓋一樣的資訊量，但參數量卻比較少。 do pooling after mutli-time Conv: 相較於以前一個Conv後面就接一個Pooling，VGG做了很多次Conv才接Pooling，因為這樣可以透過activation function使data有更多non-linear的變化。 GoogLeNet 2014 ILSVRC Winner(top 5 error rate: 6.7% ) Let’s keep go deeper!! 22 layers inception model use 1x1 Conv to reduction dimension 1D-CNN? NIN! 人在江湖飄，不能不知道。走CV你不能不知道的牛逼論文: Network In Network ResNet 2015 ILSVRC Winner(top 5 error rate: 3.6% ) more!!give me more depper!!!!! 152 layers residual block 層數越多時，其實效果不一定會越好，因為information在高層中會很難進行傳播 SENet 2017 ILSVRC Winner(top 5 error rate: 2.251% ) ResNet + SE block learn weights for different kernel 可以想成是對feature map做attention Global Average Pooling GAP? NIN! 再說一次，人在江湖飄，不能不知道。走CV你不能不知道的牛逼論文: Network In Network Issue探討CNN越來越大到底GPU怎麼吃 ( •́ _ •̀)？ DenseNet Dense Block 每一層的輸入都包含了前一層的資訊(concat)，降低了在層數過大時不容易propagation的問題 每一層的輸入雖然很多，但參數量其實很少(Densnet的每一層的kernel數不用像其他model那麼多) 因為特徵共用，大部分輸入來自其它層 越深層，concat的input還是會越來越多，怎麼辦? GAP GAP? NIN! 再說一遍，走CV你不能不知道的牛逼論文: Network In Network MobileNet Depthwise separable convolution: 透過不同的計算方式，在減少參數計算量下達到和一般convolution等價的結果 Depthwise convolution pointwise convolution Conv1d -&gt; NIN! 最後一遍，走CV你不能不知道的牛逼論文: Network In Network 也就是說，任何CNN model都可以透過Depthwise separable convolution得到計算量跟參數量更小的等價model","categories":[{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"}],"tags":[{"name":"cv","slug":"cv","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/cv/"}],"author":"John"},{"title":"NLP Automated Collocation Suggestion for Japanese Second Language Learners","slug":"NLP Automated Collocation Suggestion for Japanese Second Language Learners","date":"2020-03-25T19:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/25/NLP Automated Collocation Suggestion for Japanese Second Language Learners/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/25/NLP%20Automated%20Collocation%20Suggestion%20for%20Japanese%20Second%20Language%20Learners/","excerpt":"NLP Automated Collocation Suggestion for Japanese Second Language Learnerstags: references, NLP論文網址 Collocation簡介：當使用bigram處理一串文字時，要如何知道這個bigram是有意義的？(代表這兩個tokens經常一起出現，Note：不是兩個字 一起才有意義)通常有三種測量方法","text":"NLP Automated Collocation Suggestion for Japanese Second Language Learnerstags: references, NLP論文網址 Collocation簡介：當使用bigram處理一串文字時，要如何知道這個bigram是有意義的？(代表這兩個tokens經常一起出現，Note：不是兩個字 一起才有意義)通常有三種測量方法 使用token的mean,variance(deviation)看兩字經常出現的相對固定位置 使用虛無假設：假設兩個字彼此獨立，期望透過significant level來拒絕此假設 交互訊息：看兩個字的相對關係名詞方法簡介 Collocation 如上述 Automatic grammatical error correction 自動語言偵測錯誤 word similarity measures 單詞相似性度量 collocation candidate 搭配候選詞 wordNet：可用來對每一個字串產生同意義(synonymms)的字串 first language：語言學習者的第一語言（對我們來說中文） correction ： 當成ground truth learner/writer ： 當成input noun wo verb ： 本文特定focus的句型（語法) alternative : 替代方案 confusion set : 混亂集0 大綱學日語的人儘管寫出語意上正確的句子但可能“不自然”，希望能夠創造一個系統讓日語學習者自動搭配文法。方法是使用不同的語料搭配和單詞相似度(word similarity)方法去分析正確的單詞組合。此文的方法比其他模型更一般，儘管其他模型使用較完美訓練資料;而此文使用大型日語資料，但此文取得較好效果。 1 介紹訂正機器的語法錯誤是常見的NLP問題，而另一篇論文[Gamon]只注重在data error(noise) Collocation 在日語中是普遍現象，但在機器上並非如此，因為語言對文字的使用方法不同。不好的範例:一般日文的使用yume wo miru [lit. to see a dream]但日語學習者常會使用yume wo suru [lit. do a dream]suru 在英語上表示 do, 但 miru 常代表 to see.故這篇文章希望能夠透過分析多種日文語料庫去給予最佳的文法搭配給日語學習者。為了讓模型學到這類更為細部的結構資訊，我們使用單詞相似性度量(word similarity)，這些度量使用大型日語學習者語料庫去生成搭配候選詞(Candidate)。 2 相關資訊Collocation在訓練時通常將學習者的單詞與文本生成的混亂集去做比較 ，如果一個或多個替代方案(alternative)更適合上下文，則標記錯誤，並且建議其更正文法，並且利用測量方法去排序最好的替代方案。本篇使用與一般方法相同，但我們使用了更大的語料庫去取代原本的well-formed text去產生混亂集。 而為了使confusion的組合數目不要過大，通常使用similarity measure。為了找到最適合的組合，需要考量句子結構和被產生的alternative結構。ex：wordNet：可用來對每一個字串產生同意義的字串 使用wordnet和(rank ration measure)，再算他們的semantic similarity(scoring) 使用wordnet和Pointwise Mutual information當成測量排名方法 使用bilingual dictionaries（字典集）來找collocation，使用log-likelihood測排名（但是此方法受限於字典及大小，詞庫或手動建構數據庫） 本文使用：similarity來產生confusion的集合，和比較相關度來ranking 在本篇論文，主要關注在noun-を-verb，總共有三個步驟：(1) 由於兩語言字用法不同，對於日語學習者文章中的每一個token，利用word similarity去創建一組候選詞語(2) 我們用association measures計算關聯性在使用者的詞語和候選詞語的collocation scores(3) 並且將最高分的候選詞語當作我們的更正，並與日語學習者文章去做比對是否相同。 對所有文本結構：使用替代短句來創建confusion set，測量文本中短句和被生成的短句的關聯強度(association measure)，並且只在文本短句的score較低時標記為錯誤，且把排名較高的替代詞語當成正確 Word Similarity1234567相似度測量用來生成搭配候選詞(collocation candidate)，這些搭配候選詞會用關聯度測量去排名。這裡我們使用三種方法去測量1. 基於詞庫的相關性 (Thesaurus-based word similarity)2. 分布相關度 (Distributional similarity)3. 從學習者的語料庫取出的混亂集 (Confusion set derived from learner corpus)1和2 都是從與作者相似的詞彙去選出搭配候選詞。3則是根據學習者語料庫中母語使用者的更正產生候選者。 1. 基於詞庫的相關性 (thesaurus-based word similarity):檢查兩個字之間是否有相似度(glosses)也就是檢查兩者的詞庫層次是否接近(距離在某個threshold內)但召回率通常偏低 2. 分布相關度 (Distributional Similarity):因為第一種的記憶力很弱且因為是人力定義的沒有考量到短句語意，特別是在verb adj。因為後選詞集較大，這種方法精準度較低，但召回率還不錯高。用來判斷“相似”的方法為：看是否有類似的上下文。 本文基於語法結構的相依性把context表成vector，ex(object-verb)會找類似的noun verb，上下文在此時(Table 2)特定noun+一個verb vector 且對此noun有一定的相關性(Table 3)特定verb+一個noun vector 且對此verb有一定的相關性圖中的數字為一起出現的frequency，計算這些一起出現的單字的相似度：(1)Cosine Similarity(2)Dice coefficient(3)KL divergence/relative entropy(4)Jenson-Shannon divergence根據這些上下文數字出現的頻率我們可以去計算他們的相似度 3. 從學習者的語料庫取出的混亂集 (Confusion set derived from learner corpus):為了構建可以“猜測”常見構造錯誤的模塊，我們使用Lang-8語料庫創建了一個混亂集。不生成與學習者的書面結構具有相似含義的單詞，而是從數據中找到的每個名詞和動詞提取了所有可能的名詞和動詞更正。(Table 1 第一列)就是寫了suru，但可能他的想法應該是另外一個的其中一個的verb(Table 1 第二列)就是寫了biru，但可能他的想法應該是另外一個的其中一個的nounEX: 對日文學習者可能造成的confusion：biru-&gt;beer拉長音了，或是語言用法不同 4. Word Association Strength：根據上述三個相關性產出的候選詞去找出最佳的候選詞，計算最佳相關性分數(association scores)本文使用1.Dice coefficient(best measurement)當成associate的量度2.Pointwise Mutual information3.log-likelihood這個過程也基本可以排除偶然產生的單詞對，高分表示關聯性大。 3 模型 實驗分成兩個部分:一個為動詞替代(verb suggestion)一個為名詞替代(noum suggestion) 4 資料集Training set: 1) Bunrui Goi Hyo Thesaurus (The National Institute for Japanese Language, 1964) :超過10萬單字，3萬多個語意類別，用來計算字的相似度(word similarity)，使用在相同子樹的word當candidate，在與leaf距離=2當成預定的tree traversal。2) Mainichi Shimbun Corpus (Mainichi Newspaper Co., 1991) :The newspaper data, 用來提取字的元組(noun-を-verb)去計算相似度(Cosine similarity)和他的搭配分數(collocation score)。224185個pair，16781個unique verbs，37300 unique nouns。3) Balanced Corpus of Contemporary Written Japanese, BCCWJ Corpus (Maekawa, 2008) :一億個字彙，雜誌，報紙，教科書和部落格，多類別用以減少學習者差距，提取了接近20萬的名詞-を-動詞，43,243個名詞和18,212個動詞，同樣的會去計算相似度(Cosine similarity)和他的搭配分數(collocation score)。4) Lang-8 Corpus: Consisted of two year data(2010):100萬個pairs of corrections and learner’s sentence. 同樣的去計算相似度(Cosine similarity)和他的搭配分數(collocation score)，資料有163880 pairs，38999 nouns，16086 verbs，使用第三種方法去創建混亂集。2011年的資料會被作為測試集 Test set: Lang-8 (2011 data):把Lang-8拿來用，對動詞來說，基於名詞-を-動詞把不正確和正確動詞拿來一起看，當有動詞被相同的動詞糾正(corrected)$\\ge 5$次時（by native speakers），對名詞亦然。由於我們只使用名詞+動詞，故我們需要刪除上下文的修正，我們使用(Weighted Dice coefficient)去計算所有名詞和動詞的關聯性，過濾掉學習者的結構分數高於校正的結構，動詞有185組測試，名詞有85組。 Evaluation Metrics我們將系統建議的”分數排序的混亂集的動詞”與Lang-8數據中的人類correction動詞和名詞進行比較:Match：tp沒有提供任何有效建議：fnMRR 最主要是去評估建議清單是否包含人工correction或距離有多遠。 $N = test \\ size$model doesn’t have correction set $MRR = \\frac{1}{rank(i)}=0$Recall rate = $\\frac{tp}{tp+fn}$ 5 結果評估指標 : precision, recall, mean reciprocal rank(MRR 平均倒數排名) 主結果使用各式(3種)word similarity 和 Weighted Dice(association)data set來算word similarity，再來算collocation score(by Weighted Dice)用2個dataset來算減少learner domain（不同領域文字的） gap且限制confusion set的size：270$\\le$verbs; 160$\\le$nouns 動詞的結果：M1 有著高準確率，但是低召回率為了增加召回率我們創了M2M6，但同時準確率卻下降了。為了與其他的相似度模型去做比對，我們創建了M7M10，最高的召回率和MRR都是M10，並且也有不錯的準確率在k=5的時候(k=1的時候還是很差)名詞的結果：與動詞大致相同，某些現象並未出現在動詞的推薦，這表示只需要簡單的拼寫校正即可，並且對於名詞的選擇更有自信大於動詞的選擇。 最佳 MRR value in Distributional Similarity : Jenson-Shannon divergence.最佳 recall and MRR值 : M10, confusion set 被 Lang-8 data 生成 6 未來工作該模型可以直接去使用在其他類型上，例如形容詞-動詞，名詞+授與動詞(ditave-verb)。改善我們目前的結果，考慮更廣泛的上下文大小和其他類型的構造將是本研究的下一步。","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"}],"author":"KoH Po , Yiwei Chiu"},{"title":"Automatic Generation of Personalized Annotation Tags for Twitter Users","slug":"Automatic Generation of Personalized Annotation Tags for Twitter Users","date":"2020-03-24T22:30:00.000Z","updated":"2021-12-17T10:05:22.900Z","comments":true,"path":"2020/03/24/Automatic Generation of Personalized Annotation Tags for Twitter Users/","link":"","permalink":"https://github.com/dsmilab/dsmi-lab-website/2020/03/24/Automatic%20Generation%20of%20Personalized%20Annotation%20Tags%20for%20Twitter%20Users/","excerpt":"What’s the problem in this paper?In this paper, they want to tag Twitter user’s personal interests via extracting keywords.","text":"What’s the problem in this paper?In this paper, they want to tag Twitter user’s personal interests via extracting keywords. How did they solve this problem? Identifying an individual user’s interests and concerns can help potential commercial applications.For instance, this information can be employed to produce “following” suggestions, either a person who shares similar interests (for expanding their social network) or a company providing products or services the user is interested in (for personalized advertisement). Dataset in this paper Training: Messages from 11,376 Twitter users. Each user has 180~200 messages. Testing: randomly selected 156 Twitter users to evaluate the top-N precision of TFIDF ranking and TextRank techniques in this paper flow chart:針對蒐集起來的twitter進行下列的前處理 remove reply message: 因為這些回應訊息是比較針對對方的觀點而不是針對自己的言論 remove emoticons: 對於keyword analysis比較沒有幫助 Substituting / removing internet slangs(厘語) and abbreviations: 這類詞彙可以分成三類 a. 可以被正名的有意義的詞彙: bff(best friend forever), fone(phone) b. 文法上的縮寫: im(i’m), abt(about)，如果刪除會影響POS tagging的結果 c. 用來斷句的詞彙，通常沒有實質意義: lol(laugh out loud), clm(cool like me)，在這篇的方法中會被刪除 Part-of-Speech tagging and filtering Stanford POS tagger只篩選出名詞和形容詞 Stemming and stopword removing：use Porter stemmer去除字尾，然後去除stop words TFIDF ranking: messages from user are put together as one document. $n_{i,u}:$ the count of word i in user u’s messages $U_i:$ the number of users whose messages containg word i $U:$ the total number of users in the Twiter corpus TextRank: build a TextRank graph with undirected edges for each Twitter user weight of edge: 這兩個字在幾篇文章中有同時出現(co-exist) where $w_{ji}$ is the weight of the edge that links $V_{j}$ and $V_{i}$, $E(V_{i})$ is the set of vertices which $V_{i}$ is connected to. $d$(a damping factor) is set to 0.85: reference Google’s PageRank algorithm The rank update iteration continues until convergence. evaluation After we obtained the topN outputs from the system, three human evaluators were asked to judge whether the output tags from the two systems (unidentified) reflected the corresponding Twitter user’s interests or concerns according to the full set of his/her messages result top-N代表選出的前N個標籤都要經過三個專家判斷是否這些tags都符合該文章內容，所以很合理的，當N越大的時候precision會越低 Although most Twitter users express their interests to some extent in their messages, there are someusers whose message content is not rich enough toextract reliable information. We investigated twomeasures for identifying such users: standard deviation of the top-10 TextRank :一個text rank graph，每一個rank所計算出的標準差。越高代表說每一個word之間的rank差異很大，因此找出的tag比較能反應user的偏好。相反越低代表每一個word之間rank差異不大，因此找出的tag就比較不具有鑑別性。 text entropy: 刻劃user message 的豐富度。越高表示message 內容越豐富。當user message 內容越豐富，越能找出具有代表性的tag。 new idea To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres.Previous work on both TFIDF ranking and TextRank has been done mainly on formal language style text, such as academic papers, spoken documents or web pages than that of Twitter messages. 這是第一個如何透過”user message”來找出重要的tag的研究 Reference Stanford Log-linear Part-Of-Speech Tagger The Porter Stemming Algorithm Google’s PageRank Algorithm: A Diagram of theCognitive Capitalism and the Rentier of the CommonIntellect google search engine早期使用的技術: PageRank problem?fantastic !wonderful !amazing !fabulous !","categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"}],"tags":[{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"}],"author":"John"}],"categories":[{"name":"nlp study group","slug":"nlp-study-group","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/nlp-study-group/"},{"name":"meeting","slug":"meeting","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/meeting/"},{"name":"Learning Note","slug":"Learning-Note","permalink":"https://github.com/dsmilab/dsmi-lab-website/categories/Learning-Note/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP/"},{"name":"Attention","slug":"Attention","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Attention/"},{"name":"SVM","slug":"SVM","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SVM/"},{"name":"SSL","slug":"SSL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/SSL/"},{"name":"Semi supervised learning","slug":"Semi-supervised-learning","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Semi-supervised-learning/"},{"name":"Git","slug":"Git","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/GitHub/"},{"name":"Adversarial","slug":"Adversarial","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Adversarial/"},{"name":"NLP Topic_Modeling","slug":"NLP-Topic-Modeling","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Topic-Modeling/"},{"name":"NLP Pointer Summary","slug":"NLP-Pointer-Summary","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Pointer-Summary/"},{"name":"NLP Embedding","slug":"NLP-Embedding","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/NLP-Embedding/"},{"name":"sentence similarity","slug":"sentence-similarity","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/sentence-similarity/"},{"name":"papers","slug":"papers","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/papers/"},{"name":"attention","slug":"attention","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/attention/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Leetcode/"},{"name":"cv","slug":"cv","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/cv/"},{"name":"semi-supervised","slug":"semi-supervised","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/semi-supervised/"},{"name":"language model","slug":"language-model","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/language-model/"},{"name":"clustering","slug":"clustering","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/clustering/"},{"name":"Time series","slug":"Time-series","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/Time-series/"},{"name":"TL","slug":"TL","permalink":"https://github.com/dsmilab/dsmi-lab-website/tags/TL/"}]}