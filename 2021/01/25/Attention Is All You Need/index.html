<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>attention is all you need? | DSMI Lab&#39;s website</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="NLP" />
  
  
  
  
  <meta name="description" content="paper : Attention Is All You Need AbstractThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best perf">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Is All You Need?">
<meta property="og:url" content="https://github.com/dsmilab/dsmi-lab-website/2021/01/25/Attention%20Is%20All%20You%20Need/index.html">
<meta property="og:site_name" content="DSMI Lab&#39;s website">
<meta property="og:description" content="paper : Attention Is All You Need AbstractThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best perf">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/FFe0szL.png">
<meta property="og:image" content="https://i.imgur.com/16ecJHq.png">
<meta property="og:image" content="https://i.imgur.com/P4Hh2ZN.png">
<meta property="og:image" content="https://i.imgur.com/gZTMYfH.png">
<meta property="og:image" content="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png">
<meta property="og:image" content="https://i.imgur.com/oqCIIAB.png">
<meta property="og:image" content="https://i.imgur.com/VaEqBFE.png">
<meta property="og:image" content="https://i.imgur.com/mli8HW6.png">
<meta property="og:image" content="https://i.imgur.com/rr8btFr.gif">
<meta property="article:published_time" content="2021-01-25T12:30:00.000Z">
<meta property="article:modified_time" content="2021-12-17T10:05:22.896Z">
<meta property="article:author" content="DSMI members">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/FFe0szL.png">
  
    <link rel="alternate" href="/atom.xml" title="DSMI Lab&#39;s website" type="application/atom+xml">
  

  

  <link rel="icon" href="/dsmi-lab-website/css/images/DSMI.png">
  <link rel="apple-touch-icon" href="/dsmi-lab-website/css/images/DSMI.png">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/dsmi-lab-website/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/dsmi-lab-website/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/dsmi-lab-website/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/dsmi-lab-website/css/style.css">


  
<script src="/dsmi-lab-website/js/jquery-3.1.1.min.js"></script>

  
<script src="/dsmi-lab-website/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/dsmi-lab-website/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/dsmi-lab-website/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/dsmi-lab-website/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 4.2.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/dsmi-lab-website/css/images/DSMI.png">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/dsmi-lab-website/',
        CONTENT_URL: '/dsmi-lab-website/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/dsmi-lab-website/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Attention Is All You Need" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Attention Is All You Need?
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/dsmi-lab-website/2021/01/25/Attention%20Is%20All%20You%20Need/" class="article-date">
	  <time datetime="2021-01-25T12:30:00.000Z" itemprop="datePublished">2021-01-25</time>
	</a>

      
    <a class="article-category-link" href="/dsmi-lab-website/categories/nlp-study-group/">nlp study group</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
        
          <p>paper : <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely .<br>這篇論文中提出了一個新的簡單網路結構 Transformer，它是一個基於 attention mechanisms的網路結構。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t−1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p>
<ul>
<li>RNN主要是透過將sequence的位置與time steps對齊來考慮不同sequence之間的關係</li>
<li>有一個問題 : 無法平行運算</li>
</ul>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3. Model Architecture"></a>3. Model Architecture</h2><p><img src="https://i.imgur.com/FFe0szL.png" alt=""></p>
<h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><ul>
<li><p>Encoder : 每層有兩個sub-layers</p>
<ul>
<li>multi-head self-attention</li>
<li>fully connected </li>
<li>residual connection + layer normalization<ul>
<li>the output of each sub-layer is $LayerNorm(x + Sublayer(x))$</li>
</ul>
</li>
</ul>
</li>
<li><p>Decoder : 在原本的兩個子層中間插入一個新的 sub-layer，其連接 Encoder 的輸出進行 Multi-Head Attention。與 Encoder 部分相同，每一個 sub-layer均有 Residual Connection，並進行 Layer Normalization。</p>
<ul>
<li>masked multi-head attention<ul>
<li>透過一個 mask 確保 attention 在 i 時刻不會關注到後面的的資料<br>(by masking out (setting to −∞)) </li>
</ul>
</li>
<li>multi-head attention</li>
<li>fully connected</li>
<li>residual connection + layer normalization</li>
</ul>
</li>
</ul>
<h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<ul>
<li>透過 query 和 key-value 的 mapping<br>$q$ : query(to match others)<br>$\quad q^i = W^q a^i$<br>$k$ : key(to be matched)<br>$\quad k^i = W^k a^i$<br>$v$ : value(information to be extracted)<br>$\quad v^i = W^v a^i$</li>
</ul>
<p>![](<a href="https://i.imgur.com/MAtkxWw.png" target="_blank" rel="noopener">https://i.imgur.com/MAtkxWw.png</a> =80%x)</p>
<p><img src="https://i.imgur.com/16ecJHq.png" alt=""></p>
<h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p>
<ul>
<li>$d_k$ 是Q, K的dimension</li>
<li>attention常見的兩種操作<ul>
<li>additive attention</li>
<li>dot attention </li>
</ul>
</li>
</ul>
<h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>$MultiHead(Q,K,V) = Concat(head_1,…,head_h)W^O$<br>where $head_i$ = $Attention(Q{W_i}^Q,K{W_i}^K,V{W_i}^V)$</p>
<!-- where ${W_i}^Q\in \mathbb{R}^{d_{model}\times d_k},$ -->
<p>![](<a href="https://i.imgur.com/VR9ZUH7.png" target="_blank" rel="noopener">https://i.imgur.com/VR9ZUH7.png</a> =80%x)</p>
<h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>The Transformer uses multi-head attention in three different ways:<br>![](<a href="https://i.imgur.com/FFe0szL.png" target="_blank" rel="noopener">https://i.imgur.com/FFe0szL.png</a>  =50%x)</p>
<ul>
<li>encoder-decoder attention layers : Q來自上一層decoder的輸出; K跟V來自encoder最後一層的輸出</li>
<li>encoder layers : Q,K,V來自相同的input</li>
<li>decoder layers : Q,K,V來自相同的input</li>
</ul>
<h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h3><p>有兩層FC, 第一層的acivation 是 ReLU, 第二層是 linear activation, 可以表示為<br>$$FFN(Z)= max(0,ZW_1+b_1)W_2+b_2$$ </p>
<h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a>3.4 Embeddings and Softmax</h3><h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>由於 self attention 會看 sequence 的每一個資料(也就是說 , “天涯若比鄰” “比天若涯鄰” 的結果會是相同的 )<br>![](<a href="https://i.imgur.com/woA80xJ.png" target="_blank" rel="noopener">https://i.imgur.com/woA80xJ.png</a> =30%x)<br>![](<a href="https://i.imgur.com/etHqUjb.png" target="_blank" rel="noopener">https://i.imgur.com/etHqUjb.png</a> =80%x)</p>
<p>$$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$</p>
<p>$$PE_{(pos,2i+1)} =  cos(pos/10000^{2i/d_{model}})$$</p>
<p>![](<a href="https://i.imgur.com/d9pVblB.png" target="_blank" rel="noopener">https://i.imgur.com/d9pVblB.png</a> =40%x)</p>
<p>以下引用<a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" target="_blank" rel="noopener">自此文章</a> , 有興趣可以去看。</p>
<blockquote>
<p><img src="https://i.imgur.com/P4Hh2ZN.png" alt=""><br><img src="https://i.imgur.com/gZTMYfH.png" alt=""></p>
</blockquote>
<p>畫起來就是下面這種神奇的圖</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png" alt=""></p>
<p>有點像用binary format來表示的概念,只是把他轉成用$cos, sin$<br>以下引用<a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" target="_blank" rel="noopener">自此文章</a> </p>
<blockquote>
<p>But using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - Sinusoidal functions.<br><img src="https://i.imgur.com/oqCIIAB.png" alt=""></p>
</blockquote>
<p>為什麼要用這個position轉換呢？</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.</p>
</blockquote>
<p>（至於詳細證明可以參考這篇<a href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/" target="_blank" rel="noopener">文章</a>）</p>
<h3 id="4-Attention-Visualization"><a href="#4-Attention-Visualization" class="headerlink" title="4. Attention Visualization"></a>4. Attention Visualization</h3><p><img src="https://i.imgur.com/VaEqBFE.png" alt=""><br><img src="https://i.imgur.com/mli8HW6.png" alt=""></p>
<p><img src="https://i.imgur.com/rr8btFr.gif" alt=""></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></li>
<li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf</a></li>
<li><a href="https://meetonfriday.com/posts/5839a8bf/#Position-wise-Feed-Forward-Networks" target="_blank" rel="noopener">https://meetonfriday.com/posts/5839a8bf/#Position-wise-Feed-Forward-Networks</a></li>
</ol>

        
      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>yuan</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/dsmi-lab-website/2021/01/25/Attention Is All You Need/" target="_blank" title="Attention Is All You Need?">https://github.com/dsmilab/dsmi-lab-website/2021/01/25/Attention Is All You Need/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'dsmi-lab';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dsmi-lab-website/tags/NLP/" rel="tag">NLP</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/dsmi-lab-website/2021/05/07/Data%20Noising%20as%20Smoothing%20in%20Neural%20Network%20Language%20Models/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Data Noising as Smoothing in Neural Network Language Models
        
      </div>
    </a>
  
  
    <a href="/dsmi-lab-website/2020/12/01/TSVM%20(Transductive%20Inference%20for%20Text%20Classification%20using%20Support%20Vector%20Machines)/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">TSVM (Transductive Inference for Text Classification using Support Vector Machines)</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Background"><span class="nav-number">3.</span> <span class="nav-text">2. Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Model-Architecture"><span class="nav-number">4.</span> <span class="nav-text">3. Model Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Encoder-and-Decoder-Stacks"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Encoder and Decoder Stacks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Attention"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-Scaled-Dot-Product-Attention"><span class="nav-number">4.2.1.</span> <span class="nav-text">3.2.1 Scaled Dot-Product Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-Multi-Head-Attention"><span class="nav-number">4.2.2.</span> <span class="nav-text">3.2.2 Multi-Head Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-Applications-of-Attention-in-our-Model"><span class="nav-number">4.2.3.</span> <span class="nav-text">3.2.3 Applications of Attention in our Model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Position-wise-Feed-Forward-Networks"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Position-wise Feed-Forward Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Embeddings-and-Softmax"><span class="nav-number">4.4.</span> <span class="nav-text">3.4 Embeddings and Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Positional-Encoding"><span class="nav-number">4.5.</span> <span class="nav-text">3.5 Positional Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Attention-Visualization"><span class="nav-number">4.6.</span> <span class="nav-text">4. Attention Visualization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">4.7.</span> <span class="nav-text">Reference</span></a></li></ol></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 DSMI Lab&#39;s website All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/dsmi-lab-website/" class="mobile-nav-link">Home</a>
  
    <a href="/dsmi-lab-website/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/dsmi-lab-website/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/dsmi-lab-website/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/dsmi-lab-website/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/dsmi-lab-website/fancybox/jquery.fancybox.css">

  
<script src="/dsmi-lab-website/fancybox/jquery.fancybox.pack.js"></script>




<script src="/dsmi-lab-website/js/scripts.js"></script>





  
<script src="/dsmi-lab-website/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字號大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已調整頁面字體大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜間護眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜間模式已經開啟，再次單擊按鈕即可關閉
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            DSMI Lab&#39;s website
          </div>
          <div class="panel-body">
            Copyright © 2021 DSMI members All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>