<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>supervised topic modeling using word embedding with machine learning techniques | DSMI Lab&#39;s website</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="NLP Topic_Modeling" />
  
  
  
  
  <meta name="description" content="Supervised Topic Modeling Using Word Embedding with Machine Learning Techniquestags: NLP,Topic Modelingieee論文網址 Abstract基於 HMM model做出的 supervised embedding達成分類問題貢獻：單詞含義和順序對於topic model的重要性。使用LSTM和CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques">
<meta property="og:url" content="https://github.com/dsmilab/dsmi-lab-website/2020/10/22/Supervised%20Topic%20Modeling%20Using%20Word%20Embedding%20with%20Machine%20Learning%20Techniques/index.html">
<meta property="og:site_name" content="DSMI Lab&#39;s website">
<meta property="og:description" content="Supervised Topic Modeling Using Word Embedding with Machine Learning Techniquestags: NLP,Topic Modelingieee論文網址 Abstract基於 HMM model做出的 supervised embedding達成分類問題貢獻：單詞含義和順序對於topic model的重要性。使用LSTM和CNN">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/VFtYP2t.png">
<meta property="article:published_time" content="2020-10-22T10:00:00.000Z">
<meta property="article:modified_time" content="2021-12-17T10:05:22.900Z">
<meta property="article:author" content="DSMI members">
<meta property="article:tag" content="NLP Topic_Modeling">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/VFtYP2t.png">
  
    <link rel="alternate" href="/atom.xml" title="DSMI Lab&#39;s website" type="application/atom+xml">
  

  

  <link rel="icon" href="/dsmi-lab-website/css/images/DSMI.png">
  <link rel="apple-touch-icon" href="/dsmi-lab-website/css/images/DSMI.png">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/dsmi-lab-website/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/dsmi-lab-website/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/dsmi-lab-website/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/dsmi-lab-website/css/style.css">


  
<script src="/dsmi-lab-website/js/jquery-3.1.1.min.js"></script>

  
<script src="/dsmi-lab-website/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/dsmi-lab-website/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/dsmi-lab-website/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/dsmi-lab-website/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 4.2.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/dsmi-lab-website/css/images/DSMI.png">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/dsmi-lab-website/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/dsmi-lab-website/',
        CONTENT_URL: '/dsmi-lab-website/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/dsmi-lab-website/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/dsmi-lab-website/2020/10/22/Supervised%20Topic%20Modeling%20Using%20Word%20Embedding%20with%20Machine%20Learning%20Techniques/" class="article-date">
	  <time datetime="2020-10-22T10:00:00.000Z" itemprop="datePublished">2020-10-22</time>
	</a>

      
    <a class="article-category-link" href="/dsmi-lab-website/categories/nlp-study-group/">nlp study group</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
        
          <h1 id="Supervised-Topic-Modeling-Using-Word-Embedding-with-Machine-Learning-Techniques"><a href="#Supervised-Topic-Modeling-Using-Word-Embedding-with-Machine-Learning-Techniques" class="headerlink" title="Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques"></a>Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques</h1><h6 id="tags-NLP-Topic-Modeling"><a href="#tags-NLP-Topic-Modeling" class="headerlink" title="tags: NLP,Topic Modeling"></a>tags: <code>NLP</code>,<code>Topic Modeling</code></h6><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194177&fbclid=IwAR33UhJ0qpNSdmLx3NB47kYVIKJyT1ARsB2HThvuJDWBccwTCtpk1-GZ_Cw&tag=1" target="_blank" rel="noopener">ieee論文網址</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>基於 HMM model做出的 supervised embedding達成分類問題<br>貢獻：單詞含義和順序對於topic model的重要性。<br>使用LSTM和CNN達到SOTA</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>非結構化的文本資料到處都有。儘管文本中包含了豐富的訊息，為了利用這些資料，必須先根據內容組織，理解和總結。需要通過新的統計模型分析這些數據，且速度必須要快。</p>
<p>Topic modeling使用 supervised 或是 unsupervised 的統計機器學習技術來處理大型語料庫。</p>
<ul>
<li>與unsupervised的方法相比較，supervised可以節省研究者大量時間。</li>
<li>而unsupervised的方法在於可以處理大多dataset，但代價是其準確性和更長的訓練時間</li>
</ul>
<p>LSA和LDA分析文件中的單詞，已發現貫穿他們的主題以及這些主體如何與另一個主題聯繫，但是由於缺少對於特定task的優化，缺少task-specific feature。而後來有word embedding可以用來fine tuning on task。<br>此外，這個model可以使用單詞結構，含義和順序，因為由HMM和RNN組成，可以分辨出數據序列。當雨word embedding一起使用時，可以帶別單詞的含義。<br> 本文提出了一種新的主題建模方法，該方法考慮了語義（單詞的含義）並具有句子中單詞順序的感覺。<br> Model在訓練時，使用”分佈式表示(就是embedding i.e.Word2vec and Glove)”來訓練模型，這些表示允許具有相似含義或語法特性的單詞具有相似向量。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Generative-Models-機率"><a href="#Generative-Models-機率" class="headerlink" title="Generative Models(機率)"></a>Generative Models(機率)</h3><ul>
<li>用來抓取全局的語意一致性的model，例如LDA使用Dirichlet distribution，假設每一個文件都是主題的機率分佈，每一個主題都是文件中單詞的機率分佈，使用conjugate prior 可以從小型到大型數據之間衍生。</li>
<li>主題關聯模型(CTM)，解決LDA無法對建構的主題考量其關聯性的問題。使用normalize log normal prior，可以抓取成對的主題相關性，使用stochastic variational inference。其增強版本：Efficient CTM，可以透過比較topic vector的相近成度，判斷相關性。</li>
<li>Gaussian-LDA假設文件的輸入是一系列的word embedding而不是系列的單詞類型。</li>
<li>最近較新的方法(DocNADE,an extension of NADE)，從未標記的文件中學習有意義的文件表示，並以bows表示文件。</li>
</ul>
<h3 id="Using-Word-Embedding"><a href="#Using-Word-Embedding" class="headerlink" title="Using Word Embedding"></a>Using Word Embedding</h3><ul>
<li>TWE通過使用lantent topic model，給文件中的每一個單詞分配主題，TWE通過使用NN來同時學習topic 和word。</li>
<li>BoE(Bag-of-Embeddings)，通過給定文件的指定主題最大化其單詞的embedding probability來預測其主題。</li>
<li>ToWE(Task-oriented Word Embedding)，學習與給定的task相關的word distribution representation。著重於結合單詞的語意和特定任務的特徵。透過regularize突出詞的分佈，使其具有明確的分類邊界獲取task specific feature，丙且調整空間中的其他詞的分佈。</li>
<li>GMNTM：使用文件中單詞的語意和順序，且作為另一個方向：使用Neural probabilistic methods，例如RNNLM。</li>
</ul>
<h3 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><ul>
<li>NMF，關注主題一致性（有意義的主題），數入室Bow並產生兩個較小的矩陣，文件到主題矩陣和單詞到主題矩陣，相乘後生成Bow矩陣</li>
<li>NNDSVD，為NMF得增強版，使用SVD以其向量初始化NMF，對於sparse data特別有效(i.e. text)</li>
<li>Ensemble learning strategy：基於NFM，結合一堆ML方法：SVM,KNN,CNN</li>
<li>LRP，用以識別文件中的相關單詞。</li>
<li>ctx-DocNADE：DocNADE和LSTM，學習補充語意的功能。</li>
<li>LTMF：unsupervised透過使用LSTM和LDA</li>
</ul>
<h2 id="WORD-EMBEDDING-AND-MACHINE-LEARNING-MODEL-ARCHITECTURE"><a href="#WORD-EMBEDDING-AND-MACHINE-LEARNING-MODEL-ARCHITECTURE" class="headerlink" title="WORD EMBEDDING AND MACHINE LEARNING MODEL ARCHITECTURE"></a>WORD EMBEDDING AND MACHINE LEARNING MODEL ARCHITECTURE</h2><h3 id="Word-Embedding-Models"><a href="#Word-Embedding-Models" class="headerlink" title="Word Embedding Models"></a>Word Embedding Models</h3><ul>
<li>Word2Vec：CBOW or Skip-Gram</li>
<li>Glove：Count-based，計算單詞在上下文中出現的頻率來建造co-matrix，大型矩陣要通過分解來降維。</li>
</ul>
<h3 id="Machine-Learning-1"><a href="#Machine-Learning-1" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><ul>
<li>HMM：一種圖形模型，預測順序的hidden state。不會觀察到狀態，但是可以透過使用前－後遞algorithm來推斷state</li>
<li>CNN</li>
<li>RNN：GRU用在小型dataset，LSTM用在大的</li>
</ul>
<h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><h3 id="Word-Embedding-and-HMM"><a href="#Word-Embedding-and-HMM" class="headerlink" title="Word Embedding and HMM"></a>Word Embedding and HMM</h3><p>使用word embedding替代word。然後為每一個topic建構一個HMM。在dataset中有20個topic所以有20個HMM model，看哪一種state和Gaussian Mixture會給出較好的結果，在每一個state$state_1,…,state_{10}$，每一個GM都會進行測試$GM_1,…,GM_{10}$<img src="https://i.imgur.com/VFtYP2t.png" alt=""></p>
<h3 id="Word-Embedding-and-CNN"><a href="#Word-Embedding-and-CNN" class="headerlink" title="Word Embedding and CNN"></a>Word Embedding and CNN</h3><p>圖太大去看原論文</p>
<h3 id="Word-Embedding-and-RNN-CNN"><a href="#Word-Embedding-and-RNN-CNN" class="headerlink" title="Word Embedding and RNN-CNN"></a>Word Embedding and RNN-CNN</h3><p>圖太大去看原論文</p>
<h2 id="Experimental"><a href="#Experimental" class="headerlink" title="Experimental"></a>Experimental</h2><p>dataset：20NewsGroup dataset(分類)<br>18846 document分為20個topic<br>60% for training 40% for testing。<br>使用word2vec(google)並且在google news dataset pretrain，每一個都是300維。</p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>值得注意的是，與基於LSTM-CNN的模型經過訓練以找出最有區別的特徵並同時捕獲上下文相比，使用非歧視性算法訓練的基於HMM的模型提供的結果較差。</p>
<h2 id="Conclusion-and-Furure-work"><a href="#Conclusion-and-Furure-work" class="headerlink" title="Conclusion and Furure work"></a>Conclusion and Furure work</h2><p>提出supervised topic modeling，抓取單詞相關的global semantic meaning。獲取單詞序列和局部結構，<br>就Topic modeling而言，純粹的CNN會比RNN好，將兩個合在一起會更好。<br>Future work：跟訓練在特定領域的word embedding和通用的word embedding比較。<br>比較右到左dataset的各類model的性能，嘗試與unsupervised 獲得可比較的結果。</p>
<hr>
<h2 id="為啥你能上IEEE"><a href="#為啥你能上IEEE" class="headerlink" title="為啥你能上IEEE"></a>為啥你能上IEEE</h2>
        
      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>PoH_Ko</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/dsmi-lab-website/2020/10/22/Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques/" target="_blank" title="Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques">https://github.com/dsmilab/dsmi-lab-website/2020/10/22/Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'dsmi-lab';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dsmi-lab-website/tags/NLP-Topic-Modeling/" rel="tag">NLP Topic_Modeling</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/dsmi-lab-website/2020/10/22/Get%20To%20The%20Point_%20Summarization%20with%20Pointer-Generator%20Networks/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Get To The Point- Summarization with Pointer-Generator Networks
        
      </div>
    </a>
  
  
    <a href="/dsmi-lab-website/2020/10/22/Supervised%20Understanding%20of%20Word%20Embeddings/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Supervised Understanding of Word Embeddings</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Supervised-Topic-Modeling-Using-Word-Embedding-with-Machine-Learning-Techniques"><span class="nav-number">1.</span> <span class="nav-text">Supervised Topic Modeling Using Word Embedding with Machine Learning Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#tags-NLP-Topic-Modeling"><span class="nav-number">1.0.0.0.0.1.</span> <span class="nav-text">tags: NLP,Topic Modeling</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">1.3.</span> <span class="nav-text">Related Work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Generative-Models-機率"><span class="nav-number">1.3.1.</span> <span class="nav-text">Generative Models(機率)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-Word-Embedding"><span class="nav-number">1.3.2.</span> <span class="nav-text">Using Word Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Learning"><span class="nav-number">1.3.3.</span> <span class="nav-text">Machine Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WORD-EMBEDDING-AND-MACHINE-LEARNING-MODEL-ARCHITECTURE"><span class="nav-number">1.4.</span> <span class="nav-text">WORD EMBEDDING AND MACHINE LEARNING MODEL ARCHITECTURE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Embedding-Models"><span class="nav-number">1.4.1.</span> <span class="nav-text">Word Embedding Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Learning-1"><span class="nav-number">1.4.2.</span> <span class="nav-text">Machine Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proposed-Method"><span class="nav-number">1.5.</span> <span class="nav-text">Proposed Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Embedding-and-HMM"><span class="nav-number">1.5.1.</span> <span class="nav-text">Word Embedding and HMM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Embedding-and-CNN"><span class="nav-number">1.5.2.</span> <span class="nav-text">Word Embedding and CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Embedding-and-RNN-CNN"><span class="nav-number">1.5.3.</span> <span class="nav-text">Word Embedding and RNN-CNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experimental"><span class="nav-number">1.6.</span> <span class="nav-text">Experimental</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Discussion"><span class="nav-number">1.7.</span> <span class="nav-text">Discussion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion-and-Furure-work"><span class="nav-number">1.8.</span> <span class="nav-text">Conclusion and Furure work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#為啥你能上IEEE"><span class="nav-number">1.9.</span> <span class="nav-text">為啥你能上IEEE</span></a></li></ol></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 DSMI Lab&#39;s website All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/dsmi-lab-website/" class="mobile-nav-link">Home</a>
  
    <a href="/dsmi-lab-website/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/dsmi-lab-website/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/dsmi-lab-website/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/dsmi-lab-website/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/dsmi-lab-website/fancybox/jquery.fancybox.css">

  
<script src="/dsmi-lab-website/fancybox/jquery.fancybox.pack.js"></script>




<script src="/dsmi-lab-website/js/scripts.js"></script>





  
<script src="/dsmi-lab-website/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字號大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已調整頁面字體大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜間護眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜間模式已經開啟，再次單擊按鈕即可關閉
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            DSMI Lab&#39;s website
          </div>
          <div class="panel-body">
            Copyright © 2021 DSMI members All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>